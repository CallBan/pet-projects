{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClcTC1PbNFeZ"
      },
      "source": [
        "# Практика \"Transformer с нуля\"\n",
        "\n",
        "В этом задании мы напишем архитектуру transformer с нуля. Мы пройдемся по всем слоям трансформера - от эмбеддингов и аттеншена до FFN и финального выходного слоя. В конце также напишем различные техники сэмплирования для генерации текста!\n",
        "\n",
        "\n",
        "В качестве весов мы будем использовать веса gpt2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnqvoOGqZ2oX"
      },
      "source": [
        "# Устанавливаем зависимости"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcuFItWyPbey",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%pip install transformer_lens\n",
        "%pip install einops\n",
        "%pip install jaxtyping\n",
        "%pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAVtXXZiPJlr"
      },
      "outputs": [],
      "source": [
        "import os; os.environ['ACCELERATE_DISABLE_RICH'] = \"1\"\n",
        "import einops\n",
        "from dataclasses import dataclass\n",
        "from transformer_lens import HookedTransformer\n",
        "import torch as t\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "from jaxtyping import Float, Int\n",
        "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Загружаем веса gpt2 для проверки\n",
        "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
        "reference_gpt2 = reference_gpt2.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4LYJKDzNFep"
      },
      "source": [
        "Конфиг, который хранит в себе всю информацию о размерностях модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjdS6H9pO9E0"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    d_model: int = 768 # он же hidden_dim - внутрення размерность модели\n",
        "    debug: bool = True\n",
        "    layer_norm_eps: float = 1e-5\n",
        "    d_vocab: int = 50257 # он же vocab_size, размер словаря модели\n",
        "    init_range: float = 0.02\n",
        "    n_ctx: int = 1024 # число позиционных эмбеддингов\n",
        "    d_head: int = 64 # размерность головы аттеншена\n",
        "    d_mlp: int = 3072 # внутренняя размерность FFN-слоя\n",
        "    n_heads: int = 12 # число голов аттеншена\n",
        "    n_layers: int = 12 # число слоев трансформера\n",
        "\n",
        "cfg = Config()\n",
        "print(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaqOVKXtNFer"
      },
      "source": [
        "Код для генерации тестов, которые мы будем использовать для проверки слоев!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctCDi2ccPx-H"
      },
      "outputs": [],
      "source": [
        "def rand_float_test(cls, shape):\n",
        "    cfg = Config(debug=True)\n",
        "    layer = cls(cfg).to(device)\n",
        "    random_input = torch.randn(shape).to(device)\n",
        "    print(\"Input shape:\", random_input.shape)\n",
        "    output = layer(random_input)\n",
        "    if isinstance(output, tuple): output = output[0]\n",
        "    print(\"Output shape:\", output.shape, \"\\n\")\n",
        "\n",
        "def rand_int_test(cls, shape):\n",
        "    cfg = Config(debug=True)\n",
        "    layer = cls(cfg).to(device)\n",
        "    random_input = torch.randint(100, 1000, shape).to(device)\n",
        "    print(\"Input shape:\", random_input.shape)\n",
        "    output = layer(random_input)\n",
        "    if isinstance(output, tuple): output = output[0]\n",
        "    print(\"Output shape:\", output.shape, \"\\n\")\n",
        "\n",
        "def load_gpt2_test(cls, gpt2_layer, input):\n",
        "    cfg = Config(debug=True)\n",
        "    layer = cls(cfg).to(device)\n",
        "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
        "    print(\"Input shape:\", input.shape)\n",
        "    output = layer(input)\n",
        "    if isinstance(output, tuple): output = output[0]\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    try: reference_output = gpt2_layer(input)\n",
        "    except: reference_output = gpt2_layer(input, input, input)\n",
        "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
        "    comparison = t.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
        "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSNvnHYaQrqZ"
      },
      "outputs": [],
      "source": [
        "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
        "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
        "print(tokens)\n",
        "print(tokens.shape)\n",
        "print(reference_gpt2.to_str_tokens(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iN1wz9mUNTY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
        "print(logits.shape)\n",
        "\n",
        "print(\"Все работает, мы готовы к выполнению задания!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEWC0tJhNFew"
      },
      "source": [
        "# Архитектура Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kIRmiHZ87sw"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvsBc6HvNFex"
      },
      "source": [
        "Здесь нам даются токены размерности `[batch_size, seq_len]` - индексы слов в словаре. Нужно описать слой Embed, который будет отображать каждый токен в соответствующий вектор из матрицы эмбеддингов. Таким образом каждому токену предоставляется вектор, который будет иметь размерности `[batch_size, seq_len, d_model]`\n",
        "\n",
        "Внимание - здесь не нужно исользовать цикл for и проходиться по матрице. Все стандартные операции доступны в [документации](https://pytorch.org/docs/stable/nn.functional.html), в частности тут нам понадобится одна из операций в секции [sparse functions](https://pytorch.org/docs/stable/nn.functional.html#sparse-functions).\n",
        "\n",
        "Важное замечание - на самом деле этот слой уже есть [готовый в pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html), но мы в учебных целях переписываем его сами.\n",
        "\n",
        "\n",
        "Также можно решить этот пример через индексацию или через einops.\n",
        "\n",
        "**Вообще почти во всех примерах есть несколько возможных стилей описания операций над тензорами - через torch.nn.functional, через различные индексации и трюки pytorch, через einops - можно делать любым удобным способом!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srX7Xj8FaMHX"
      },
      "outputs": [],
      "source": [
        "class Embed(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, input_ids: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
        "        pass\n",
        "        # Ваш код здесь!\n",
        "\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 4\n",
        "rand_int_test(Embed, [batch_size, seq_len])\n",
        "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDtlhxmmNFey"
      },
      "source": [
        "# Positional Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ4BU-pbNFez"
      },
      "source": [
        "В трансформерах есть не только обычные эмбеддинги, которые отвечают за \"смысл\" токенов, но и позиционные эмбеддинги! Вход у них такой же, как и у обычных эмбеддингов, только они должны эмбеддить позиции токенов, а не сами токены. Т.е. в матрице W_pos хранятся не эмбеддинги токенов, а эмбеддинги позиций.\n",
        "\n",
        "Поэтому в этом слое нужно:\n",
        "1. По tokens получить тензор positions размера `[batch_size, seq_len]`\n",
        "2. Заэмбеддить тензор positions, как в предыдущем слое.\n",
        "\n",
        "Важно - как и в предыдущем случае, для этот слой обычно используется через [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "\n",
        "\n",
        "Вспомним еще про то, откуда берутся позиционные эмбеддинги: в оригинальном трансформере позиционные эмбеддинги состояли из синусов и косинусов (см. пункт 3.5 из оригинальной статьи https://arxiv.org/pdf/1706.03762), однако позиционные эмбеддинги можно учить и с нуля, как и обычные эмбеддинги. **В рамках данного задания не нужно никак дополнительно инициализировать веса, только применить позиционные эмбеддинги**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trB2m2P8Rgrk"
      },
      "outputs": [],
      "source": [
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, input_ids: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
        "        pass\n",
        "        # Ваш код здесь!\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 4\n",
        "rand_int_test(PosEmbed, [batch_size, seq_len])\n",
        "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4RHGt27NFe0"
      },
      "source": [
        "# LM head\n",
        "\n",
        "Финальный слой. У нас есть выходы из трансформера размерности `[batch_size, seq_len, d_model]`. Это контекстуализированные представления каждого токена. По ним мы предсказываем следующий токен, т.е. применяем линейный слой - умножаем на матрицу `[d_model, vocab_size]`.\n",
        "\n",
        "В этом нам поможет секция [linear functions](https://pytorch.org/docs/stable/nn.functional.html#linear-functions). Не забудьте про bias!\n",
        "\n",
        "В pytorch этот слой тоже есть - [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLXcAsSMU58C"
      },
      "outputs": [],
      "source": [
        "# LM_head, но для совместимости с библиотекой для проверки пришлось назвать его Unembed\n",
        "# по аналогии с тем, что мы из индексов в словаре получаем эмбеддинги, а тут из эмбеддингов обратно\n",
        "# распределение по словарю\n",
        "\n",
        "class Unembed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
        "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
        "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
        "\n",
        "    def forward(\n",
        "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
        "    ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
        "        pass\n",
        "        # Ваш код здесь!\n",
        "\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 4\n",
        "d_model = 768\n",
        "rand_float_test(Unembed, [batch_size, seq_len, d_model])\n",
        "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9SqOFi9NFe1"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfxZG2l7A-wQ"
      },
      "source": [
        "# Attention-формулы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30Q7cmNtFudx"
      },
      "source": [
        "1. **Входные эмбеддинги**:\n",
        "   $$X \\in \\mathbb{R}^{seq \\times d} $$\n",
        "2. **Маскированный мультихед-аттеншен (Masked Multi-Head Attention)**:\n",
        "$$M = \\begin{cases}\n",
        " &  m_{ij} = -\\infty, \\quad i < j \\\\\n",
        " &  m_{ij} = 0\n",
        "\\end{cases} $$\n",
        "\n",
        "$$\n",
        "M = \\begin{pmatrix}\n",
        "0 & -\\infty & -\\infty & \\ldots & -\\infty \\\\\n",
        "0 & 0 & -\\infty & \\ldots & -\\infty \\\\\n",
        "0 & 0 & 0 & \\ldots & -\\infty \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & 0 & \\ldots & 0 \\\\\n",
        "\\end{pmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FwfAHyzQ1Bs"
      },
      "source": [
        "3. Для каждой головы $ h_i $:\n",
        "\n",
        "    3.1 **Матрицы весов для запросов, ключей и значений**:\n",
        "     - $ W_Q \\in \\mathbb{R}^{d \\times d_h} $\n",
        "     - $ W_K \\in \\mathbb{R}^{d \\times d_h} $\n",
        "     - $ W_V \\in \\mathbb{R}^{d \\times d_h} $\n",
        "     \n",
        "    3.2. **Запросы, ключи и значения**:\n",
        "     - $ Q = X W_Q \\in \\mathbb{R}^{seq \\times d_h} $\n",
        "     - $ K = X W_K \\in \\mathbb{R}^{seq \\times d_h} $\n",
        "     - $ V = X W_V \\in \\mathbb{R}^{seq \\times d_h} $\n",
        "\n",
        "    3.3. **Скалярные произведения запросов и ключей**:\n",
        "     - $ \\frac{Q K^T}{\\sqrt{d_h}} + M \\in \\mathbb{R}^{seq \\times seq} $\n",
        "\n",
        "    3.4. **Веса внимания**:\n",
        "     - $ \\alpha = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_h}} + M\\right) \\in \\mathbb{R}^{seq \\times seq} $\n",
        "\n",
        "    3.5. **Агрегация значений**:\n",
        "     - $ z = \\alpha V \\in \\mathbb{R}^{seq \\times d_h} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEuVgMeMR6ce"
      },
      "source": [
        "4. **Конкатенация выходов всех голов**:\n",
        "   - $ Z = \\text{Concat}(z_1, z_2, \\ldots, z_h) \\in \\mathbb{R}^{seq \\times d} $\n",
        "\n",
        "5. **Выходной линейный слой**:\n",
        "   - Матрица весов: $ W^O \\in \\mathbb{R}^{d \\times d} $\n",
        "   - Итоговый выход: $ O = Z W^O + X \\in \\mathbb{R}^{seq \\times d} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc6hbJeBNFe3"
      },
      "source": [
        "# Attention - детали реализации\n",
        "Самое сложное в этом домашнем задании - подсчет механизма внимания. Как и в предыдущих вариантах, считать можно через torch или с помощью einops и любыми другими удобными способами.\n",
        "\n",
        "\n",
        "В данном задании нужно реализовать multihead attention с маскированием. Давайте разбираться по шагам, что нам нужно сделать.\n",
        "\n",
        "Далее будет описан один из возможных алогритмов написания аттеншена, но повторимся - писать можно любым удобным способом (голый torch или einops).\n",
        "\n",
        "1. Нам попадает на вход вектор x `[batch, seq_len, d_model]`. Нужно превратить его в матрицы проекций i-й головы аттеншена: Q_i, K_i, V_i. Для этого у нас есть матрицы W_Q, W_K, W_V (и их bias!). Это набор n_heads матриц размеров `[d_model, d_head]`. Зачастую число голов n_head и d_head подобраны так, что d_model == n_head * d_head, наш случай не исключение. Предлагается перевести (этот шаг сделан) матрицу `[num_heads, d_model, d_head]` в матрицу `[d_model, num_heads * d_head]` = `[d_model, d_model]`, после чего получить через матричное умножение на X размерности `[batch_size, seq_len, d_model]` получить матрицы Q, K, V размерностей `[batch_size, seq_len, d_model] = [batch_size, seq_len, num_heads * d_head]` и преобразовать их к виду `[batch_size, seq_len, num_heads, d_head]`. Не забудьте при матричном умножении транспонировать матрицы W_Q, W_K, W_V, если пойдете этим путем! В качестве шпаргалки посмотрите, как происходило умножение в lm_head!\n",
        "\n",
        "2. После этого можно сделать первый шаг и посчитать attention_scores, т.е. домножить $Q \\times K^T$. Тут нам поможет .transpose или .permute вместе с torch.matmul. Нужно переставить размерности матриц таким образом, чтобы финальное матричное умножение происходило по двум последним размерностям `[seq_len, d_head]` на `[d_head, seq_len]`, а все предыдущие размерности `[batch_size, num_heads]` совпадали\n",
        "\n",
        "\n",
        "3. Не забудем нормализацию, т.е. делим attention_scores на sqrt(d_head)\n",
        "\n",
        "4. Теперь нужно исползьовать маскирование! В данных заданиях предполагается, что у нас нет паддингов, поэтому нам нужно наложить маску с одним простым условием: i-й элемент не может смотреть на j-й элемент, если j > i. Это треугольная маска, с ней нам поможет приведение треугольной форме, которое вам предлагается найти в pytorch! Замаскированные значения нужно заполнить каким-нибудь большим по модулю отрицательным числом. В классе уже определено значение IGNORE, можно использовать его. Для этого реализуйте и используйте функцию `apply_causal_mask`. Заполнять значениями можно через индексацию, например через `torch.masked_fill`.\n",
        "\n",
        "5. Теперь к замаскированным attention_scores `[batch_size, num_heads, seq_len, seq_len]` нужно применить softmax. Подумайте, по какой размерности его применять и на что это повлияет.\n",
        "\n",
        "6. После этого остается последнее матричное умножение softmax(attention_scores) на V, к которому тоже придется применить .view, .permute и torch.matmul\n",
        "\n",
        "7. Теперь, если вы следовали этому плану у вас остается матрица `ouput` размерностей `[batch_size, num_heads, seq_len, d_head]`. С помощью permute и view собираем (конкатенируем) ее обратно в матрицу `[batch_size, seq_len, num_heads * d_head] = [batch_size, seq_len, d_model]` и применяем к ней выходной линейный слой W_O. Всё, аттеншен готов!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65yvUiIvTbTk"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    IGNORE: Float[Tensor, \"\"]\n",
        "\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
        "\n",
        "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
        "\n",
        "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
        "\n",
        "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
        "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
        "\n",
        "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
        "        self.register_buffer(\"IGNORE\", t.tensor(float(\"-inf\"), dtype=t.float32, device=device))\n",
        "\n",
        "    def forward(\n",
        "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
        "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
        "\n",
        "        # Берем размерности\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "        num_heads = self.cfg.n_heads\n",
        "        d_head = self.cfg.d_head\n",
        "\n",
        "        # 1. Трансформируем матрицы проекций в формат [d_model, d_model]\n",
        "        W_Q = self.W_Q.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
        "        W_K = self.W_K.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
        "        W_V = self.W_V.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
        "\n",
        "        b_Q = self.b_Q.view(-1)\n",
        "        b_K = self.b_K.view(-1)\n",
        "        b_V = self.b_V.view(-1)\n",
        "\n",
        "        # 1. получаем проекции  Q, K, V\n",
        "        pass\n",
        "        # Ваш код здесь!\n",
        "\n",
        "        # 2. Q x K^T\n",
        "\n",
        "\n",
        "        # 3. Нормализация\n",
        "\n",
        "        # 4. Маскирование\n",
        "\n",
        "        # 5. softmax\n",
        "\n",
        "\n",
        "        # 6. Финальная проекция\n",
        "        ...\n",
        "\n",
        "    def apply_causal_mask(\n",
        "        self, attn_scores: Float[Tensor, \"batch n_heads seq_len seq_len\"]\n",
        "    ) -> Float[Tensor, \"batch n_heads seq_len seq_len\"]:\n",
        "        '''\n",
        "        Используем треугольную маску, чтобы не смотреть в будущее, паддингов нет\n",
        "        В качестве масикировочного значения перед софтмаксом можно использовать self.IGNORE (-inf)\n",
        "        '''\n",
        "        pass\n",
        "        # Ваш код здесь!\n",
        "\n",
        "torch.manual_seed(1)\n",
        "batch_size = 2\n",
        "seq_len = 4\n",
        "d_model = 768\n",
        "rand_float_test(Attention, [batch_size, seq_len, d_model])\n",
        "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"normalized\", 0, \"ln1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwdYr4XnNFe5"
      },
      "source": [
        "Если вы справились с этим, то поздравляю - ничего сложнее мы сегодня уже не будем делать)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anGg23a4_aTy"
      },
      "source": [
        "# MLP (или FFN в других терминологиях)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRLuXGJb6NT2"
      },
      "source": [
        "Реализуем MLP слой - это 2 матричных умножения с нелинейностью GELU.\n",
        "\n",
        "- $$ \\text{MLP}(X) = (\\text{GeLU}(X W_1 + b_1)) W_2 + b_2 \\in \\mathbb{R}^{\\text{seq} \\times d}$$\n",
        "-    $$W_1 \\in \\mathbb{R}^{d \\times d_{mlp}}, \\quad b_1 \\in \\mathbb{R}^{d_{mlp}} \\\\\n",
        "W_2 \\in \\mathbb{R}^{d_{mlp} \\times d}, \\quad b_2 \\in \\mathbb{R}^{d} \\\\ $$\n",
        "\n",
        "\n",
        "$$GELU(X) = 0.5 * x * (1 + tanh(\\sqrt {\\frac {2} {\\pi}} * (x + 0.44715 * x^3)))$$\n",
        "\n",
        "если будете использовать gelu из pytorch, то **обязательно** проставьте approximate=\"tanh\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hQCIDevT0h3"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
        "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
        "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
        "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
        "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(\n",
        "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
        "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
        "        pass\n",
        "        # Ваш код здесь!\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "rand_float_test(MLP, [batch_size, seq_len, d_model])\n",
        "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"normalized\", 0, \"ln2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVAp7w1gR8tU"
      },
      "source": [
        "# Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aM9JUorIwwj"
      },
      "source": [
        "**Layer Normalization**:\n",
        "   - $ \\text{LayerNorm}(X) = \\frac{X - \\mu}{\\sigma} \\cdot \\gamma + \\beta $\n",
        "   - $\\mu = \\text{mean}(X, \\text{dim}=-1) \\in \\mathbb{R}^{d}$\n",
        "   - $\\sigma = \\sqrt{\\text{var}(X, \\text{dim}=-1) + \\epsilon} \\in \\mathbb{R}^{d}$\n",
        "   - $\\gamma \\in \\mathbb{R}^{d}$\n",
        "   - $\\beta \\in \\mathbb{R}^{d}$\n",
        "   \n",
        "   \n",
        "1. Не забудьте про эпсилон, который хранится в cfg!\n",
        "2. В [подсчете дисперсии](https://pytorch.org/docs/stable/generated/torch.var.html) не используете коррекцию Бесселя! Для этого в зависимости от версии pytorch поставьте `unbiased=False` или `correction=0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnNVykkAP49l"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.w = nn.Parameter(t.ones(cfg.d_model)) # gamma\n",
        "        self.b = nn.Parameter(t.zeros(cfg.d_model)) # beta\n",
        "\n",
        "    def forward(self, x: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
        "        pass\n",
        "        # Ваш код здесь!\n",
        "\n",
        "\n",
        "\n",
        "rand_float_test(LayerNorm, [2, 4, 768])\n",
        "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, cache[\"resid_post\", 11])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9RPPHidUzRB"
      },
      "source": [
        "# Transformer Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO7XJrFGNFe9"
      },
      "source": [
        "Это блок трансформера, который получает на вход тензор x `[batch_size, seq_len, d_model]` и выдает тензор таких же размерностей. Блок GPT2 немного отличается от классического трансформера.\n",
        "\n",
        "\n",
        "![image.png](https://camo.githubusercontent.com/ebd052b635f156d5d24224f25fa078d804156be51125cd6626b92d9f8b406bbb/68747470733a2f2f6c6f6e6570617469656e742d313235373934353937382e636f732e61702d6368656e6764752e6d7971636c6f75642e636f6d2f53656c656374696f6e5f3030312e706e67)\n",
        "\n",
        "GPT2 следует схеме PreLN, а \"классический\" трансформер схеме PostLN. **Реализовать нужно PreLN схему!**\n",
        "\n",
        "В PostLN схеме нормализация происходит после слоев attention и MLP, а в PreLN до них согласно иллюстрации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7fEX6-8VRjV"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.ln1 = LayerNorm(cfg)\n",
        "        self.attn = Attention(cfg)\n",
        "        self.ln2 = LayerNorm(cfg)\n",
        "        self.mlp = MLP(cfg)\n",
        "\n",
        "    def forward(\n",
        "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
        "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
        "        pass\n",
        "        # Ваш код здесь!\n",
        "\n",
        "\n",
        "rand_float_test(TransformerBlock, [2, 4, 768])\n",
        "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFaCjaBmNFe-"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQBb_bL3NFe-"
      },
      "source": [
        "Собираем все в один большой трансформер.\n",
        "1. Применяем эмбеддинги и позиционные эмбеддинги, складываем результаты\n",
        "2. Прогоняем в цикле через все блоки трансформера\n",
        "3. Применяем финальную нормализацию и lm_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdbjvO9sVLrk"
      },
      "outputs": [],
      "source": [
        "class DemoTransformer(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.embed = Embed(cfg)\n",
        "        self.pos_embed = PosEmbed(cfg)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
        "        self.ln_final = LayerNorm(cfg)\n",
        "        self.unembed = Unembed(cfg)\n",
        "\n",
        "    def forward(self, input_ids: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
        "        pass\n",
        "        # Ваш код здесь!\n",
        "\n",
        "\n",
        "\n",
        "rand_int_test(DemoTransformer, [2, 4])\n",
        "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcNrSDzgVjQg"
      },
      "outputs": [],
      "source": [
        "demo_gpt2 = DemoTransformer(Config(debug=False)).to(device)\n",
        "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
        "\n",
        "demo_logits = demo_gpt2(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb2_gL9uVmPj"
      },
      "outputs": [],
      "source": [
        "demo_gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkrsjh5SVpl3"
      },
      "outputs": [],
      "source": [
        "reference_gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyiHpWcvV2mn"
      },
      "outputs": [],
      "source": [
        "def get_log_probs(\n",
        "    logits: Float[Tensor, \"batch posn d_vocab\"],\n",
        "    tokens: Int[Tensor, \"batch posn\"]\n",
        ") -> Float[Tensor, \"batch posn-1\"]:\n",
        "\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
        "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    return log_probs_for_tokens\n",
        "\n",
        "\n",
        "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
        "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
        "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}\")\n",
        "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFqu7OlPV685"
      },
      "outputs": [],
      "source": [
        "test_string = '''The Total Perspective Vortex derives its picture of the whole Universe on the principle of'''\n",
        "for i in tqdm(range(100)):\n",
        "    test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
        "    demo_logits = demo_gpt2(test_tokens)\n",
        "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
        "\n",
        "print(test_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG0zGo4iJFMa"
      },
      "source": [
        "# Сэмплирование\n",
        "Теперь разберем различные техники сэмплирования: `apply_temperature`, `apply_frequency_penalty`, `sample_basic`, `sample_top_k`, `sample_top_p`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK5fbWLLWwSS"
      },
      "source": [
        "\n",
        "1. **Temperature Sampling**:\n",
        "   - Применяется первым, поскольку изменение температуры изменяет масштабы логитов перед дальнейшими операциями.\n",
        "\n",
        "2. **Frequency Penalty**:\n",
        "   - Применяется следующим, чтобы учесть частоты токенов до того, как логиты будут обрезаны методами top-k или top-p.\n",
        "\n",
        "3. **Top-k Sampling**:\n",
        "   - Применяется после temperature sampling и frequency penalty, так как он отбирает фиксированное количество наиболее вероятных токенов.\n",
        "\n",
        "4. **Top-p (Nucleus Sampling)**:\n",
        "   - Применяется после top-k sampling, чтобы отфильтровать токены на основе совокупной вероятности."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgdKJfPfbR-J"
      },
      "source": [
        "Обозначим размер словаря для удобства $\\Sigma = vocab\\_size$\n",
        "\n",
        "Пусть $ \\text{logits} \\in \\mathbb{R}^{\\text{seq} \\times \\Sigma} $:\n",
        "\n",
        "1. **Temperature Sampling**:\n",
        "   $$\n",
        "   \\text{logits}'_{i,j} = \\frac{\\text{logits}_{i,j}}{T} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, |\\Sigma|]\n",
        "   $$\n",
        "\n",
        "2. **Frequency Penalty**:\n",
        "   $$\n",
        "   \\text{penalty}(t_j) = 1 + \\alpha \\cdot f(t_j) \\\\\n",
        "   \\text{logits}''_{i,j} = \\text{logits}'_{i,j} - \\text{penalty}(t_j) \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma]\n",
        "%    \\text{logits}''_{i,j} = \\frac{\\text{logits}'_{i,j}}{\\text{penalty}(t_j)} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma]\n",
        "   $$\n",
        "\n",
        "3. **Top-k Sampling**:\n",
        "   $$\n",
        "   top\\_k\\_indices_i = \\text{argtop-k}(\\text{logits}''_i, k) \\quad \\forall \\ i \\in [1, \\text{seq}] \\\\\n",
        "   \\text{mask}_{i,j} =\n",
        "   \\begin{cases}\n",
        "   1 & \\text{если} \\ j \\in top\\_k\\_indices_i \\\\\n",
        "   0 & \\text{иначе}\n",
        "   \\end{cases} \\\\\n",
        "   \\text{logits}'''_{i,j} = \\text{logits}''_{i,j} \\cdot \\text{mask}_{i,j} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma]\n",
        "   $$\n",
        "\n",
        "4. **Top-p (Nucleus Sampling)**:\n",
        "   $$\n",
        "   sorted\\_logits_i, sorted\\_indices_i = \\text{sort}(\\text{logits}'''_i, \\text{descending=True}) \\quad ∀ \\ i \\in [1, \\text{seq}] \\\\\n",
        "   probs_i = softmax(sorted\\_logits_i) \\quad \\\\\n",
        "    cumulative\\_probs_{i,j} = \\sum_{k=1}^{j} \\text{probs}_{i,k} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma\n",
        "    \\quad \\forall \\ i \\in [1, \\text{seq}] \\\\\n",
        "   top\\_p\\_mask_{i,j} =\n",
        "   \\begin{cases}\n",
        "   1, & cumulative\\_probs_{i,j} \\leq p \\\\\n",
        "   0 &\n",
        "   \\end{cases} \\\\\n",
        "   \\text{logits}^{\\text{final}}_{i,j} = sorted\\_logits_{i,j} \\cdot top\\_p\\_mask_{i,j} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma]\n",
        "   $$\n",
        "\n",
        "5. **Softmax**:\n",
        "   $$\n",
        "   \\mathbf{probs}_{i,j} = \\text{softmax}(\\text{logits}^{\\text{final}}_{i,j}) \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, |\\Sigma|]\n",
        "\\\\\n",
        "   \\mathbf{probs}_{i,j} = \\frac{e^{\\text{logits}^{\\text{final}}_{i,j}}}{\\sum_{k=1}^{|\\Sigma|} e^{\\text{logits}^{\\text{final}}_{i,k}}}\n",
        "   $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhLvuGw5UpOh"
      },
      "outputs": [],
      "source": [
        "model_cfg = Config()\n",
        "model = DemoTransformer(model_cfg).to(device)\n",
        "model.load_state_dict(reference_gpt2.state_dict(), strict=False) # загружаем веса gpt2\n",
        "\n",
        "tokenizer = reference_gpt2.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opp-1FmGWgGn"
      },
      "outputs": [],
      "source": [
        "class TransformerSampler:\n",
        "\n",
        "    def __init__(self, model: DemoTransformer, tokenizer: GPT2TokenizerFast):\n",
        "        self.model = model\n",
        "        self.cfg = model.cfg\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):\n",
        "        '''\n",
        "        Возвращаем сгенерированную строку, включая промпт.\n",
        "        Генерация заканчивается после max_tokens_generated токенов или по генерации EOS.\n",
        "\n",
        "        kwargs передаются в sample_next_token\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def sample_next_token(\n",
        "        input_ids: Int[Tensor, \"seq_len\"],\n",
        "        logits: Float[Tensor, \"d_vocab\"],\n",
        "        temperature=1.0,\n",
        "        top_k=0,\n",
        "        top_p=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "        seed=None\n",
        "    ):\n",
        "        assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
        "        assert temperature >= 0, \"Temperature should be non-negative\"\n",
        "        assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
        "        assert 0 <= top_k, \"Top-k must be non-negative\"\n",
        "        assert not (top_p != 0 and top_k != 0), \"At most one of top-p and top-k supported\"\n",
        "\n",
        "        # Set random seeds for reproducibility\n",
        "        if seed is not None:\n",
        "            t.manual_seed(seed)\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        # Apply all the specialized sampling methods\n",
        "        if temperature == 0:\n",
        "            return TransformerSampler.greedy_search(logits)\n",
        "        elif temperature != 1.0:\n",
        "            logits = TransformerSampler.apply_temperature(logits, temperature)\n",
        "        if frequency_penalty != 0.0:\n",
        "            logits = TransformerSampler.apply_frequency_penalty(input_ids, logits, frequency_penalty)\n",
        "        if top_k > 0:\n",
        "            return TransformerSampler.sample_top_k(logits, top_k)\n",
        "        if top_p > 0.0:\n",
        "            return TransformerSampler.sample_top_p(logits, top_p)\n",
        "        return TransformerSampler.sample_basic(logits)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def greedy_search(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
        "        '''\n",
        "        Возвращаем самый вероятный токен жадно\n",
        "        '''\n",
        "        out = logits.argmax().item()\n",
        "        return out\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_temperature(logits: Float[Tensor, \"d_vocab\"], temperature: float) -> Float[Tensor, \"d_vocab\"]:\n",
        "        '''\n",
        "        Применяем температуру к логитам\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_frequency_penalty(input_ids: Int[Tensor, \"seq_len\"], logits: Float[Tensor, \"d_vocab\"], freq_penalty: float) -> Float[Tensor, \"d_vocab\"]:\n",
        "        '''\n",
        "        Применяем frequency penalty к логитам\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def sample_basic(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
        "        '''\n",
        "        Простое сэмплирование! Тут нам поможет torch.multinomial\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def sample_top_k(logits: Float[Tensor, \"d_vocab\"], k: int) -> int:\n",
        "        '''\n",
        "        top-k сэмплирование\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def sample_top_p(logits: Float[Tensor, \"d_vocab\"], top_p: float, min_tokens_to_keep: int = 1) -> int:\n",
        "        '''\n",
        "        top_p сэмплирование\n",
        "        '''\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tear0Nn0WjZY"
      },
      "outputs": [],
      "source": [
        "sampler = TransformerSampler(model, tokenizer)\n",
        "\n",
        "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
        "print(f\"Greedy decoding with prompt: {prompt!r}\\n\")\n",
        "\n",
        "output = sampler.sample(prompt, max_tokens_generated=8, temperature=0.0)\n",
        "print(f\"Your model said: {output!r}\\n\")\n",
        "\n",
        "expected = \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
        "assert output == expected\n",
        "\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3VYdChTdNLc"
      },
      "outputs": [],
      "source": [
        "logits = t.tensor([1, 2]).log()\n",
        "\n",
        "cold_logits = TransformerSampler.apply_temperature(logits, temperature=0.001)\n",
        "print('A low temperature \"sharpens\" or \"peaks\" the distribution: ', cold_logits)\n",
        "t.testing.assert_close(cold_logits, 1000.0 * logits)\n",
        "\n",
        "hot_logits = TransformerSampler.apply_temperature(logits, temperature=1000.0)\n",
        "print(\"A high temperature flattens the distribution: \", hot_logits)\n",
        "t.testing.assert_close(hot_logits, 0.001 * logits)\n",
        "\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0no5kj-dTos"
      },
      "outputs": [],
      "source": [
        "bieber_prompt = \"And I was like Baby, baby, baby, oh Like, Baby, baby, baby, no Like, Baby, baby, baby, oh I thought you'd always be mine, mine\"\n",
        "input_ids = tokenizer.encode(bieber_prompt, return_tensors=\"pt\")\n",
        "logits = t.ones(tokenizer.vocab_size)\n",
        "penalized_logits = TransformerSampler.apply_frequency_penalty(input_ids.squeeze(), logits, 2.0)\n",
        "\n",
        "assert penalized_logits[5156].item() == -11, \"Expected 6 occurrences of ' baby' with leading space, 1-2*6=-11\"\n",
        "assert penalized_logits[14801].item() == -5, \"Expected 3 occurrences of ' Baby' with leading space, 1-2*3=-5\"\n",
        "\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjQi5zDMWtTJ"
      },
      "outputs": [],
      "source": [
        "prompt = \"John and Mary went to the\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "logits = model(input_ids)[0, -1]\n",
        "\n",
        "expected_top_10pct = {\n",
        "    \" church\": 0.0648,\n",
        "    \" house\": 0.0367, # These are the two most likely tokens, and add up to >10%\n",
        "}\n",
        "top_10pct_sum = sum(expected_top_10pct.values())\n",
        "\n",
        "observed_freqs = defaultdict(int)\n",
        "\n",
        "N = 10000\n",
        "for _ in tqdm(range(N)):\n",
        "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_p=0.1)\n",
        "    observed_freqs[tokenizer.decode(token)] += 1\n",
        "\n",
        "for word in expected_top_10pct:\n",
        "    expected_freq = expected_top_10pct[word] / top_10pct_sum\n",
        "    observed_freq = observed_freqs[word] / N\n",
        "    print(f\"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}\")\n",
        "    assert abs(observed_freq - expected_freq) < 0.01, \"Try increasing N if this fails by a small amount.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_5YlWllNFfF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xnqvoOGqZ2oX"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}