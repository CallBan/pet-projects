{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11861b12",
      "metadata": {
        "id": "11861b12"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "# можете сменить на mps на макбуке, но лично у меня он криво работает\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3",
      "metadata": {
        "id": "ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3"
      },
      "source": [
        "# Знакомство с Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3df5693",
      "metadata": {
        "id": "a3df5693"
      },
      "source": [
        "## Создание модели и предсказание следующего токена\n",
        "Нужно создать модель через `AutoModelForCausalLM`, создать токенайзер через `AutoTokenizer` и олучить следующий токен через жадную генерацию!\n",
        "\n",
        "**Внимание** на каких-то из функций далее у вас может кончаться видеопамять из-за хранения активаций. Чтобы этого не происходило рекомендуется все вычисления оборачивать в контекстный менеджер `with torch.no_grad()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6ab0d8-60a0-4def-a45c-b1becf4930e1",
      "metadata": {
        "id": "5a6ab0d8-60a0-4def-a45c-b1becf4930e1"
      },
      "outputs": [],
      "source": [
        "def move_to_device(inputs, device):\n",
        "    for k, v in inputs.items():\n",
        "        inputs[k] = v.to(device)\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec7e08b",
      "metadata": {
        "id": "6ec7e08b"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "\n",
        "text = \"This is a sample text\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "inputs = move_to_device(inputs, device)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "next_token_idx = logits[0][-1].argmax().item()\n",
        "\n",
        "\n",
        "next_token = tokenizer.decode([next_token_idx])\n",
        "\n",
        "assert next_token.strip() == \"file\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6809813",
      "metadata": {
        "id": "e6809813"
      },
      "source": [
        "## Используем Generate\n",
        "\n",
        "Мы с вами помним про различные виды сэмплинга - top_k, top_p, temperature,frequency penalty.\n",
        "Отличная новость заключается в том, что нам не нужно все это писать самим! Оно уже включено в [GenerationMixin](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#generation), от которого наследуются модели для генерации текста.\n",
        "\n",
        "Для генерации есть функция [generate](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "\n",
        "Ваша задача написать для модели выше генерацию по тексту с:\n",
        "* Температурой - 0.9\n",
        "* Top-K - 20\n",
        "* Repetition Penalty (Frequency Penalty) - 1.2\n",
        "* максимальное число новых токенов - 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b62dbf",
      "metadata": {
        "id": "a6b62dbf"
      },
      "outputs": [],
      "source": [
        "text = \"This is still a sample text, but\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "inputs = move_to_device(inputs, device)\n",
        "\n",
        "results = []\n",
        "for i in range(10):\n",
        "    gens = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_k=20,\n",
        "        repetition_penalty=1.2,\n",
        "        max_new_tokens=10,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    results.append(tokenizer.decode(gens[0, inputs.input_ids.size(1):]))\n",
        "\n",
        "assert len(set(results)) > 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b90512b-9420-45b3-9f4c-22fb5fa1bfc7",
      "metadata": {
        "id": "8b90512b-9420-45b3-9f4c-22fb5fa1bfc7"
      },
      "source": [
        "## Generate Batched\n",
        "Теперь давайте жадно сгенерируем текст, но забатчуем несколько сэмплов. До этого мы всегда генерировали по батчу размера 1, поэтому у нас не было паддингов!\n",
        "\n",
        "Когда появляется несколько текстов разной длины, то появляются и паддинги.\n",
        "\n",
        "Представим себе ситуцию, что у нас батч из двух элементов длины 2 и 5 (токен -1 будет выступать в качестве паддинга **только для удобства визуализации**).\n",
        "\n",
        "Тогда\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [3, 2, -1, -1, -1]\n",
        "    [5, 6,  7,  1,  2]\n",
        "]\n",
        "attention_mask = [\n",
        "    [1, 1, 0, 0, 0],\n",
        "    [1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "Представим, что мы сгенерировали еще один токен, тогда\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [3, 2, -1, -1, -1, 7]\n",
        "    [5, 6,  7,  1,  2, 8]\n",
        "]\n",
        "attention_mask = [\n",
        "    [1, 1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "Получается, что у нас паддинги в маске возникают посередине. Мы не будем заниматься реализацией своего алгоритма генерации здесь, но отметим, что добавление паддинга слева значительно упрощает этот процесс.\n",
        "Тогда исходная последовательность будет:\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [-1, -1, -1, 3, 2]\n",
        "    [ 5,  6,  7, 1, 2]\n",
        "]\n",
        "attention_mask = [\n",
        "    [0, 0, 0, 1, 1],\n",
        "    [1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "и после генерации следующего токена\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [-1, -1, -1, 3, 2, 7]\n",
        "    [ 5,  6,  7, 1, 2, 8]\n",
        "]\n",
        "attention_mask = [\n",
        "    [0, 0, 0, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "В качестве задания давайте соберем батч с левым паддингом и проверим, что жадная генерация (10 токенов) совпадает с генерацией на текстах по отдельности!\n",
        "\n",
        "Для этого нам придется использовать параметр padding_side в конструкторе токенизатора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1",
      "metadata": {
        "id": "5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\")\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd38bdc-3e5e-400d-8815-e9c08a757c03",
      "metadata": {
        "scrolled": true,
        "id": "1bd38bdc-3e5e-400d-8815-e9c08a757c03",
        "outputId": "05914f48-e34a-48dc-aaa1-e17b08b068b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "texts = [\"This is a sample text\", \"I'm really tired and this is just about\"]\n",
        "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
        "inputs = move_to_device(inputs, device)\n",
        "\n",
        "batched_generations: List[str] = []\n",
        "single_generations: List[str] = []\n",
        "\n",
        "generations_batched = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=10\n",
        ")\n",
        "\n",
        "batched_generations = tokenizer.batch_decode(generations_batched[:, inputs.input_ids.size(1):])\n",
        "for text in texts:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    inputs = move_to_device(inputs, device)\n",
        "    g = model.generate(**inputs, max_new_tokens=10)\n",
        "    single_generations.append(tokenizer.decode(g[0, inputs.input_ids.size(1):]))\n",
        "\n",
        "assert len(batched_generations) == 2 and len(single_generations) == 2\n",
        "for s, b in zip(batched_generations, single_generations):\n",
        "    assert s == b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9280196b-2526-4596-a534-07d5f368c8f2",
      "metadata": {
        "id": "9280196b-2526-4596-a534-07d5f368c8f2"
      },
      "source": [
        "# KV Cache\n",
        "При генерации есть опция use_cache - это использование KV cache для генерации.\n",
        "В рамках этой техники в при генерации в декодере считается только аттеншн последнего токена по всем векторам предыдущих токенов, которые посчитали на предыдущих этапах, а для \"старых\" (левых) токенов аттеншн не пересчитывается, т.к. \"новые\" (правые) токены на них не влияют.\n",
        "\n",
        "\n",
        "\n",
        "В рамках данного задания нужно:\n",
        "1. Посчитать скорость генерации 100 токенов с и без kv cache, сказать, какая техника и во сколько раз быстрее.\n",
        "2. Подсчитать скорость генерации 1 токена с и без kv cache, сказать, какая техника быстрее и почему.\n",
        "\n",
        "Чтобы корректно сравнивать время генерации нужно использовать жадный сэмплинг!\n",
        "\n",
        "**Ответы на оба вопроса нужно оставить письменно прямо здесь**.\n",
        "\n",
        "1. Генерация 100 токенов должна быть быстрее, но во сколько раз зависит от процессора или видеокарты.\n",
        "2. Генерация одного не должна иметь разницы по скорости, т.к. kv cache не имеет смысла на генерации одного токена по заданному контекста."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72aaccda-6934-4cfc-a803-fc482409c7eb",
      "metadata": {
        "id": "72aaccda-6934-4cfc-a803-fc482409c7eb",
        "outputId": "f92bccdc-350c-4070-d740-f01758bd6a36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "with KV caching: 0.786 +- 0.016 seconds\n",
            "without KV caching: 1.138 +- 0.003 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "text = \"\"\"\n",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem justo, semper dignissim ipsum vitae, sollicitudin aliquet eros. Duis id ultricies erat. Vivamus commodo auctor massa ut mollis. Maecenas lacinia tempus orci, imperdiet ullamcorper felis accumsan et. Etiam mattis neque diam, at egestas nunc eleifend id. Fusce tristique orci nec sollicitudin elementum. Nullam dui est, feugiat ac pellentesque at, posuere non massa.\n",
        "\n",
        "Suspendisse accumsan ullamcorper dolor sed dictum. Mauris quis varius felis, quis gravida odio. Vestibulum diam arcu, aliquet convallis congue non, rutrum non turpis. Fusce vel orci ac diam suscipit lacinia. Curabitur maximus orci a dui gravida, accumsan convallis libero ornare. Phasellus dapibus, sapien pulvinar lacinia dictum, massa lacus scelerisque tellus, eu porta dolor eros vitae ex. Maecenas maximus, urna id pharetra dictum, dolor lorem sollicitudin ipsum, sit amet vestibulum orci felis quis leo. Pellentesque vel ligula ut urna eleifend condimentum nec et sem. Integer ligula nunc, rutrum ultricies urna et, congue suscipit lectus.\n",
        "\"\"\".strip()\n",
        "\n",
        "for use_cache in (True, False):\n",
        "  times = []\n",
        "  for _ in range(10):\n",
        "    start = time.time()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    inputs = move_to_device(inputs, device)\n",
        "    model.generate(**inputs, use_cache=use_cache, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
        "    times.append(time.time() - start)\n",
        "  print(f\"{'with' if use_cache else 'without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd1a3f3-741c-4b9e-9ae5-73446d3e7b6e",
      "metadata": {
        "id": "bdd1a3f3-741c-4b9e-9ae5-73446d3e7b6e",
        "outputId": "21e0d3f9-6c38-4c02-d6bd-419bed06d342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "with KV caching: 0.013 +- 0.002 seconds\n",
            "without KV caching: 0.011 +- 0.0 seconds\n"
          ]
        }
      ],
      "source": [
        "for use_cache in (True, False):\n",
        "  times = []\n",
        "  for _ in range(20):\n",
        "    start = time.time()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    inputs = move_to_device(inputs, device)\n",
        "    model.generate(**inputs, use_cache=use_cache, max_new_tokens=1, pad_token_id=tokenizer.eos_token_id)\n",
        "    times.append(time.time() - start)\n",
        "  print(f\"{'with' if use_cache else 'without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5da008c-3653-40d5-89ba-cd831352fd3d",
      "metadata": {
        "id": "f5da008c-3653-40d5-89ba-cd831352fd3d"
      },
      "source": [
        "# Скоринг, Perplixity\n",
        "\n",
        "Можно не только генерировать текст. Вспомним, что выдает после lm_head - вектор `[batch_size, seq_len, vocab_size]`, где для каждый вектор `[vocab_size]` это распределение вероятностей по следующему токену!\n",
        "\n",
        "Опустим размерность batch_size=1 для удобства, seq_len = 4. Пусть у нас есть текст `bos мама мыла раму` (`bos` спецсимвол для начала текста)\n",
        "\n",
        "Тогда вероятность этого текста расписывается через произведение условных вероятностей:\n",
        "\n",
        "```\n",
        "P(bos мама мыла раму) = P(мама | bos) * P(мыла | bos мама) * P(раму| bos мама мыла)\n",
        "```\n",
        "\n",
        "Т.е. это вероятность слова при условии его левого контекста.\n",
        "Зачастую ее обозначают как $P(x_i|x_{<i})$ где $x_i$ - i-е слово, $x_{<i}$ - контекст $[x_1, x_2, x_3, ... x_{i-1}]$\n",
        "Эти вероятности можно взять из выходного вектора!\n",
        "\n",
        "Давайте попробуем подсчитать вероятность и perplexity текстов!\n",
        "perplexity как и вероятность мера того насколько модель \"уверена\" в тексте, т.е. насколько по оценки ее параметрами данный текст вероятен.\n",
        "\n",
        "$$Perplexity(X) = exp(-\\frac {1} {N} \\sum_{i}^{N} log P(x_i | x_{<i}))$$\n",
        "\n",
        "В этом задании нужно:\n",
        "1. Посчитать вероятность **text**\n",
        "2. Посчитать перплексию **text**\n",
        "\n",
        "Еще одна важная деталь:\n",
        "работать с вероятностями плохо. Т.к. вероятность представляет собой число от 0 до 1, то при перемножении десятков или даже сотен таких числе теряется точность!\n",
        "Для этого от произведения вероятностей берут логарифм и получают logprobs - логарифмы вероятностей. Их можно складывать, по свойству логарифма логарифм произведения равен произведению логарифма.\n",
        "\n",
        "$$ p = p_1 * p_2 * p_3 $$\n",
        "$$log(p) = log (p_1) + log (p_2) + log (p_3)$$\n",
        "$$exp(log (p)) = p = exp(log (p_1) + log (p_2) + log (p_3)) = exp (log (p_1 * p_2 * p_3)) = p_1 * p_2 * p_3$$\n",
        "\n",
        "В pytorch для этого есть `torch.log_softmax`, который считается численно стабильно!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1c7ba39-a451-43a2-ac55-629c99259abe",
      "metadata": {
        "id": "e1c7ba39-a451-43a2-ac55-629c99259abe",
        "outputId": "738db00c-e323-4bb1-b16f-a028a4484bc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning of sentence (BOS) token = `<|endoftext|>`\n",
            "End of sentence (EOS) token  = `<|endoftext|>`\n",
            "tensor(2.1782e-14, device='cuda:0')\n",
            "tensor(51.0197, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(f\"Beginning of sentence (BOS) token = `{tokenizer.bos_token}`\")\n",
        "print(f\"End of sentence (EOS) token  = `{tokenizer.eos_token}`\")\n",
        "text = \"<|endoftext|>I'm so very tired of this<|endoftext|>\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "inputs = move_to_device(inputs, device)\n",
        "input_ids = inputs.input_ids\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits[0] # seq_len, vocab_size\n",
        "    logits = logits[:-1] # т.к. для последнего токена ничего не генерируем\n",
        "    probs = torch.log_softmax(logits, dim=1) # превращаем в вероятноси\n",
        "    targets = input_ids[0, 1:] # сдвигаем, т.к. 0й токен мы не предсказываем, а предсказывем сразу следующий, т.е. 1й\n",
        "    text_probs = torch.gather(probs, 1, targets.unsqueeze(1)).squeeze(1)\n",
        "    text_P = text_probs.sum().exp()\n",
        "    ppl = (-text_probs.mean()).exp()\n",
        "\n",
        "print(text_P)\n",
        "print(ppl)\n",
        "\n",
        "# должно получиться что-то около 2.1783e-14 для вероятности и около 51 для ppl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddd5038-620b-48bb-bbc1-db3729141d78",
      "metadata": {
        "id": "5ddd5038-620b-48bb-bbc1-db3729141d78"
      },
      "source": [
        "# Chat-Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Формат\n",
        "Мы уже знаем, что все chat-модели принимают входы в своем особом формате.\n",
        "Он может быть описан текстом, а может быть заложен в шаблон, который доступен через `tokenizer.apply_chat_template`"
      ],
      "metadata": {
        "id": "khA60oJDEE33"
      },
      "id": "khA60oJDEE33"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5fe593-63a8-406d-9678-6d805c180670",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3626491fc4c5431fa190782577c5af72"
          ]
        },
        "id": "7f5fe593-63a8-406d-9678-6d805c180670",
        "outputId": "0cbe2793-d891-4e8d-8f64-f192c19f5a5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/pypoetry/virtualenvs/luka-generative-lm-ra4rWkzy-py3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3626491fc4c5431fa190782577c5af72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/pypoetry/virtualenvs/luka-generative-lm-ra4rWkzy-py3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.half).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7134f0bb-1ee4-4508-a26d-5326ea96562b",
      "metadata": {
        "id": "7134f0bb-1ee4-4508-a26d-5326ea96562b",
        "outputId": "bb9be6eb-fde6-429f-b5fd-3930c268b9d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello how are you?\n",
            "- I'm good, thanks for asking. How about you?\n",
            "- I'm doing well,\n",
            "============\n",
            "hello how are you?\") and the user's response. It then uses a simple if-else statement to determine whether\n",
            "============\n",
            "hello how are you?')\n",
            "    # print('Hello! How are you?')\n",
            "    # print('I\\'m\n",
            "============\n",
            "hello how are you doing today? I am doing well, thanks for asking. I have been busy with work and other\n",
            "============\n",
            "hello how are you?\"). The model is then fine-tuned on a downstream task, such as sentiment analysis or question\n",
            "============\n"
          ]
        }
      ],
      "source": [
        "text = \"hello how are you\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "for i in range(5):\n",
        "    print(tokenizer.decode(model.generate(**move_to_device(inputs, device), max_new_tokens=20, use_cache=True, do_sample=True, pad_token_id=tokenizer.eos_token_id)[0]))\n",
        "    print(\"====\" * 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd50470-64b9-4a21-8748-0e9c5ea439fc",
      "metadata": {
        "id": "3fd50470-64b9-4a21-8748-0e9c5ea439fc"
      },
      "source": [
        "Видим, что текст зачастую разламывается. Это потому что формат входных данных сильно отличается от того, что модель видела на обучении. У всех chat-моделей свой формат. Где-то он описан просто словами, где-то он заложен в токенайзер. Мы рассмотрим как раз такой случай - за нас есть удобно написанная функция `apply_chat_template`. Давайте используем ее, чтобы получить префикс для генерации модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec7ca96",
      "metadata": {
        "id": "fec7ca96",
        "outputId": "ecc36c9b-ef35-48d0-a370-6901efe9c685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a helpful assistant, who always helps user<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "How to learn about LLMs?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "You can always attend deepschool!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Thank you!<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "prefix = tokenizer.apply_chat_template(\n",
        "    conversation=\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant, who always helps user\"},\n",
        "        {\"role\": \"user\", \"content\": \"How to learn about LLMs?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"You can always attend deepschool!\"},\n",
        "        {\"role\": \"user\", \"content\": \"Thank you!\"},\n",
        "    ],\n",
        "    tokenize=False)\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79a3701-c80f-4b90-90bd-fa010e32ea36",
      "metadata": {
        "id": "e79a3701-c80f-4b90-90bd-fa010e32ea36"
      },
      "outputs": [],
      "source": [
        "prefix = tokenizer.apply_chat_template(\n",
        "    conversation=\n",
        "    [{\"role\": \"user\", \"content\": \"hello\"},\n",
        "     {\"role\": \"assistant\", \"content\": \"I'm good. How can I help you today\"},\n",
        "     {\"role\": \"user\", \"content\": \"I love you\"},\n",
        "    ],\n",
        "    tokenize=False,\n",
        "add_generation_prompt=True)\n",
        "\n",
        "reference = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "I'm good. How can I help you today<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "I love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "assert prefix.strip() == reference.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4284b18d-4f9b-4e7d-b3ea-bb365e90093c",
      "metadata": {
        "id": "4284b18d-4f9b-4e7d-b3ea-bb365e90093c",
        "outputId": "aeb93dc0-d46c-41b7-88ca-a4ee44b15c66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/pypoetry/virtualenvs/luka-generative-lm-ra4rWkzy-py3.8/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/root/.cache/pypoetry/virtualenvs/luka-generative-lm-ra4rWkzy-py3.8/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "I'm good. How can I help you today<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "I love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "That's so sweet! I'm happy to hear that.\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(prefix, return_tensors=\"pt\")\n",
        "print(tokenizer.decode(model.generate(**move_to_device(inputs, device), max_new_tokens=12, use_cache=True, do_sample=False, pad_token_id=tokenizer.eos_token_id)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72482f3-c296-46f3-851c-57b4f91a717b",
      "metadata": {
        "id": "a72482f3-c296-46f3-851c-57b4f91a717b"
      },
      "source": [
        "## Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52f422a9-c2ee-4c17-8aee-1830f1d143e6",
      "metadata": {
        "id": "52f422a9-c2ee-4c17-8aee-1830f1d143e6"
      },
      "source": [
        "Перед нами датасет MMLU - датасет вопросов и ответов в стиле multiple choice.\n",
        "* question - вопрос\n",
        "* choices - варианты ответа\n",
        "* answer - номер правильного ответа"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "530d1721-6623-4ca6-816c-d4f90203ceb2",
      "metadata": {
        "id": "530d1721-6623-4ca6-816c-d4f90203ceb2",
        "outputId": "50183c2c-6290-4959-e100-44866ecc6582"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'What was GDP per capita in the United States in 1850 when adjusting for inflation and PPP in 2011 prices?',\n",
              " 'subject': 'global_facts',\n",
              " 'choices': ['About $300', 'About $3k', 'About $8k', 'About $15k'],\n",
              " 'answer': 1}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "mmlu = load_dataset(\"cais/mmlu\", \"global_facts\", split=\"test\")\n",
        "mmlu[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc14ce5-267d-40da-b35a-193c60cc68ca",
      "metadata": {
        "id": "efc14ce5-267d-40da-b35a-193c60cc68ca"
      },
      "outputs": [],
      "source": [
        "def calc_acc(p, y):\n",
        "    assert len(p) == len(y)\n",
        "    return sum(pi == yi for pi, yi in zip(p, y)) / len(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "230aed9b-32af-4a9d-8615-c2a2ddb864b2",
      "metadata": {
        "id": "230aed9b-32af-4a9d-8615-c2a2ddb864b2"
      },
      "outputs": [],
      "source": [
        "y_true = [sample[\"answer\"] for sample in mmlu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dcbcde2-7948-409b-b40f-0544a7bf332d",
      "metadata": {
        "id": "1dcbcde2-7948-409b-b40f-0544a7bf332d"
      },
      "outputs": [],
      "source": [
        "def calculate_log_probs(model, inputs):\n",
        "    logits = model(**inputs).logits[0] # seq_len, vocab_size\n",
        "    logits = logits[:-1] # т.к. для последнего токена ничего не генерируем\n",
        "    probs = torch.log_softmax(logits, dim=1) # превращаем в вероятноси\n",
        "    targets = input_ids[0, 1:] # сдвигаем, т.к. 0й токен мы не предсказываем, а предсказывем сразу следующий, т.е. 1й\n",
        "    text_probs = torch.gather(probs, 1, targets.unsqueeze(1)).squeeze(1)\n",
        "    text_P = text_probs.sum().exp()\n",
        "    ppl = (-text_probs.mean()).exp()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dccb1953-cf28-4629-9786-4fb71c178ac8",
      "metadata": {
        "id": "dccb1953-cf28-4629-9786-4fb71c178ac8"
      },
      "source": [
        "Считаем вероятности по одному question и choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a873756-abb8-4454-b68b-86ee139b8bd9",
      "metadata": {
        "id": "9a873756-abb8-4454-b68b-86ee139b8bd9",
        "outputId": "f78d8168-9065-4714-c17e-c427477d4700"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:13,  7.48it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "preds_single = []\n",
        "all_single_logps = []\n",
        "for i, sample in tqdm(enumerate(mmlu)):\n",
        "    question = sample[\"question\"]\n",
        "    sample_logps = []\n",
        "    for choice in sample[\"choices\"]:\n",
        "        inputs = move_to_device(tokenizer(question + \" \" + choice, return_tensors=\"pt\"), device)\n",
        "        input_ids = inputs.input_ids\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits[0]\n",
        "            logits = logits[:-1] # т.к. для последнего токена ничего не генерируем\n",
        "            logps = torch.log_softmax(logits, dim=1) # превращаем в вероятноси\n",
        "            targets = input_ids[0, 1:] # сдвигаем, т.к. 0й токен мы не предсказываем, а предсказывем сразу следующий, т.е. 1й\n",
        "            text_logps = torch.gather(logps, 1, targets.unsqueeze(1)).squeeze(1)\n",
        "            sample_logps.append(text_logps.sum().item())\n",
        "    all_single_logps.append(sample_logps)\n",
        "    preds_single.append(np.argmax(sample_logps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba7c471-ce0d-4cdc-8640-ceb5d71248c9",
      "metadata": {
        "id": "fba7c471-ce0d-4cdc-8640-ceb5d71248c9",
        "outputId": "0ce8fbc7-f8c2-4147-f36a-9acf4f63e4d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.49\n"
          ]
        }
      ],
      "source": [
        "print(calc_acc(preds_single, y_true))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "254fd3d3-5d15-4fed-9738-f035c4d4c81f",
      "metadata": {
        "id": "254fd3d3-5d15-4fed-9738-f035c4d4c81f"
      },
      "source": [
        "Считаем все сразу"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9d7fe88-3062-4b03-87f2-0b190c47e762",
      "metadata": {
        "id": "a9d7fe88-3062-4b03-87f2-0b190c47e762",
        "outputId": "2a65cbe7-e666-42ab-9097-5b91a253c30c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 17760.43it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_prompts = []\n",
        "for sample in tqdm(mmlu):\n",
        "    question = sample[\"question\"]\n",
        "    sample_logps = []\n",
        "    for choice in sample[\"choices\"]:\n",
        "        all_prompts.append(question + \" \" + choice)\n",
        "len(all_prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a5e581f-9b36-4757-9061-39a262efd91a",
      "metadata": {
        "id": "8a5e581f-9b36-4757-9061-39a262efd91a"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be893b6e-a91c-45ff-bce5-be70a4a7a696",
      "metadata": {
        "id": "be893b6e-a91c-45ff-bce5-be70a4a7a696",
        "outputId": "fc1c5645-eb94-49d3-e2c9-beeecae86a11"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:06<00:00, 19.71it/s]\n"
          ]
        }
      ],
      "source": [
        "all_logps = []\n",
        "batch_size = 3\n",
        "for i in tqdm(range(len(all_prompts) // batch_size + 1)):\n",
        "    batch = all_prompts[i * batch_size: (i + 1) * batch_size]\n",
        "    inputs = move_to_device(tokenizer(batch, return_tensors=\"pt\", padding=\"longest\"), device)\n",
        "    input_ids = inputs.input_ids\n",
        "    mask = inputs.attention_mask\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits # batch_size, seq_len, d_model\n",
        "        logits = logits[:, :-1]\n",
        "        logps = torch.log_softmax(logits, dim=2)\n",
        "        targets = input_ids[:, 1:]\n",
        "        text_logps_vect = torch.gather(logps, 2, targets.unsqueeze(2)).squeeze(2) # batch_size, seq_len - 1\n",
        "        text_logps = text_logps_vect.masked_fill(~mask[:, 1:].bool(), 0).sum(dim=1)\n",
        "        all_logps += text_logps.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1bb06e-c20e-497e-bb2d-d612ae496e56",
      "metadata": {
        "id": "ac1bb06e-c20e-497e-bb2d-d612ae496e56",
        "outputId": "3e1c4387-3d3f-4f3d-f7db-a2b9c21fd142"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.49"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds_batched = torch.FloatTensor(all_logps).view(-1, 4)\n",
        "preds_batched = preds_batched.argmax(dim=1).tolist()\n",
        "calc_acc(preds_batched, y_true)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}