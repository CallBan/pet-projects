{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Практика: RAG"
      ],
      "metadata": {
        "id": "tIbPcR04pMHM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ZjF3yqwgO8"
      },
      "source": [
        "Цель данной практики — познакомить вас с концепцией и реализацией RAG в LLM, а также развить навыки интеграции механизмов поиска с языковыми моделями. Вы научитесь извлекать данные из специализированных источников, использовать их для поддержки генеративного процесса и оценивать качество полученных результатов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0uGRTx_xGI1"
      },
      "source": [
        "# Устанавливаем зависимости"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjgVt4aB7Enn"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyImHSqi7yrI"
      },
      "outputs": [],
      "source": [
        "!pip install datasets sentence_transformers==3.3.1 trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GSuCJ7owjgG"
      },
      "source": [
        "# Текст на чанки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsuU9wgFw-tr"
      },
      "source": [
        "Для наших заданий будем использовать датасет rag-dataset-12000, в котором есть вопросы, ответы и большие контексты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGEzuBQk7TgS"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "wikiq = load_dataset('neural-bridge/rag-dataset-12000')\n",
        "\n",
        "print(len(wikiq['train']))\n",
        "print(wikiq['train'][0]['question'], '\\n')\n",
        "print(wikiq['train'][0]['answer'], '\\n')\n",
        "print(wikiq['train'][0]['context'], '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8pN_pIXxxfh"
      },
      "source": [
        "Для налаживания процессов возьме 100 самых длинных текстов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrLSMxEjAmgE"
      },
      "outputs": [],
      "source": [
        "k_longest = 100\n",
        "train_data = sorted(wikiq['train'], key=lambda w: len(w[\"context\"]))[-k_longest:]\n",
        "\n",
        "train_full_docs = [elem['context'] for elem in train_data]\n",
        "train_queries = [elem['question'] for elem in train_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "807ZLp8Lx3Pr"
      },
      "source": [
        "Реализуйте класс Chunker, в котором метод split_text_to_chunks разбивает входный текст на чанки с перекрытием и возвращает список полученных чанков, и метод get_chunked_list, которые по списку текстов возвращает список чанков из этих текстов согласно методу split_text_to_chunks. Размер чанка и перекрытие измеряется в кол-ве слов, то есть chunk_words = 10 означает, что чанк состоит из 10 слов (слова - сущности, которые получаются после простого сплита строки по одному пробелу \" \"), перекрытие 3 означает, что если взять два соседний чанка, то 3 последних слова первого являются 3мя первыми словами второго чанка."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0AiV3BJ8yhB"
      },
      "outputs": [],
      "source": [
        "class Chunker:\n",
        "    def __init__(self, chunk_words: int = 100, overlap: int = 30):\n",
        "        # your code here\n",
        "\n",
        "    def split_text_to_chunks(self, text: str) -> list[str]:\n",
        "        # your code here\n",
        "\n",
        "    def get_chunked_list(self, texts: list[str]) -> list[str]:\n",
        "        # your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty39etCG_LEb"
      },
      "outputs": [],
      "source": [
        "chunker = Chunker(chunk_words=100, overlap=30)\n",
        "train_chunked_docs = chunker.get_chunked_list(train_full_docs)\n",
        "print(train_chunked_docs, '\\n')\n",
        "print(train_chunked_docs[0], '\\n')\n",
        "print(train_chunked_docs[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUgm3xEhyN-i"
      },
      "outputs": [],
      "source": [
        "print(len(train_chunked_docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_QlqbaGzQJ9"
      },
      "source": [
        "# Векторный поиск\n",
        "\n",
        "Построить простой векторный поиск на основе энкодерной модели modernbert-embed-base. Модель устроена так, что эмбеддинги запросов (всегда должны начинаться с префикса \"search_query: \") всегда близки в векторном пространстве эмбеддингам похожих/релевантных документов (их текст всегда должен начинаться на \"search_document: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLpAgEhdCBgG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "model = SentenceTransformer(\"nomic-ai/modernbert-embed-base\").to(device)\n",
        "\n",
        "query_embeddings = model.encode([\n",
        "    \"search_query: What is TSNE?\",\n",
        "    \"search_query: Who is Laurens van der Maaten?\",\n",
        "])\n",
        "doc_embeddings = model.encode([\n",
        "    \"search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\",\n",
        "])\n",
        "print(query_embeddings.shape, doc_embeddings.shape)\n",
        "# (2, 768) (1, 768)\n",
        "\n",
        "similarities = model.similarity(query_embeddings, doc_embeddings)\n",
        "print(similarities)\n",
        "# tensor([[0.7214],\n",
        "#         [0.3260]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eetvsCSnCEJI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, embed_model_name: str = \"nomic-ai/modernbert-embed-base\",\n",
        "                 device: str = \"cuda\"):\n",
        "        self.embed_model = SentenceTransformer(embed_model_name).to(device)\n",
        "\n",
        "    def encode_query(self, texts: list[str]) -> np.ndarray:\n",
        "        # тут энкодим запросы, не забываем про префикс\n",
        "\n",
        "    def encode_docs(self, texts: list[str]) -> np.ndarray:\n",
        "        # тут энкодим документы, не забываем про префикс\n",
        "\n",
        "    def similarity(self, *args, **kwargs):\n",
        "        # тут считаем косинусную близость\n",
        "\n",
        "\n",
        "class VectorSearchEngine:\n",
        "    def __init__(self,\n",
        "                 init_base: list[str],\n",
        "                 encoder: Encoder,\n",
        "                 ):\n",
        "        # тут строим векторный индекс по исходной базе init_base с помощью энкодера\n",
        "\n",
        "    def insert_doc(self, doc: str) -> None:\n",
        "        # тут добавляем в векторный индекс и в список документов новый документ\n",
        "\n",
        "    def get_k_most_similar(self, query: str, k: int) -> tuple:\n",
        "        # тут пытаемся оптимально найти k ближайших для query документов, вернуть список текстов этих доков в порядке убывания близости,\n",
        "        # а также соответствующими им похожести"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wYwh0XnIvcU"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOhrRLA2Hvuz"
      },
      "outputs": [],
      "source": [
        "vse = VectorSearchEngine(train_chunked_docs[:3], encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbx4FwFuCEXk"
      },
      "outputs": [],
      "source": [
        "vse.get_k_most_similar(train_queries[0], k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKKPYiR3CD33"
      },
      "outputs": [],
      "source": [
        "train_queries[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4disQ6j17BS"
      },
      "source": [
        "# Генератор\n",
        "\n",
        "Реализуйте метод generate генерации языковой модели, который по списку запросов выдает список текстовый ответов. **kwargs должны пойти как аргументы в model.generate при генерации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK5-2GVFLjCP"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8JBk3N8MSNn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class Generator:\n",
        "    def __init__(self, model_name: str = \"Qwen/Qwen2-0.5B\", device: str = \"cuda\" ):\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=\"auto\",\n",
        "            device_map=\"auto\"#=device\n",
        "        ).to(device)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def generate(self, inputs: list[str],\n",
        "                 **kwargs) -> str:\n",
        "        #  your code here\n",
        "\n",
        "def template_text(query: str, doc: str) -> str:\n",
        "    return f\"# Document: {doc}\\n\\n# Question: {query}\\n\\n# Answer: \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9-mzD5mNi4M"
      },
      "outputs": [],
      "source": [
        "generator = Generator(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbwHNEmmNh5U"
      },
      "outputs": [],
      "source": [
        "doc = \"Turunturundia is a captivating and enigmatic country located in a secluded part of the world, yet to be discovered by adventurous travelers. The landscape of Turunturundia is a harmonious blend of expansive emerald plains, lush forests, and towering mountains capped with eternal snow. Its people are known for their warm hospitality and celebrate a rich tapestry of cultural traditions passed down through generations. Turunturundia's history is steeped in legends and folklore, with ancient ruins and artifacts indicating a civilization that valued art, philosophy, and nature. The capital of Turunturundia's is Turuncity. The country is also home to several unique species of flora and fauna found nowhere else on Earth, making it a treasure trove for botanists and ecologists alike. Despite its modest size, Turunturundia's spirit and charm leave a lasting impression on all who have the privilege of exploring its wonders.\"\n",
        "query = \"what is the capital of turunturundia?\"\n",
        "templated_example = template_text(query, doc)\n",
        "\n",
        "generator.generate([templated_example], max_new_tokens=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzzH87rd3LKu"
      },
      "source": [
        "# Генерация гипотез"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duoKuZsF2gLZ"
      },
      "source": [
        "функция template_text готовит вход в нужном формате. Реализуйте функцию create_hypotheses, которая по заданному входу генерирует n_candidates + 1 генераций:\n",
        "* n_candidates с помощью семплинга с заданной температурой\n",
        "* 1 кандидат - greedy генерация (без семлинга, макс вероятность каждого токена)\n",
        "и возвращает в формате: (гриди генерация, список семлпинг генераций)\n",
        "\n",
        "Семлпинг и его параметры можно найти в документации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zv_OA7QOgY6"
      },
      "outputs": [],
      "source": [
        "def create_hypotheses(templated_example: str,\n",
        "                      generator: Generator,\n",
        "                      temperature: float = 0.5,\n",
        "                      max_new_tokens: int = 512,\n",
        "                      n_candidates: int = 10) -> (str, list[str]):\n",
        "    # your code here\n",
        "\n",
        "doc = \"Turunturundia is a captivating and enigmatic country located in a secluded part of the world, yet to be discovered by adventurous travelers. The landscape of Turunturundia is a harmonious blend of expansive emerald plains, lush forests, and towering mountains capped with eternal snow. Its people are known for their warm hospitality and celebrate a rich tapestry of cultural traditions passed down through generations. Turunturundia's history is steeped in legends and folklore, with ancient ruins and artifacts indicating a civilization that valued art, philosophy, and nature. The capital of Turunturundia's is Turuncity. The country is also home to several unique species of flora and fauna found nowhere else on Earth, making it a treasure trove for botanists and ecologists alike. Despite its modest size, Turunturundia's spirit and charm leave a lasting impression on all who have the privilege of exploring its wonders.\"\n",
        "query = \"what is the capital of turunturundia?\"\n",
        "templated_example = template_text(query, doc)\n",
        "\n",
        "greedy_example, hypotheses = create_hypotheses(templated_example, generator)\n",
        "greedy_example, hypotheses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12WRV5xy35Bt"
      },
      "source": [
        "# Reward Model\n",
        "\n",
        "Возьмем из открытого доступа реворд-модель, которая обучалась понимать, какой из 2 ответов лучше, и будем использовать ее pointwise - для оценки одного ответа. Будем скорить всех сгенерированных на один запрос кандидатов и брать максимальный по скору. А пока просто посмотрим на ее ранжирующие свойства для кандидатов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SU1PS4j6NLt"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "class RewardModel:\n",
        "    def __init__(self,\n",
        "                 model_name: str = \"OpenAssistant/reward-model-deberta-v3-large-v2\",\n",
        "                 device: str = \"cuda\"\n",
        "                 ):\n",
        "        rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(model_name), AutoTokenizer.from_pretrained(model_name)\n",
        "        self.rank_model = rank_model.to(device)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def get_score(self, q: str, ans: str) -> float:\n",
        "        inputs = self.tokenizer(q, ans, return_tensors='pt').to(self.rank_model.device)\n",
        "        score = self.rank_model(**inputs).logits[0].cpu().detach()\n",
        "        return score\n",
        "\n",
        "\n",
        "reward_model = RewardModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eeldu-a7Lvg"
      },
      "outputs": [],
      "source": [
        "doc = \"Turunturundia is a captivating and enigmatic country located in a secluded part of the world, yet to be discovered by adventurous travelers. The landscape of Turunturundia is a harmonious blend of expansive emerald plains, lush forests, and towering mountains capped with eternal snow. Its people are known for their warm hospitality and celebrate a rich tapestry of cultural traditions passed down through generations. Turunturundia's history is steeped in legends and folklore, with ancient ruins and artifacts indicating a civilization that valued art, philosophy, and nature. The capital of Turunturundia's is Turuncity. The country is also home to several unique species of flora and fauna found nowhere else on Earth, making it a treasure trove for botanists and ecologists alike. Despite its modest size, Turunturundia's spirit and charm leave a lasting impression on all who have the privilege of exploring its wonders.\"\n",
        "query = \"what is the capital of turunturundia?\"\n",
        "templated_example = template_text(query, doc)\n",
        "\n",
        "greedy_example, hypotheses = create_hypotheses(templated_example, generator)\n",
        "\n",
        "ranks = []\n",
        "for ans in hypotheses:\n",
        "    score = reward_model.get_score(query, ans)\n",
        "    ranks.append((score, query, ans))\n",
        "\n",
        "sorted_ranks = sorted(ranks, key=lambda w: w[0])\n",
        "\n",
        "for elem in sorted_ranks:\n",
        "    print(elem, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r7ffYGT-p_n"
      },
      "source": [
        "# Всё вместе и RS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR_XPQ9h8J2j"
      },
      "outputs": [],
      "source": [
        "k_longest = 100\n",
        "train_data = sorted(wikiq['train'], key=lambda w: len(w[\"context\"]))[-k_longest:]\n",
        "\n",
        "train_full_docs = [elem['context'] for elem in train_data]\n",
        "train_queries = [elem['question'] for elem in train_data]\n",
        "\n",
        "chunker = Chunker(chunk_words=100, overlap=30)\n",
        "train_chunked_docs = chunker.get_chunked_list(train_full_docs)\n",
        "\n",
        "encoder = Encoder(device=\"cpu\")\n",
        "vse = VectorSearchEngine(train_chunked_docs, encoder)\n",
        "\n",
        "generator = Generator()\n",
        "reward_model = RewardModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfOefA9o_7Hq"
      },
      "source": [
        "На основе решений выше соберите pandas датафрейм, в котором каждая строка будет состоять из запроса, контекста, одной из генераций (гриди или семплинг), типа генерации (гриди или семплинг) и значения реворда для данного ответа на данный запрос.\n",
        "Такие результаты должны быть получены для всех запросов из train_queries.\n",
        "В семплинг генерациях возьмите 10 кандидатов с дефолтной температурой, макс длина контекста 512 токенов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baHKPDzv8WJj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dct = {\n",
        "    \"query\" : [],\n",
        "    \"ctx\": [],\n",
        "    \"generation\": [],\n",
        "    \"type\": [],\n",
        "    \"score\": []\n",
        "}\n",
        "\n",
        "# your code here\n",
        "\n",
        "df = pd.DataFrame(dct)\n",
        "df['idx'] = np.arange(df.shape[0])\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0S3nEIc-M8z"
      },
      "outputs": [],
      "source": [
        "print(df[df[\"type\"] == \"greedy\"][\"score\"].mean())\n",
        "# среднее значение ревордов генератор без дообучения. После дообучения оно аналогичная статистика должна стать больше"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a55mBbe-GIFv"
      },
      "source": [
        "Напишите логику для Rejection Sampling: сохраните в best_score_df только те строки из df, в которых для запроса и контекста выбрана гипотеза с максимальным скором реворда. Посчитайте среднее значение реворда в такой выборке и сравните со среднем значениям только по гриди генерациям."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVO-MiknAjzy"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "# your code here\n",
        "\n",
        "best_score_df = pd.DataFrame(rows)\n",
        "\n",
        "print(best_score_df[\"score\"].mean())\n",
        "best_score_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "759ZUyMMAVot"
      },
      "source": [
        "# Обучение\n",
        "\n",
        "Попробуем улучшить наш генератор с помощью данных, полученных на предыдущем шаге. Весь пайплайн выглядит так:\n",
        "1. Мы сгенерировали ряд гипотез\n",
        "2. Мы оценили ответы с помощью reward модели\n",
        "3. Мы берем лучшие ответы для того, чтобы обучить на них модель\n",
        "\n",
        "Таким образом мы получим новый датасет, на котором сможем обучиться.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNmekKDEpHxu"
      },
      "outputs": [],
      "source": [
        "# создаем датасет в нужном формате\n",
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    return template_text(example[\"query\"], example[\"ctx\"]) + example[\"generation\"].strip()\n",
        "\n",
        "dataset_raw = [{\"text\": formatting_prompts_func(sample)} for _, sample in best_score_df.iterrows()]\n",
        "print(*dataset_raw[:3], sep=\"\\n--------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0u5dkdkpHxv"
      },
      "source": [
        "Дальше написан код обучения, подробнее, что как работает мы разберем на следующей лекции! Сейчас вам нужно проставить следующие параметры:\n",
        "* learning rate 2e-4\n",
        "* число шагов обучения или число эпох\n",
        "* агрументы для сохранения чекпоинта (save_strategy, save_steps...)\n",
        "\n",
        "Все аргументы описаны в [документации](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbPcvkmD18n6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "model = generator.model.float()\n",
        "tokenizer = generator.tokenizer\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "dataset = Dataset.from_list(dataset_raw)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "args = SFTConfig(\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    warmup_steps = 5,\n",
        "\n",
        "\n",
        "    # Заполните этот блок аргументов\n",
        "    # num_train_epochs = 1, # 1 эпоха = 1 полный проход по данным\n",
        "    # max_steps = 60, # сколько шагов обучения сделать\n",
        "    save_strategy=...,\n",
        "    save_interval=...,\n",
        "    learning_rate = # ваш код здесь,\n",
        "    ############\n",
        "\n",
        "    fp16 = True,\n",
        "    logging_steps = 1,\n",
        "    optim = \"adamw_hf\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 3407,\n",
        "    output_dir = \"outputs\",\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 512,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    report_to=None,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    args = args\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKqikNppHxv"
      },
      "source": [
        "После того, как вы обучили свою модель, давайте проверим наш пайплайн еще раз, посчитаем среднюю награду наших генераций, она должна была увеличиться"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wf95o_mpHxv"
      },
      "outputs": [],
      "source": [
        "# ваш код здесь"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlSWKD8o1NUe"
      },
      "source": [
        "# Расширение запроса\n",
        "\n",
        "Зачастую пользователи предоставляют нам неподробный запрос и хочется его переписать или расширить для лучшего поиска по базе данных. Мы рассмотрим самый простой вариант расширения запроса - давайте добавим в запрос синонимов к каждому слову! Для этого нам поможет wordnet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zXbLuNVpHxw"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "synsets = wordnet.synsets('dog')\n",
        "for sn in synsets[:3]:\n",
        "    for lemma in sn.lemmas()[:4]:\n",
        "        print(lemma.name().replace(\"_\", \" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V-5iXVOpHxw"
      },
      "source": [
        "Ваша задача дописать функцию expand_query: она должна проходиться по всем словам из текста и добавлять по одному синониму на каждое слово в запрос. Посмотрите, как поменяется близость между расширенным query и documents по сравнению с обычнм query и documents!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhhUZkLBpHxw"
      },
      "outputs": [],
      "source": [
        "def expand_query(query: str) -> str:\n",
        "    words = query.split()\n",
        "    # ваш код здесь\n",
        "    pass\n",
        "\n",
        "documents = [\n",
        "    \"The Eiffel Tower is a landmark in Paris, France.\",\n",
        "    \"Paris is the capital of France and known for its art, fashion, and culture.\",\n",
        "    \"France has a rich history, including revolutions and world wars.\",\n",
        "    \"The Louvre Museum in Paris holds many famous artworks, including the Mona Lisa.\"\n",
        "]\n",
        "query = \"Paris landmarks\"\n",
        "model = encoder.embed_model\n",
        "# ваш код здесь"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}