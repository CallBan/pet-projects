{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33c44059",
      "metadata": {
        "id": "33c44059"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import List, Dict\n",
        "\n",
        "import requests\n",
        "from together import Together\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782f4847",
      "metadata": {
        "id": "782f4847"
      },
      "source": [
        "# Предобработка входных данных\n",
        "\n",
        "\n",
        "В данном задании мы будем ходить в онлайн модель. Для получения моего API-ключа пишем мне в лс! Посмотрим, как походы в API соотносятся с тем, что мы делали в практике в шаге \"LLM в индустрии\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ca6131",
      "metadata": {
        "id": "c3ca6131"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3.1-70B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c702b9d",
      "metadata": {
        "id": "6c702b9d"
      },
      "source": [
        "## Ручное форматирование промпта"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0ad05b7",
      "metadata": {
        "id": "f0ad05b7"
      },
      "source": [
        "Давайте попробуем собрать вход для llama3.1 руками, для этого допишем функцию `format_messages_to_prompt`.\n",
        "Она принимает messages - массив словарей, где указаны роли и текст сообщений, а возвращает она текст в формате, который нужно подать модели.\n",
        "\n",
        "Например для истории сообщений\n",
        "\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Some system message\"},\n",
        "    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n",
        "]\n",
        "```\n",
        "\n",
        "должен выдаваться итоговый промпт\n",
        "\n",
        "```text\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Some system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "This is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "this is a mesage from the assistant<|eot_id|>\n",
        "```\n",
        "\n",
        "Что важно:\n",
        "1. Текст начинается со спецтокена bos\n",
        "2. Дальше идет заголовок start_header_id + end_header_id, которые содержат роль\n",
        "3. Дальше после \\n\\n идет текст, заканчивающийся на eot_id\n",
        "4. Дальше следующий заголовок с новой ролью и т.д.\n",
        "\n",
        "**Важно** - в данной функции нельзя использовать `tokenizer.apply_chat_template`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0fb161e",
      "metadata": {
        "id": "f0fb161e"
      },
      "outputs": [],
      "source": [
        "def format_messages_to_prompt(messages: List[Dict[str, str]]) -> str:\n",
        "    builder = \"\"\n",
        "    builder += tokenizer.bos_token\n",
        "    for message in messages:\n",
        "        role = message[\"role\"]\n",
        "        content = message[\"content\"]\n",
        "        builder += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n\"\n",
        "        builder += content + \"<|eot_id|>\"\n",
        "    return builder\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Some system message\"},\n",
        "    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "reference_text = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Some system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "This is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "this is a mesage from the assistant<|eot_id|>\"\"\"\n",
        "\n",
        "\n",
        "assert format_messages_to_prompt(messages) == reference_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a536107",
      "metadata": {
        "id": "3a536107"
      },
      "source": [
        "Мы также помним, что раньше у нас была `tokenizer.apply_chat_template`. Т.к. у нас неофициальный форк llama3.1, то chat_template в токенайзер нам не завезли, поэтому придется добавить его руками"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e5db895",
      "metadata": {
        "id": "3e5db895"
      },
      "outputs": [],
      "source": [
        "chat_template = \"\"\"\n",
        "{{- bos_token }}\n",
        "{%- if custom_tools is defined %}\n",
        "    {%- set tools = custom_tools %}\n",
        "{%- endif %}\n",
        "{%- if not tools_in_user_message is defined %}\n",
        "    {%- set tools_in_user_message = true %}\n",
        "{%- endif %}\n",
        "{%- if not date_string is defined %}\n",
        "    {%- set date_string = \"26 Jul 2024\" %}\n",
        "{%- endif %}\n",
        "{%- if not tools is defined %}\n",
        "    {%- set tools = none %}\n",
        "{%- endif %}\n",
        "\n",
        "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
        "{%- if messages[0]['role'] == 'system' %}\n",
        "    {%- set system_message = messages[0]['content']|trim %}\n",
        "    {%- set messages = messages[1:] %}\n",
        "{%- else %}\n",
        "    {%- set system_message = \"\" %}\n",
        "{%- endif %}\n",
        "\n",
        "{#- System message + builtin tools #}\n",
        "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
        "{%- if builtin_tools is defined or tools is not none %}\n",
        "    {{- \"Environment: ipython\\n\" }}\n",
        "{%- endif %}\n",
        "{%- if builtin_tools is defined %}\n",
        "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
        "{%- endif %}\n",
        "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
        "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
        "{%- if tools is not none and not tools_in_user_message %}\n",
        "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
        "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
        "    {{- \"Do not use variables.\\n\\n\" }}\n",
        "    {%- for t in tools %}\n",
        "        {{- t | tojson(indent=4) }}\n",
        "        {{- \"\\n\\n\" }}\n",
        "    {%- endfor %}\n",
        "{%- endif %}\n",
        "{{- system_message }}\n",
        "{{- \"<|eot_id|>\" }}\n",
        "\n",
        "{#- Custom tools are passed in a user message with some extra guidance #}\n",
        "{%- if tools_in_user_message and not tools is none %}\n",
        "    {#- Extract the first user message so we can plug it in here #}\n",
        "    {%- if messages | length != 0 %}\n",
        "        {%- set first_user_message = messages[0]['content']|trim %}\n",
        "        {%- set messages = messages[1:] %}\n",
        "    {%- else %}\n",
        "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
        "{%- endif %}\n",
        "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
        "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
        "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
        "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
        "    {{- \"Do not use variables.\\n\\n\" }}\n",
        "    {%- for t in tools %}\n",
        "        {{- t | tojson(indent=4) }}\n",
        "        {{- \"\\n\\n\" }}\n",
        "    {%- endfor %}\n",
        "    {{- first_user_message + \"<|eot_id|>\"}}\n",
        "{%- endif %}\n",
        "\n",
        "{%- for message in messages %}\n",
        "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
        "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
        "    {%- elif 'tool_calls' in message %}\n",
        "        {%- if not message.tool_calls|length == 1 %}\n",
        "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
        "        {%- endif %}\n",
        "        {%- set tool_call = message.tool_calls[0].function %}\n",
        "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
        "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
        "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
        "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
        "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
        "                {%- if not loop.last %}\n",
        "                    {{- \", \" }}\n",
        "                {%- endif %}\n",
        "                {%- endfor %}\n",
        "            {{- \")\" }}\n",
        "        {%- else  %}\n",
        "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
        "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
        "            {{- '\"parameters\": ' }}\n",
        "            {{- tool_call.arguments | tojson }}\n",
        "            {{- \"}\" }}\n",
        "        {%- endif %}\n",
        "        {%- if builtin_tools is defined %}\n",
        "            {#- This means we're in ipython mode #}\n",
        "            {{- \"<|eom_id|>\" }}\n",
        "        {%- else %}\n",
        "            {{- \"<|eot_id|>\" }}\n",
        "        {%- endif %}\n",
        "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
        "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
        "        {%- if message.content is mapping or message.content is iterable %}\n",
        "            {{- message.content | tojson }}\n",
        "        {%- else %}\n",
        "            {{- message.content }}\n",
        "        {%- endif %}\n",
        "        {{- \"<|eot_id|>\" }}\n",
        "    {%- endif %}\n",
        "{%- endfor %}\n",
        "{%- if add_generation_prompt %}\n",
        "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
        "{%- endif %}\n",
        "\"\"\".strip()\n",
        "tokenizer.chat_template = chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18ea72a3",
      "metadata": {
        "id": "18ea72a3"
      },
      "source": [
        "## Автоматическая сборка промпта"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32fe9b9f",
      "metadata": {
        "id": "32fe9b9f"
      },
      "source": [
        "Давайте вспомним теперь на деле, как используется chat_template! Попробуем использовать функцию `tokenizer.apply_chat_template`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e493aae",
      "metadata": {
        "id": "7e493aae"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Some system message\"},\n",
        "    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fee33f0b",
      "metadata": {
        "id": "fee33f0b"
      },
      "outputs": [],
      "source": [
        "reference_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Cutting Knowledge Date: December 2023\n",
        "Today Date: 26 Jul 2024\n",
        "\n",
        "Some system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "This is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "this is a mesage from the assistant<|eot_id|>\"\"\"\n",
        "\n",
        "assert prompt == reference_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed104b62",
      "metadata": {
        "id": "ed104b62"
      },
      "source": [
        "Обратите внимание, что в заданном chat_template указаны Cutting Knowledge Date, т.е. до данные до какого периода видела модели, и Today Date - захардкоженная дата текущего диалога.\n",
        "\n",
        "**Вопрос, обязательно напишите свой ответ здесь!**\n",
        "На что влияет аргумент `add_generation_prompt` в функции `tokenizer.apply_chat_template`? Зачем его использовать?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ab24edd",
      "metadata": {
        "id": "8ab24edd"
      },
      "source": [
        "## Походы в API\n",
        "\n",
        "Теперь давайте посмотрим, как можно ходить в API. Вообще говоря различных провайдеров много, API у них у всех очень похожий, т.к. все мимикрируют под OpenAI. Собственно, за апишкой – ко мне!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f3931a",
      "metadata": {
        "id": "a3f3931a"
      },
      "outputs": [],
      "source": [
        "# Вставьте свой ключ из https://api.together.ai/\n",
        "API_KEY = \"PASTE YOUR KEY HERE\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5df9d33",
      "metadata": {
        "id": "d5df9d33"
      },
      "source": [
        "Есть несколько способов сходить в API. Можно ходить напрямую через библиотеку **requests**. Допишите post запрос в `url` с данными `data` и заголовками `headers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9f97ed",
      "metadata": {
        "id": "9e9f97ed"
      },
      "outputs": [],
      "source": [
        "headers = {\n",
        "    'Authorization': 'Bearer ' + API_KEY,\n",
        "    'Content-Type': 'application/json',\n",
        "}\n",
        "url = \"https://api.together.xyz/v1/chat/completions\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n",
        "]\n",
        "\n",
        "data = {\n",
        "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "    \"messages\": messages\n",
        "}\n",
        "\n",
        "\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "model_answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "assert \"london\" in model_answer.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8efb521",
      "metadata": {
        "id": "a8efb521"
      },
      "source": [
        "Мы подали messages, дальше они каким-то образом собрались в promt и подались модели. Мы не знаем, какой промпт используется на стороне провайдера. Вспомним про Today Date из предыдущего пункта задания - использует ли его together? Обновляют ли они его сегодняшним днем или оставляют Today Date? Если обновляют, то по какому часовому поясу?\n",
        "\n",
        "Чтобы ответы на эти и многие другие вопросы не мучали нас по ночам, можно использовать prompt формат, а именно подать модели текст напрямую на генерацию. Давайте для этого используем `tokenizer.apply_chat_template`. Модель будет принимать текст ровно так, как вы его подадите, без каких-либо предобработок. Подумайте, нужно ли вам использовать аргумент `add_generation_prompt`?\n",
        "\n",
        "Чтобы послать запрос напрямую, нужно в предыдущем запросе убрать messages, который представляет из себя список словарей, и послать поле prompt - строку с промптом для модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "772bd506",
      "metadata": {
        "id": "772bd506"
      },
      "outputs": [],
      "source": [
        "headers = {\n",
        "    'Authorization': 'Bearer ' + API_KEY,\n",
        "    'Content-Type': 'application/json',\n",
        "}\n",
        "url = \"https://api.together.xyz/v1/chat/completions\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template ...\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "    \"prompt\": prompt\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "model_answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "assert \"london\" in model_answer.lower() and \"assistant\" not in model_answer.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9789bff9",
      "metadata": {
        "id": "9789bff9"
      },
      "source": [
        "## Клиент\n",
        "\n",
        "Теперь мы понимаем общую схему взаимодействия с провайдером - они предоставляют апи, куда можно посылать или промтп или историю диалога. При посылке промпта вся ответственность за формат ложится на нас, при посылке messages форматтинг происходит на стороне провайдера, но мы не всегда представляем, как он работает. Выбор в пользу того или иного варианта всегда остается на вас.\n",
        "\n",
        "Мы использовали выше библиотеку requests, чтобы послать HTTP-запрос на сервера together, однако есть способ и проще - python client. Давайте познакомимся с ним поближе. Для этого давайте используем функцию `client.chat.completions.create`. Также давайте добавим опции сэмплинга, которые в этой функции поддержаны. Их можно посылать и в запросах через requests, но мы здесь и далее будем пользоваться клиентом.\n",
        "* top_k = 100\n",
        "* temperature = 0.5\n",
        "* top_p = 0.9\n",
        "* repetition_penalty = 1.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88dc9341",
      "metadata": {
        "id": "88dc9341"
      },
      "outputs": [],
      "source": [
        "client = Together(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b88e3ef8",
      "metadata": {
        "id": "b88e3ef8"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=messages,\n",
        "    temperature=0.5,\n",
        "    top_k=100,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.05,\n",
        ")\n",
        "\n",
        "response_text = response ...# Ваш код здесь\n",
        "assert \"london\" in response_text.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c0c4a4c",
      "metadata": {
        "id": "1c0c4a4c"
      },
      "source": [
        "Аналогично посылать просто prompt можно через `client.completions.create`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08d5758f",
      "metadata": {
        "id": "08d5758f"
      },
      "source": [
        "## Tools\n",
        "\n",
        "Давайте теперь посмотрим, как можно использовать tools в связке с моделями. У нас есть функция, которая входит в базу данных и получает информацию о юзере. Базы данных, конечно же, у нас никакой нет, но у нас есть некоторая функция, которая эмулирует это поведение, так что давайте попробуем ее описать.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd5090a",
      "metadata": {
        "id": "2dd5090a"
      },
      "outputs": [],
      "source": [
        "def get_user_info_from_db(person_name: str) -> Dict[str, str]:\n",
        "    database = {\n",
        "        \"ilya\": {\n",
        "            \"job\": \"Software Developer\",\n",
        "            \"pets\": \"dog\",\n",
        "        },\n",
        "        \"farruh\": {\n",
        "            \"job\": \"Senior Data & Solution Architect\",\n",
        "            \"hobby\": \"travelling, hiking\",\n",
        "        },\n",
        "        \"timur\": {\n",
        "            \"job\": \"DeepSchool Founder\",\n",
        "            \"city\": \"Novosibirsk\",\n",
        "        }\n",
        "    }\n",
        "    no_info = {\"err\": f\"No info about {person_name}\"}\n",
        "    return database.get(person_name.lower(), no_info)\n",
        "\n",
        "print(get_user_info_from_db(\"Timur\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf7e8052",
      "metadata": {
        "id": "bf7e8052"
      },
      "source": [
        "Давайте попробуем описать эту функцию в формате json, чтобы модель могла ее увидеть!\n",
        "Заполните поля в определении дальше"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbb3a3cf",
      "metadata": {
        "id": "dbb3a3cf"
      },
      "outputs": [],
      "source": [
        "get_user_info_from_db_tool = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_user_info_from_db\",\n",
        "        \"description\": \"This function returns a json of information about the user\", # Напишите, что функция делает своими словами\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"person_name\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Name of the person to look for.\" # Напишите название аргумента\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"person_name\"] # укажите обязательные аргументы для функции\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e9f5bd6",
      "metadata": {
        "id": "5e9f5bd6"
      },
      "source": [
        "Теперь давайте подадим это описание в `tokenizer.apply_chat_template`. Обратите внимание на его аргумент `tools`! Не забудьте `add_generation_prompt`, если он нужен."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad9aaf2",
      "metadata": {
        "id": "0ad9aaf2"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, tools=[get_user_info_from_db_tool], add_generation_prompt=True)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e980a9be",
      "metadata": {
        "id": "e980a9be"
      },
      "source": [
        "Давайте пошлем наш запрос в модель. На выбор 2 модели, если не будет работать с 8b, то предлагается посылать в 70b.\n",
        "Для данного запроса для 8b был подобран работающий `seed=9706540181089681000`, который можно подать в функцию.\n",
        "\n",
        "Давайте воспользуемся `client.completions.create` для генерации ответа от модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25e080b5",
      "metadata": {
        "id": "25e080b5"
      },
      "outputs": [],
      "source": [
        "model_8b = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
        "model_70b = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2cdaebf",
      "metadata": {
        "scrolled": true,
        "id": "b2cdaebf"
      },
      "outputs": [],
      "source": [
        "response_8b = client.completions.create(model=model_8b, prompt=prompt, seed=9706540181089681000)\n",
        "response_70b = client.completions.create(model=model_70b, prompt=prompt)\n",
        "\n",
        "print(response_8b.choices[0].text)\n",
        "print(response_70b.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b870a67",
      "metadata": {
        "id": "3b870a67"
      },
      "source": [
        "Если все хорошо, то мы получили ответ от модели, который выглядит как некоторый структурированный вывод, который можно использовать для вызова модели. Давайте попробуем написать функцию, которая принимает ответ модели в \"сыром виде\", выбирает, какую функцию с какими аргументами вызвать.\n",
        "\n",
        "Здесь нам поможет FUNCTION_REGISTRY и то, что параметры в функцию можно передавать как словарь, например так\n",
        "```python\n",
        "def foo(a, b, c):\n",
        "    print(a, b, c)\n",
        "\n",
        "obj = {'b':10, 'c':'lee'}\n",
        "\n",
        "foo(100, **obj)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92feaf07",
      "metadata": {
        "id": "92feaf07"
      },
      "outputs": [],
      "source": [
        "FUNCTION_REGISTRY = {\"get_user_info_from_db\": get_user_info_from_db}\n",
        "# На случай, если модель не генерит function call\n",
        "reference_answer = \"\"\"{\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}\"\"\"\n",
        "\n",
        "\n",
        "def parse_function_call(model_answer):\n",
        "    # 1. Проверим, является ли это function call.\n",
        "    try:\n",
        "        call = json.loads(reference_answer)\n",
        "    except:\n",
        "        return None\n",
        "    # 2. Вызов нужной функции с указанными аргументами\n",
        "    return FUNCTION_REGISTRY[call[\"name\"]](**call[\"parameters\"])\n",
        "\n",
        "\n",
        "assert parse_function_call(reference_answer) == get_user_info_from_db(\"Ilya\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed947f2",
      "metadata": {
        "id": "eed947f2"
      },
      "source": [
        "Теперь давайте попробуем объединить все это в историю диалога и сгенерировать моделью финальный ответ.\n",
        "Для этого в messages, где хранится наша история диалога нужно добавить\n",
        "1. Вызов function call моделью с ролью ХХХ (это часть задания, напишите сами)\n",
        "2. Ответ function call с ролью tool\n",
        "\n",
        "После этого данный промпт нужно послать модели снова, чтобы получить финальный ответ.\n",
        "Для этого опять используем `tokenizer.apply_chat_template` и `client.completions.create`.\n",
        "\n",
        "В зависимости от модели может понадобиться убрать tools (на 8b, 70b должна справиться). Для 8b опять же подобран seed=2017684582943914000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7e6d47",
      "metadata": {
        "id": "5e7e6d47"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
        "]\n",
        "messages.append({\"role\": \"assistant\", \"content\": reference_answer})\n",
        "messages.append({\"role\": \"tool\", \"content\": json.dumps(parse_function_call(reference_answer))})\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, tools=[get_user_info_from_db_tool])\n",
        "# print(prompt)\n",
        "\n",
        "response_8b = client.completions.create(model=model_70b, prompt=prompt, temperature=0.8)\n",
        "print(response_8b.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93ce7746",
      "metadata": {
        "id": "93ce7746"
      },
      "source": [
        "Теперь давайте посмотрим на chat-API, как обрабатываются function calls там?\n",
        "Используем для этого уже знакомый `client.chat.completions.create`, обратим внимание на аргумент tools внутри него. Здесь рекомендуется использовать 70b модель. На всякий случай работающий seed=14157400267283583000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae78906",
      "metadata": {
        "id": "bae78906"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
        "]\n",
        "response = client.chat.completions.create(model=model_70b, messages=messages, tools=[get_user_info_from_db_tool])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9e2dd36",
      "metadata": {
        "id": "f9e2dd36"
      },
      "source": [
        "Мы можем видеть, что у нас не работает предыдущий подход с полем `content`, однако должно было появиться поле `tool_calls`, которое содержит в себе информацию о вызове инструмента"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c84279",
      "metadata": {
        "scrolled": true,
        "id": "13c84279"
      },
      "outputs": [],
      "source": [
        "response.choices[0].message.tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e907c917",
      "metadata": {
        "id": "e907c917"
      },
      "source": [
        "# Использование библиотек\n",
        "\n",
        "Теперь, когда мы руками прошли весь пути обработки function call можно посмотреть уже на готовые инструменты.\n",
        "Мы много чего сделали руками:\n",
        "1. Писали описание функции\n",
        "2. Обрабатывали ответ\n",
        "3. Вызывали функцию\n",
        "4. Возвращали все это в модель\n",
        "\n",
        "Давайте теперь посмотрим, как оно работает в библиотеках!\n",
        "\n",
        "**NB** - библиотеки развиваются и вполне, возможно, что к концу роадмапа те интерфейсы, которые мы используем в этом домашнем задании будут уже неактуальны, но я уверен, что знаний и принципов, полученных из этих заданий хватит, чтобы адаптироваться к будущим вызовам!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d89c627",
      "metadata": {
        "scrolled": true,
        "id": "6d89c627"
      },
      "outputs": [],
      "source": [
        "# ! pip install langchain==0.2.16 llama-index-core==v0.11.16 langchain-together==0.2.0 llama-index-llms-together==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51b964f9",
      "metadata": {
        "id": "51b964f9"
      },
      "source": [
        "# LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef83e32",
      "metadata": {
        "id": "5ef83e32"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_core.tools import tool"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a7b08ae",
      "metadata": {
        "id": "0a7b08ae"
      },
      "source": [
        "Давайте ознакомимся с langchain-интеграцией together.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f75658",
      "metadata": {
        "id": "f7f75658"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TOGETHER_API_KEY\"] = API_KEY\n",
        "\n",
        "\n",
        "llm = ChatTogether(\n",
        "    model=model_70b,\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d548c4e",
      "metadata": {
        "id": "9d548c4e"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
        "]\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d698905",
      "metadata": {
        "id": "1d698905"
      },
      "source": [
        "Теперь, когда мы разобрались, как базово работать с langchain, давайте попробуем добавить инструментов. Чтобы нам было не так скучно, давайте напишем новую функцию, которая считает \"волшебную операцию\".\n",
        "\n",
        "Эта функция принимает 2 строки, возвращает строку строку b в обратном порядке, сконкатенированную со строкой a. Допишите эту функцию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6976bf",
      "metadata": {
        "id": "bb6976bf"
      },
      "outputs": [],
      "source": [
        "def magic_operation(a, b):\n",
        "    return b[::-1] + a\n",
        "\n",
        "assert magic_operation(\"456\", \"321\") == \"123456\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89726ec1",
      "metadata": {
        "id": "89726ec1"
      },
      "source": [
        "Теперь давайте обернем эту функцию в декоратор tool из langchain, аннотируем типы и допишем docstring. После этого можно будет автоматически сгенерировать описани функции в function call формате!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "482c8b38",
      "metadata": {
        "id": "482c8b38"
      },
      "outputs": [],
      "source": [
        "@tool # декоратор\n",
        "def magic_operation_tool(a: str, b: str): # аннотации типов\n",
        "    \"\"\"\n",
        "    This function takes two strings, reverses the second one and concatenates to it the first string.\n",
        "    It returns the concatenation as a result\n",
        "    \"\"\" # docstring\n",
        "    return magic_operation(a, b)\n",
        "\n",
        "print(magic_operation_tool.args_schema.schema())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a60cf8b",
      "metadata": {
        "id": "5a60cf8b"
      },
      "source": [
        "Теперь давайте попробуем подать запрос в нашу LLM и обогатить ее нашим function_call. Для этого нужна функция `llm.bind_tools`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c465f4",
      "metadata": {
        "id": "f6c465f4"
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools([magic_operation_tool])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0db362d",
      "metadata": {
        "id": "d0db362d"
      },
      "source": [
        "Теперь давайте как и раньше:\n",
        "1. Сгенерируем ответ на messages\n",
        "2. Проверим в ответе resp.tool_calls, вызовем нужный инструмент\n",
        "3. Расширим messages ответом модели и ответом инструмента, сгенерируем финальный ответ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b15a3c3f",
      "metadata": {
        "id": "b15a3c3f"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`\"}\n",
        "]\n",
        "resp = llm_with_tools.invoke(messages)\n",
        "messages.append(resp)\n",
        "\n",
        "messages.append(magic_operation_tool.invoke(resp.tool_calls[0]))\n",
        "magic_operation_tool.invoke(resp.tool_calls[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a20e11",
      "metadata": {
        "id": "f9a20e11"
      },
      "outputs": [],
      "source": [
        "assert len(messages) == 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d462fa5",
      "metadata": {
        "id": "0d462fa5"
      },
      "outputs": [],
      "source": [
        "res = llm.invoke(messages).content\n",
        "assert \"123456\" in res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3feb1a5b",
      "metadata": {
        "id": "3feb1a5b"
      },
      "source": [
        "# LlamaIndex\n",
        "\n",
        "Аналогичный инструмент LlamaIndex. В ней не так хороша поддержка function calls не для OpenAI, поэтому придется забежать вперед и использовать ReActAgent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e6485db",
      "metadata": {
        "id": "9e6485db"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.together import TogetherLLM\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.agent import ReActAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfef1b75",
      "metadata": {
        "id": "cfef1b75"
      },
      "outputs": [],
      "source": [
        "llm = TogetherLLM(model=model_70b, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6ea9b3e",
      "metadata": {
        "id": "b6ea9b3e"
      },
      "source": [
        "Скопируйте magic_operation_tool из части с langchain сюда,  но без декоратора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b6a47a7",
      "metadata": {
        "id": "4b6a47a7"
      },
      "outputs": [],
      "source": [
        "def magic_operation_tool(a, b): # аннотации типов\n",
        "    \"\"\"\"\"\" # docstring\n",
        "    print(\"INSIDE FUNCTION CALL\")\n",
        "    return magic_operation(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bfc72e3",
      "metadata": {
        "id": "3bfc72e3"
      },
      "source": [
        "Мы можем аналогично создать инструмент с помощью `FunctionTool.from_defaults`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b50ff121",
      "metadata": {
        "id": "b50ff121"
      },
      "outputs": [],
      "source": [
        "magic_operation_tool_llamaindex = ...# Ваш код здесь\n",
        "print(magic_operation_tool_llamaindex.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a73306d2",
      "metadata": {
        "id": "a73306d2"
      },
      "source": [
        "Давайте создадим ReActAgent: ему нужно передать tools, llm, memory=None и verbose=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84236969",
      "metadata": {
        "id": "84236969"
      },
      "outputs": [],
      "source": [
        "agent = ReActAgent(tools=[magic_operation_tool_llamaindex], llm=llm, memory=None, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12a204d8",
      "metadata": {
        "id": "12a204d8"
      },
      "outputs": [],
      "source": [
        "text = \"Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`\"\n",
        "agent.chat(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "725419ad",
      "metadata": {
        "id": "725419ad"
      },
      "source": [
        "# Agents\n",
        "\n",
        "Настала пора сделать своего агента!\n",
        "Попробуем сделать финансового аналитика. Требования следующие:\n",
        "бот должен по запросу данных о какой-либо компании смотреть самые большие изменения цены ее акций за последний месяц, после чего бот должен объяснить, с какой новостью это связано.\n",
        "\n",
        "Предлагается не строить сложную систему с классификаторами, а отдать всю сложную работу агенту. Давайте посмотрим, какие API нам доступны.\n",
        "\n",
        "Первым делом получение котировок - для этого нам поможет библиотека yfinance. По названию компании и периоду отчетности можно посмотреть открывающие цены на момент открытия и закрытия биржи."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a00d2f0",
      "metadata": {
        "id": "5a00d2f0"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "\n",
        "stock = yf.Ticker(\"AAPL\") # посмотрим котировки APPLE\n",
        "df = stock.history(period=\"1mo\")\n",
        "df[[\"Open\", \"Close\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "026d2b6b",
      "metadata": {
        "id": "026d2b6b"
      },
      "source": [
        "Для поиска новостей нам поможет https://newsapi.org/\n",
        "Можно легко получить свой ключ за короткую регистрацию, дается 1000 запросов в день, каждый запрос может включать в себя ключевое слово и промежуток дат. По бесплатному апи ключу дается ровно 1 месяц, что нам подходит."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "639e4310",
      "metadata": {
        "id": "639e4310"
      },
      "outputs": [],
      "source": [
        "api_key = \"...\" # ваш API ключ здесь!\n",
        "api_template = \"https://newsapi.org/v2/everything?q={keyword}&apiKey={api_key}&from={date_from}\"\n",
        "\n",
        "articles = requests.get(api_template.format(keyword=\"Apple\", api_key=api_key, date_from=\"2025-01-25\")).json()\n",
        "\n",
        "for article in articles[\"articles\"]:\n",
        "    if article[\"title\"] != \"[Removed]\":\n",
        "        print(article[\"title\"])\n",
        "        print(article[\"description\"])\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32f75126",
      "metadata": {
        "id": "32f75126"
      },
      "source": [
        "Очень много статей заблокированы и имеют название `[Removed]`, нужно их отфильтровать. В оставшихся статьях будем брать только title (заголовок) и description (описание или краткий пересказ)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc9378f6",
      "metadata": {
        "id": "cc9378f6"
      },
      "source": [
        "Вам необходимо реализовать [ReAct Agent](https://react-lm.github.io/). Особенность этого агента заключается в том, что он вначале формирует мысль, а потом вызывает действие (function call) для достижения какой-либо цели.\n",
        "\n",
        "Что нужно сделать:\n",
        "1. Описать и реализовать function call для определения, в какой день была самая большая разница в цене акций в момент открытия и закрытия биржи. Функция получает один аргумент - название акций компании (например AAPL для Apple), а выдает словарь с 2мя полями: с датой максимальной разницы в ценах и самой разницей в ценах.\n",
        "2. Описать и реализовать function call для получения 5 релевантных новостей о компании. В качестве аргумента принимаются название компании и дата. Ваша задача - сходить в newsapi, получить новости и вернуть 5 случайных новостей, которые произошли не позже чем день торгов. Если новостей меньше 5, то верните столько, сколько получится.\n",
        "3. После этого агент должен вернуть ответ, в котором постарается аргументировать изменения в цене.\n",
        "\n",
        "\n",
        "Реализовывать агента можно любым удобным способом, в том числе взять готовые имплементации.\n",
        "1. [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/agent/react_agent/) - вдобавок можно посмотреть предыдущее задание, где он уже используется.\n",
        "2. [Langchain/Langgraph](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/#code)\n",
        "3. Написать полностью свою реализацию\n",
        "\n",
        "\n",
        "Не забудьте, что очень важно описать задачу в промпте: нужно сказать, какие цели у агента и что он должен сделать. У функций должны быть говорящие описания, чтобы LLM без лишних проблем поняла, какие есть функции и когда их использовать. По всем вопросам можно обращаться в наш телеграм-чат в канал \"Tools & Agents\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2b0dac5",
      "metadata": {
        "id": "a2b0dac5"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.together import TogetherLLM\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.agent import ReActAgent\n",
        "\n",
        "model_70b = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"\n",
        "llm = TogetherLLM(model=model_70b, api_key=API_KEY)\n",
        "\n",
        "from typing import Tuple, List\n",
        "def get_day_with_max_diff(company_stock_name: str) -> Tuple[str, float]:\n",
        "    \"\"\"\n",
        "    This function takes a company_stock_name which is a stock name\n",
        "    and returns two values:\n",
        "    1. A date when the difference between the closing and opening price was the largest\n",
        "    2. The difference between closing and opening price\n",
        "    \"\"\"\n",
        "    stock = yf.Ticker(company_stock_name)\n",
        "    df = stock.history(period=\"1mo\")\n",
        "    df[\"diff\"] = df[\"Close\"] - df[\"Open\"]\n",
        "    idxm = df[\"diff\"].idxmax()\n",
        "    row = df.loc[idxm]\n",
        "    return row.name.date().strftime(\"%Y-%m-%d\"), row[\"diff\"]\n",
        "\n",
        "\n",
        "from random import sample\n",
        "def get_news(company_name: str, date: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    This function takes company_name, searches the news for the company\n",
        "    and then returns the news for the date `date`.\n",
        "    \"\"\"\n",
        "    api_template = \"https://newsapi.org/v2/everything?q={keyword}&apiKey={api_key}&from={date_from}\"\n",
        "    articles = requests.get(api_template.format(keyword=company_name, api_key=api_key, date_from=date)).json()[\"articles\"]\n",
        "\n",
        "    good_articles = list(filter(lambda article: article[\"title\"] != \"[Removed]\", articles))\n",
        "    good_articles = list(filter(lambda article: article[\"publishedAt\"].startswith(date), good_articles))\n",
        "\n",
        "    k = min(5, len(good_articles))\n",
        "    sampled_articles = sample(good_articles, k=k)\n",
        "    return [article[\"title\"] + \"\\n\" + article[\"description\"] for article in sampled_articles]\n",
        "\n",
        "\n",
        "agent = ReActAgent(tools=[\n",
        "    FunctionTool.from_defaults(fn=get_day_with_max_diff),\n",
        "    FunctionTool.from_defaults(fn=get_news)\n",
        "], llm=llm, memory=None, verbose=True)\n",
        "\n",
        "agent.chat(\"Can you check when was the biggest difference in Apple (AAPL) stock and explain to me what caused it?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}