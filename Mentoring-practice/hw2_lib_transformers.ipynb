{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "11861b12",
      "metadata": {
        "id": "11861b12"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "# можете сменить на mps на макбуке, но лично у меня он криво работает\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3",
      "metadata": {
        "id": "ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3"
      },
      "source": [
        "# Знакомство с Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3df5693",
      "metadata": {
        "id": "a3df5693"
      },
      "source": [
        "## Создание модели и предсказание следующего токена\n",
        "Нужно создать модель через `AutoModelForCausalLM`, создать токенайзер через `AutoTokenizer` и получить следующий токен через жадную генерацию!\n",
        "\n",
        "Для загрузки модели и токенайзера вам помогут функции `.from_pretrained`\n",
        "\n",
        "**Внимание** на каких-то из функций далее у вас может кончаться видеопамять из-за хранения активаций. Чтобы этого не происходило рекомендуется все вычисления оборачивать в контекстный менеджер `with torch.no_grad()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ec7e08b",
      "metadata": {
        "id": "6ec7e08b"
      },
      "outputs": [],
      "source": [
        "model_name = \"openai-community/gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name) \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "03f244e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"This is a sample text\"\n",
        "\n",
        "# Нужно преобразовать text с помощью tokenizer() и подать это в model.forward() (он же просто model())\n",
        "# после этого мы получим logits [batch_size = 1, seq_len, d_model]\n",
        "# По этому тензору нужно предсказать следующее слово!\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=inputs[\"input_ids\"])\n",
        "\n",
        "logits = outputs[\"logits\"][0,-1]\n",
        "next_token_idx: int = logits.argmax()\n",
        "\n",
        "\n",
        "next_token = tokenizer.decode([next_token_idx])\n",
        "\n",
        "assert next_token.strip() == \"file\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6809813",
      "metadata": {
        "id": "e6809813"
      },
      "source": [
        "## Используем Generate\n",
        "\n",
        "Мы с вами помним про различные виды сэмплинга - top_k, top_p, temperature,frequency penalty.\n",
        "Отличная новость заключается в том, что нам не нужно все это писать самим! Оно уже включено в [GenerationMixin](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#generation), от которого наследуются модели для генерации текста.\n",
        "\n",
        "Для генерации есть функция [generate](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "\n",
        "Ваша задача написать для модели выше генерацию по тексту с:\n",
        "* Температурой - 0.9\n",
        "* Top-K - 20\n",
        "* Repetition Penalty (Frequency Penalty) - 1.2\n",
        "* максимальное число новых токенов - 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a6b62dbf",
      "metadata": {
        "id": "a6b62dbf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "text = \"This is still a sample text, but\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "results = []\n",
        "for i in range(10):\n",
        "    gens = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_k=20,\n",
        "        repetition_penalty=1.2,\n",
        "        max_new_tokens=10\n",
        "    )\n",
        "    generation: str = tokenizer.decode(gens[0])\n",
        "    results.append(generation)\n",
        "\n",
        "assert len(set(results)) > 1, \"Все генерации получились одинаковыми, проверьте опции генерации и флаг do_sample!\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b90512b-9420-45b3-9f4c-22fb5fa1bfc7",
      "metadata": {
        "id": "8b90512b-9420-45b3-9f4c-22fb5fa1bfc7"
      },
      "source": [
        "## Generate Batched\n",
        "Теперь давайте жадно сгенерируем текст, но забатчуем несколько сэмплов. До этого мы всегда генерировали по батчу размера 1, поэтому у нас не было паддингов!\n",
        "\n",
        "Когда появляется несколько текстов разной длины, то появляются и паддинги.\n",
        "\n",
        "Представим себе ситуцию, что у нас батч из двух элементов длины 2 и 5 (токен -1 будет выступать в качестве паддинга **только для удобства визуализации**).\n",
        "\n",
        "Тогда\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [3, 2, -1, -1, -1]\n",
        "    [5, 6,  7,  1,  2]\n",
        "]\n",
        "attention_mask = [\n",
        "    [1, 1, 0, 0, 0],\n",
        "    [1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "Представим, что мы сгенерировали еще один токен, тогда\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [3, 2, -1, -1, -1, 7]\n",
        "    [5, 6,  7,  1,  2, 8]\n",
        "]\n",
        "attention_mask = [\n",
        "    [1, 1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "Получается, что у нас паддинги в маске возникают посередине. Мы не будем заниматься реализацией своего алгоритма генерации здесь, но отметим, что добавление паддинга слева значительно упрощает этот процесс.\n",
        "Тогда исходная последовательность будет:\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [-1, -1, -1, 3, 2]\n",
        "    [ 5,  6,  7, 1, 2]\n",
        "]\n",
        "attention_mask = [\n",
        "    [0, 0, 0, 1, 1],\n",
        "    [1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "и после генерации следующего токена\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [-1, -1, -1, 3, 2, 7]\n",
        "    [ 5,  6,  7, 1, 2, 8]\n",
        "]\n",
        "attention_mask = [\n",
        "    [0, 0, 0, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "В качестве задания давайте соберем батч с левым паддингом и проверим, что жадная генерация (10 токенов) совпадает с генерацией на текстах по отдельности!\n",
        "\n",
        "Для этого нам придется использовать параметр padding_side в конструкторе токенизатора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1",
      "metadata": {
        "id": "5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer # ваш код здесь\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd38bdc-3e5e-400d-8815-e9c08a757c03",
      "metadata": {
        "id": "1bd38bdc-3e5e-400d-8815-e9c08a757c03",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "texts = [\"This is a sample text\", \"I'm really tired and this is just about\"]\n",
        "\n",
        "# Внимание! В данном задании нужна жадная генерация!\n",
        "\n",
        "# Соберите оба текста в один батч и положите результаты генерации в\n",
        "# batched_generations\n",
        "batched_generations: List[str] = []\n",
        "\n",
        "....\n",
        "\n",
        "# Пройдитесь по каждому сэмплу по отдельности и положите результаты генерации\n",
        "# в single_generations\n",
        "single_generations: List[str] = []\n",
        "\n",
        "...\n",
        "\n",
        "assert len(batched_generations) == 2 and len(single_generations) == 2\n",
        "for s, b in zip(batched_generations, single_generations):\n",
        "    assert s == b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "igeljNIzCdy5",
      "metadata": {
        "id": "igeljNIzCdy5"
      },
      "source": [
        "# KV Cache\n",
        "При генерации есть опция use_cache - это использование KV cache для генерации.\n",
        "В рамках этой техники в при генерации в декодере считается только аттеншн последнего токена по всем векторам предыдущих токенов, которые посчитали на предыдущих этапах, а для \"старых\" (левых) токенов аттеншн не пересчитывается, т.к. \"новые\" (правые) токены на них не влияют.\n",
        "\n",
        "\n",
        "\n",
        "В рамках данного задания нужно:\n",
        "1. Посчитать скорость генерации 100 токенов с и без kv cache, сказать, какая техника и во сколько раз быстрее.\n",
        "2. Подсчитать скорость генерации 1 токена с и без kv cache, сказать, какая техника быстрее и почему.\n",
        "\n",
        "Чтобы корректно сравнивать время генерации нужно использовать жадный сэмплинг!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CTuWlGMfCj9h",
      "metadata": {
        "id": "CTuWlGMfCj9h"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "text = \"\"\"\n",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem justo, semper dignissim ipsum vitae, sollicitudin aliquet eros. Duis id ultricies erat. Vivamus commodo auctor massa ut mollis. Maecenas lacinia tempus orci, imperdiet ullamcorper felis accumsan et. Etiam mattis neque diam, at egestas nunc eleifend id. Fusce tristique orci nec sollicitudin elementum. Nullam dui est, feugiat ac pellentesque at, posuere non massa.\n",
        "\n",
        "Suspendisse accumsan ullamcorper dolor sed dictum. Mauris quis varius felis, quis gravida odio. Vestibulum diam arcu, aliquet convallis congue non, rutrum non turpis. Fusce vel orci ac diam suscipit lacinia. Curabitur maximus orci a dui gravida, accumsan convallis libero ornare. Phasellus dapibus, sapien pulvinar lacinia dictum, massa lacus scelerisque tellus, eu porta dolor eros vitae ex. Maecenas maximus, urna id pharetra dictum, dolor lorem sollicitudin ipsum, sit amet vestibulum orci felis quis leo. Pellentesque vel ligula ut urna eleifend condimentum nec et sem. Integer ligula nunc, rutrum ultricies urna et, congue suscipit lectus.\n",
        "\"\"\".strip()\n",
        "\n",
        "for use_cache in (True, False):\n",
        "  times = []\n",
        "  for _ in range(10):\n",
        "    start = time.time()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    inputs = move_to_device(inputs, device)\n",
        "    model.generate(...) # допишите параметры model.generate() для пункта 1 (генерация 100 токенов)\n",
        "    times.append(time.time() - start)\n",
        "  print(f\"{'with' if use_cache else 'without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hYhqzpfJDBb_",
      "metadata": {
        "id": "hYhqzpfJDBb_"
      },
      "outputs": [],
      "source": [
        "for use_cache in (True, False):\n",
        "  times = []\n",
        "  for _ in range(20):\n",
        "    start = time.time()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    inputs = move_to_device(inputs, device)\n",
        "    model.generate(...) # допишите параметры model.generate(...) для пункта 2 (генерация 1 токена)\n",
        "    times.append(time.time() - start)\n",
        "  print(f\"{'with' if use_cache else 'without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5da008c-3653-40d5-89ba-cd831352fd3d",
      "metadata": {
        "id": "f5da008c-3653-40d5-89ba-cd831352fd3d"
      },
      "source": [
        "# Скоринг, Perplexity\n",
        "\n",
        "Можно не только генерировать текст. Вспомним, что выдает после lm_head - вектор `[batch_size, seq_len, vocab_size]`, где для каждый вектор `[vocab_size]` это распределение вероятностей по следующему токену!\n",
        "\n",
        "Опустим размерность batch_size=1 для удобства, seq_len = 4. Пусть у нас есть текст `bos мама мыла раму` (`bos` спецсимвол для начала текста)\n",
        "\n",
        "Тогда вероятность этого текста расписывается через произведение условных вероятностей:\n",
        "\n",
        "```\n",
        "P(bos мама мыла раму) = P(мама | bos) * P(мыла | bos мама) * P(раму| bos мама мыла)\n",
        "```\n",
        "\n",
        "Т.е. это вероятность слова при условии его левого контекста.\n",
        "Зачастую ее обозначают как $P(x_i|x_{<i})$ где $x_i$ - i-е слово, $x_{<i}$ - контекст $[x_1, x_2, x_3, ... x_{i-1}]$\n",
        "Эти вероятности можно взять из выходного вектора!\n",
        "\n",
        "Давайте попробуем подсчитать вероятность и perplexity текстов!\n",
        "perplexity как и вероятность мера того насколько модель \"уверена\" в тексте, т.е. насколько по оценки ее параметрами данный текст вероятен.\n",
        "\n",
        "$$Perplexity(X) = exp(-\\frac {1} {N} \\sum_{i}^{N} log P(x_i | x_{<i}))$$\n",
        "\n",
        "В этом задании нужно:\n",
        "1. Посчитать вероятность **text**\n",
        "2. Посчитать перплексию **text**\n",
        "\n",
        "Еще одна важная деталь:\n",
        "работать с вероятностями плохо. Т.к. вероятность представляет собой число от 0 до 1, то при перемножении десятков или даже сотен таких числе теряется точность!\n",
        "Для этого от произведения вероятностей берут логарифм и получают logprobs - логарифмы вероятностей. Их можно складывать, по свойству логарифма логарифм произведения равен произведению логарифма.\n",
        "\n",
        "$$ p = p_1 * p_2 * p_3 $$\n",
        "$$log(p) = log (p_1) + log (p_2) + log (p_3)$$\n",
        "$$exp(log (p)) = p = exp(log (p_1) + log (p_2) + log (p_3)) = exp (log (p_1 * p_2 * p_3)) = p_1 * p_2 * p_3$$\n",
        "\n",
        "В pytorch для этого есть `torch.log_softmax`, который считается численно стабильно!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1c7ba39-a451-43a2-ac55-629c99259abe",
      "metadata": {
        "id": "e1c7ba39-a451-43a2-ac55-629c99259abe"
      },
      "outputs": [],
      "source": [
        "print(f\"Beginning of sentence (BOS) token = `{tokenizer.bos_token}`\")\n",
        "print(f\"End of sentence (EOS) token  = `{tokenizer.eos_token}`\")\n",
        "text = \"<|endoftext|>I'm so very tired of this<|endoftext|>\"\n",
        "\n",
        "inputs = tokenizer(text, ...)\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(...).logits\n",
        "    ...\n",
        "    # ваш код здесь!\n",
        "    # 1. Нужно обрезать logits по длине, т.к. для предсказаний по последнему токену нечего считать\n",
        "    # 2. Превращаем logits в log_probs\n",
        "    # 3. Берем вероятности следующих токенов, т.к. по вектору i-й позиции мы предсказываем токен на позиции (i + 1)\n",
        "    # для этого нам поможет torch.gather\n",
        "    # 4. Считаем вероятности и perplexity!\n",
        "\n",
        "\n",
        "# должно получиться что-то около 2.1783e-14 для вероятности и около 51 для ppl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddd5038-620b-48bb-bbc1-db3729141d78",
      "metadata": {
        "id": "5ddd5038-620b-48bb-bbc1-db3729141d78"
      },
      "source": [
        "# Chat-Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "599c7530-a7ce-4d23-abaf-2b0cec87301e",
      "metadata": {
        "id": "599c7530-a7ce-4d23-abaf-2b0cec87301e"
      },
      "source": [
        "# Формат\n",
        "Как мы обсуждали на лекции, все chat-модели принимают входы в своем особом формате.\n",
        "Он может быть описан текстом, а может быть заложен в шаблон, который доступен через `tokenizer.apply_chat_template`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5fe593-63a8-406d-9678-6d805c180670",
      "metadata": {
        "id": "7f5fe593-63a8-406d-9678-6d805c180670"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.half).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8dad11-6811-49ab-ad3d-8ac7de103828",
      "metadata": {
        "id": "7d8dad11-6811-49ab-ad3d-8ac7de103828"
      },
      "outputs": [],
      "source": [
        "def move_to_device(d):\n",
        "    for k, v in d.items():\n",
        "        d[k] = v.to(device)\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "503f15fd-576a-4e8e-917a-74df01a944f4",
      "metadata": {
        "id": "503f15fd-576a-4e8e-917a-74df01a944f4"
      },
      "source": [
        "Давайте посмотрим, как chat модель отработает на обычном тексте. Используйте для генерации сэмплинг и kv cache, выведите 5 результатов генерации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7134f0bb-1ee4-4508-a26d-5326ea96562b",
      "metadata": {
        "id": "7134f0bb-1ee4-4508-a26d-5326ea96562b"
      },
      "outputs": [],
      "source": [
        "text = \"hello how are you\"\n",
        "inputs = tokenizer(text, ...)\n",
        "\n",
        "for i in range(5):\n",
        "    # model.generate...\n",
        "    generated_text = ...\n",
        "    print(generated_text)\n",
        "    print(\"====\" * 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd50470-64b9-4a21-8748-0e9c5ea439fc",
      "metadata": {
        "id": "3fd50470-64b9-4a21-8748-0e9c5ea439fc"
      },
      "source": [
        "Видим, что текст зачастую выходит мусорный. Это потому что формат входных данных сильно отличается от того, что модель видела на обучении. У всех chat-моделей свой формат. Где-то он описан просто словами, где-то он заложен в токенайзер. Мы рассмотрим как раз такой случай - за нас есть удобно написанная функция `apply_chat_template`. Давайте используем ее, чтобы получить префикс для генерации модели.\n",
        "\n",
        "Не забудьте про опцию add_generation_prefix - она добавляет часть формата, после которой ожидается ответ модели!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79a3701-c80f-4b90-90bd-fa010e32ea36",
      "metadata": {
        "id": "e79a3701-c80f-4b90-90bd-fa010e32ea36"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"hello\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I'm good. How can I help you today\"},\n",
        "    {\"role\": \"user\", \"content\": \"I love you\"},\n",
        "]\n",
        "\n",
        "prefix = tokenizer.apply_chat_template(...)\n",
        "\n",
        "reference = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "I'm good. How can I help you today<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "I love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "assert prefix.strip() == reference.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048b8882-bec0-4bf4-b6fb-30e727d095c6",
      "metadata": {
        "id": "048b8882-bec0-4bf4-b6fb-30e727d095c6"
      },
      "source": [
        "Давайте посмотрим, что нам ответит модель!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4284b18d-4f9b-4e7d-b3ea-bb365e90093c",
      "metadata": {
        "id": "4284b18d-4f9b-4e7d-b3ea-bb365e90093c"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(prefix, ...)\n",
        "model.generate...\n",
        "print(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72482f3-c296-46f3-851c-57b4f91a717b",
      "metadata": {
        "id": "a72482f3-c296-46f3-851c-57b4f91a717b"
      },
      "source": [
        "## Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52f422a9-c2ee-4c17-8aee-1830f1d143e6",
      "metadata": {
        "id": "52f422a9-c2ee-4c17-8aee-1830f1d143e6"
      },
      "source": [
        "Перед нами датасет MMLU - датасет вопросов и ответов в стиле multiple choice.\n",
        "* question - вопрос\n",
        "* choices - варианты ответа\n",
        "* answer - номер правильного ответа"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "530d1721-6623-4ca6-816c-d4f90203ceb2",
      "metadata": {
        "id": "530d1721-6623-4ca6-816c-d4f90203ceb2",
        "outputId": "d192e19c-1dbe-4cf0-d44d-10c74e86c210"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'What was GDP per capita in the United States in 1850 when adjusting for inflation and PPP in 2011 prices?',\n",
              " 'subject': 'global_facts',\n",
              " 'choices': ['About $300', 'About $3k', 'About $8k', 'About $15k'],\n",
              " 'answer': 1}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "mmlu = load_dataset(\"cais/mmlu\", \"global_facts\", split=\"test\")\n",
        "mmlu[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fca61a91-5784-44f3-af9b-72250f8d58a4",
      "metadata": {
        "id": "fca61a91-5784-44f3-af9b-72250f8d58a4"
      },
      "source": [
        "Наша задача здесь решить задачу многоклассовой классификации.\n",
        "Для этого нужно посчитать\n",
        "$$P(choices_i | question)$$\n",
        "т.е. для посчитать вероятность каждого варианта ответа для вопроса. Мы это уже делали кодом выше!\n",
        "\n",
        "После этого давайте брать самый вероятный ответ и считать, что модель его выбрала.\n",
        "После этого давайте посчитаем accuracy, т.е. долю правильных ответов.\n",
        "Вместо вероятностей для подсчета лучше использовать logprobs.\n",
        "\n",
        "Итого, что нужно сделать:\n",
        "1. Пройтись по датасету, для каждого question и каждого из соответствующих choices получить самый вероятный ответ.\n",
        "2. Посчитать итоговый accuracy\n",
        "\n",
        "**Важно**\n",
        "1. Выше мы уже написали скоринг текста с помощью LLM, для этого задания можно адаптировать функцию.\n",
        "2. Если делаете варианты с батчеванием помните: длины choices могут быть разными! Нужно не считать вероятности по паддингам. В этом нам помогут attention_masks из выходов `tokenizer()`\n",
        "3. В данном задании для простоты мы игнорируем формат ответа llama3 и делаем скоринг по f\"{question} {answer}\"\n",
        "\n",
        "\n",
        "Попробуйте для начала написать вариант со скорингом для батча размера 1, а потом для батча размера 3 или 5. Код должен корректно работать для батча любого размера и выдавать одинаковую итоговую точность."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a4126e2-c463-404d-a3b5-5361f744242e",
      "metadata": {
        "id": "4a4126e2-c463-404d-a3b5-5361f744242e",
        "outputId": "a04220a7-15d9-48a0-d51d-8f240262e321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As of 2016, about what percentage of adults aged 18 years or older were overweight? 40%\n",
            "As of 2016, about what percentage of adults aged 18 years or older were overweight? 80%\n",
            "What was GDP per capita in the United States in 1850 when adjusting for inflation and PPP in 2011 prices? About $300\n",
            "What was GDP per capita in the United States in 1850 when adjusting for inflation and PPP in 2011 prices? About $3k\n"
          ]
        }
      ],
      "source": [
        "def sample_to_texts(sample):\n",
        "    return [sample[\"question\"] + \" \" + answer for answer in sample[\"choices\"]]\n",
        "\n",
        "all_samples_formatted = sum([sample_to_texts(sample) for sample in mmlu], [])\n",
        "print(*all_samples_formatted[2:6], sep=\"\\n\")\n",
        "# ваш код здесь!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c440af-dfc0-460d-a113-3fab3fefa361",
      "metadata": {
        "id": "76c440af-dfc0-460d-a113-3fab3fefa361"
      },
      "source": [
        "**Порефлексируйте над следующими вопросами**:\n",
        "1. Как влияет длина ответа на вероятность ответа при скоринге?\n",
        "2. Если к началу каждого ответа добавилить метки A) B) C) D) станет ли модель отвечать лучше или хуже? Стоит ли по-вашему добавлять эти метки?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
