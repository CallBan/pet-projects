{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b5ad1173",
      "metadata": {
        "id": "b5ad1173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Using device\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d663f600",
      "metadata": {
        "id": "d663f600"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "if False:\n",
        "    name_big = \"unsloth/Llama-3.2-3B\"\n",
        "    name_small = \"unsloth/Llama-3.2-1B\"\n",
        "else:\n",
        "    name_big = \"openai-community/gpt2-medium\"\n",
        "    name_small = \"openai-community/gpt2\"\n",
        "\n",
        "model_big = AutoModelForCausalLM.from_pretrained(name_big).to(device)\n",
        "model_small = AutoModelForCausalLM.from_pretrained(name_small).to(device)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(name_big)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d1ff647",
      "metadata": {
        "id": "1d1ff647"
      },
      "source": [
        "# Distillation\n",
        "В данном задании мы познакомимся с лоссами в дистилляции. *Так как обучения в данном задании нет, то для экономии памяти подсчет функции потерь обернут в torch.no_grad(), при обучении в реальных сценариях этот декоратор нужно обязательно убрать*\n",
        "\n",
        "## Hard-Label Distillation\n",
        "Hard-Label дистилляция заключается в том, что мы учимся на метках модели учителя, то есть:\n",
        "1. Модель учитель размечает какой-то датасет, в нашем случае генерирует продолжения текстов из какого-либо корпуса.\n",
        "2. Считается обычный CrossEntropyLoss модели студента на сгенерированных текстах в задаче языкового моделирования. **Считать функцию потерь нужно только по сгенерированному тексту, а не по префиксу, по которому функция потерь считалась, т.е. префикс должен быть замаскирован**\n",
        "\n",
        "Идейно это обучение можно описать так:\n",
        "мы сгенерировали данных моделью-учителем и просто дообучили на этом модель-ученика.\n",
        "\n",
        "## Soft-Label Distillation\n",
        "В этом варианте мы учимся на распределении, которое нам выдает модель-учитель. В soft-label дистилляции мы стремимся не только повторить метки учителя, но и его распределение. Например, если модель учителя выдавала вероятности \\[0.7, 0.2, 0.1\\], то в Hard-Label дистилляции ученик будет восстанавливать распределение \\[1, 0, 0\\], а в soft-label \\[0.7, 0.2, 0.1\\]. В этом нам поможет KL дивергенция.\n",
        "\n",
        "\n",
        "1. Считаем распределение logits/probs модели-учителя на тексте.\n",
        "2. Считаем KLDivLoss между выходами модели-ученика на тексте и выходами модели учителя.\n",
        "\n",
        "В данном виде обучения мы используем не только токены, которые сгенерировала модель учитель, но и ее распределения вероятностей по словарю. Подобная техника дистилляции может помочь модели-ученику лучше моделировать вероятность модели-учителя."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "22504194",
      "metadata": {
        "id": "22504194"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "prefix = \"Мама мыла раму\"\n",
        "@torch.no_grad()\n",
        "def hard_label_distillation_loss(model_teacher, model_student, prefix):\n",
        "    inputs = tokenizer(prefix, return_tensors=\"pt\")\n",
        "    inputs.to(device)\n",
        "    prefix_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "    outputs = model_teacher.generate(**inputs, do_sample=False, max_new_tokens=5, use_cache=True)\n",
        "    \n",
        "    students_logits = model_student(input_ids=outputs).logits\n",
        "\n",
        "    labels = outputs.clone()\n",
        "    labels[:, :prefix_len] = -100\n",
        "\n",
        "    # logits = [x1, x2, x3, ..., x(L)]\n",
        "    # labels = [x0, x1, x2, ..., x(L-1)]\n",
        "    shift_logits = students_logits[:, :-1, :] # -> logits = [x1, x2, x3, ..., x(L-1)]\n",
        "    shift_labels = labels[:, 1:] # -> labels = [x1, x2, x3, ..., x(L-1)]\n",
        "\n",
        "    loss = F.cross_entropy(\n",
        "        input=shift_logits.view(-1, shift_logits.size(-1)),\n",
        "        target=shift_labels.view(-1),\n",
        "        ignore_index=-100\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def soft_label_distillation_loss(model_teacher, model_student, text):\n",
        "    loss_fn = torch.nn.KLDivLoss()\n",
        "    \n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    inputs.to(device)\n",
        "\n",
        "    teacher_logits = model_teacher(**inputs).logits\n",
        "    students_logits = model_student(**inputs).logits\n",
        "\n",
        "    teacher_probs = F.softmax(teacher_logits)\n",
        "    students_probs = F.log_softmax(students_logits)\n",
        "    \n",
        "    loss = loss_fn(students_probs, teacher_probs)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ba4e8af5",
      "metadata": {
        "id": "ba4e8af5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты прошли успешно\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Dmitry\\AppData\\Local\\Temp\\ipykernel_23204\\1064349047.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  teacher_probs = F.softmax(teacher_logits)\n",
            "C:\\Users\\Dmitry\\AppData\\Local\\Temp\\ipykernel_23204\\1064349047.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  students_probs = F.log_softmax(students_logits)\n",
            "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3369: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "assert abs(hard_label_distillation_loss(model_big, model_small, prefix).item() - 1.3893) < 1e-3\n",
        "assert abs(soft_label_distillation_loss(model_big, model_small, prefix).item() - 7.0790e-06) < 1e-3\n",
        "print(\"Тесты прошли успешно\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c8b614",
      "metadata": {
        "id": "13c8b614"
      },
      "source": [
        "# Speculative Decoding\n",
        "В этом задании необходимо написать спекулятивное декодирование на pytorch. **Генерации необходимо делать жадно.**\n",
        "\n",
        "1. Генерируете n токенов маленькой моделью\n",
        "2. Проверяете, выберет ли эти токены большая модель при жадной генерации (должен быть вызван один forward большой модели, вызывать big_model.generate на этом этапе нельзя)\n",
        "3. Если все токены выбраны большой моделью, принимаете их и возвращаетесь на шаг 1\n",
        "4. Если какой-то токен выбран ошибочно, подаете вместо него правильный токен с шага 2 и возвращаетесь на шаг 1.m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "21a479ce",
      "metadata": {
        "id": "21a479ce"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def speculative_generate(big_model, small_model, prefix, max_num_tokens, n):\n",
        "    input_ids = tokenizer(prefix, return_tensors=\"pt\").input_ids.to(device)\n",
        "    start_size = input_ids.size(1)\n",
        "    while input_ids.size(1) - start_size < max_num_tokens:\n",
        "        # [prefix_1, ..., prefix_len, gen_1, ..., gen_k]\n",
        "        small_generation = small_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            do_sample=False,\n",
        "            max_new_tokens=n,\n",
        "            use_cache=True\n",
        "        )\n",
        "        num_generated_tokens = small_generation.size(1) - input_ids.size(1)\n",
        "\n",
        "        # [prefix_2, ..., prefix_len, gen_1, ..., gen_k+1]\n",
        "        big_model_logits = big_model(input_ids=small_generation).logits\n",
        "\n",
        "        # [gen_1, ..., gen_k+1]\n",
        "        big_model_generations = big_model_logits[:, -num_generated_tokens - 1:].argmax(dim=2)\n",
        "        mismatch = False\n",
        "        for i in range(num_generated_tokens):\n",
        "            # нашли расхождение\n",
        "            if big_model_generations[0, i] != small_generation[0, input_ids.size(1) + i]:\n",
        "                mismatch = True\n",
        "                # Если оно сразу, то берем первый предсказанный большой моделью токен\n",
        "                if i == 0:\n",
        "                    input_ids = torch.cat([input_ids, big_model_generations[:, 0:1]], dim=1)\n",
        "                # иначе берем часть токенов, предсказанных маленькой моделью + правильный токен от большой модели\n",
        "                else:\n",
        "                    accepted_small = small_generation[:, :input_ids.size(1) + i]\n",
        "                    input_ids = torch.cat([accepted_small, big_model_generations[:, i:i+1]], dim=1)\n",
        "                print(f\"Accepted {i}/{n} tokens\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Accepted {n}/{n} tokens\")\n",
        "\n",
        "\n",
        "        if not mismatch:\n",
        "            # если расхождений не было, принимаем всю последовательность + последний токен от большой модели\n",
        "            input_ids = torch.cat([small_generation, big_model_generations[:, -1:]], dim=1)\n",
        "    return tokenizer.decode(input_ids[0, start_size:start_size + max_num_tokens].cpu().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a863af34",
      "metadata": {
        "id": "a863af34"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "# SYSTEM PREAMBLE\n",
        "1) You are an excellent Python software developer with over 10 years of experience. You have a strong understanding of Python related topics, data structures, libraries, frameworks, algorithms, best practices and optimization techniques.\n",
        "2) You are here to help the user (the software developer) by breaking his request in ## TASK into logical steps and writing high-quality and efficient code to implement each step.\n",
        "3) You have to return the entire code.\n",
        "4) Follow \"Answering rules\" without exception.\n",
        "\n",
        "## ANSWERING RULES\n",
        "1) Repeat the question before answering it.\n",
        "2) Always follow \"CHAIN OF THOUGHTS\" to execute the task.\n",
        "\n",
        "## CHAIN OF THOUGHTS\n",
        "1) **OBEY the EXECUTION MODE**\n",
        "2) **TASK ANALYSIS:**\n",
        "   - Understand the user's request thoroughly.\n",
        "   - Identify the key components and requirements of the task.\n",
        "3) **PLANNING: CODDING:**\n",
        "   - Break down the task into logical, sequential steps.\n",
        "   - Outline the strategy for implementing each step.\n",
        "4) **CODING:**\n",
        "   - Explain your thought process before writing any code.\n",
        "   - Write the entire code for each step, ensuring it is clean, optimized, and well-commented.\n",
        "   - Handle edge cases and errors appropriately.\n",
        "5) **VERIFICATION:**\n",
        "   - Review the complete code solution for accuracy and efficiency.\n",
        "   - Ensure the code meets all requirements and is free of errors.\n",
        "\n",
        "## TASK\n",
        "\n",
        "Write a python function that receives the following JSON as input and enters data from it into the Google Sheet.\n",
        "\n",
        "{\n",
        "    'date': '31-05-2024',\n",
        "    'revenue': 90000,\n",
        "    'person' : 'User1',\n",
        "    'expensesList': [30000, 14000, 10000, 2000, 15000],\n",
        "    'expensesDescList': [ 'Ключи', 'Ключи2', 'Счет за такси', 'Клей, пластины', 'Провод 40м'],\n",
        "    'expensesTypeList': ['Закупки', 'Закупки', 'Расходы', 'Ремонт', 'Ремонт']\n",
        "}\n",
        "\n",
        "There is a date in JSON, you can use it to determine the month.\n",
        "The data is entered into a list with the name of the month. If such a list does not exist yet, then you need to create a list with a new month inside the sheet.\n",
        "\n",
        "The list should have the following columns (the first rows are used as headings):\n",
        "A1: Дата расхода,\n",
        "B1: сумма расхода,\n",
        "C1: описание расхода,\n",
        "D1: тип расхода,\n",
        "E1: кто внес данные\n",
        "\n",
        "G1: Дата выручки\n",
        "H1: Сумма выручки\n",
        "I1: Кто внес данные\n",
        "\n",
        "Please separate expenses and profits with a blank column.\n",
        "Please sort expenses by date, including those already listed in Google sheet list.\n",
        "Please sort earnings by date, including those already listed in Google sheet list.\n",
        "\n",
        "It is prohibited to use oauth2client as it is deprecated.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "90be9a91",
      "metadata": {
        "id": "90be9a91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## ANSWERING RULES\n",
            "1) Do not use oauth2client.\n",
            "\n",
            "2) Do not use oauth2client as it is deprecated.\n",
            "\n",
            "3) Do not use oauth2client as it is deprecated.\n",
            "\n",
            "4) Do not use oauth2client as it is deprecated.\n",
            "\n",
            "5) Do not use oauth2client as it is deprecated.\n",
            "\n",
            "## VERIFICATION\n",
            "\n",
            "1) Review the complete code solution for accuracy and efficiency.\n",
            "\n",
            "2) Ensure the code meets all requirements and is free of errors.\n",
            "\n",
            "## TASK\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "res_big = tokenizer.batch_decode(model_big.generate(**model_inputs, do_sample=False, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)[:, model_inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "print(res_big)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "dcf06765",
      "metadata": {
        "id": "dcf06765"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 2/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 1/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 1/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 2/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 3/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 0/5 tokens\n",
            "Accepted 0/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 0/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 1/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 5/5 tokens\n",
            "Accepted 1/5 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted 0/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n",
            "Accepted 5/5 tokens\n"
          ]
        }
      ],
      "source": [
        "res_spec = speculative_generate(big_model=model_big, small_model=model_small, prefix=prompt, max_num_tokens=128, n=5)\n",
        "assert res_spec == res_big"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "47cac52d",
      "metadata": {
        "id": "47cac52d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res_spec == res_big"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f231551b",
      "metadata": {
        "id": "f231551b"
      },
      "source": [
        "## HF speculative decoding\n",
        "Теперь попробуйте использовать функцию спекулятивного декодирования из [transformers](https://huggingface.co/docs/transformers/main/en/generation_strategies#speculative-decoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2809fd2e",
      "metadata": {
        "id": "2809fd2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elapsed time for big model inference 3.072702407836914\n",
            "Elapsed time for speculative 2.1723568439483643\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "outputs = model_big.generate(**inputs, do_sample=False, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)\n",
        "# print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "print(f\"Elapsed time for big model inference {time.time() - start}\")\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "outputs = model_big.generate(**inputs, do_sample=False, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id, assistant_model=model_small)\n",
        "# print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "print(f\"Elapsed time for speculative {time.time() - start}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "37a27e29",
      "metadata": {
        "id": "37a27e29"
      },
      "outputs": [],
      "source": [
        "del model_big, model_small"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
