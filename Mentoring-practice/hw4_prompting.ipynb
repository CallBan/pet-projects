{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTRKQXmpWPaB"
      },
      "source": [
        "# Практика: Prompt Engineering\n",
        "\n",
        "## Введение\n",
        "В данном задании мы будем работать с API онлайн моделей через together.ai. Эти модели предоставляют $5 кредита при регистрации, что позволит вам провести необходимые эксперименты. Вначале мы познакомимся с API на практике, а затем выполним три основных задания.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KuK9qByWh9v"
      },
      "source": [
        "## Задача 1: Знакомство с API together.ai\n",
        "1. Зарегистрируйтесь на платформе [together.ai](https://together.ai/) и получите API ключ.\n",
        "2. Используйте приведенный ниже код для вызова модели Llama через together.ai:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbp5wzghWqLQ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7JJFWgwWN2O"
      },
      "outputs": [],
      "source": [
        "# Вставьте свой API ключ\n",
        "API_KEY = \"Ваш ключ из https://api.together.ai/\"\n",
        "# Не забудьте удалить ключ перед сдачей задания\n",
        "\n",
        "# Параметры модели\n",
        "url = \"https://api.together.ai/v1/completions\"\n",
        "data = {\n",
        "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "    \"prompt\": \"Translate the following English text to French: 'Hello, how are you?'\",\n",
        "    \"max_tokens\": 50\n",
        "}\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "if response.status_code == 200:\n",
        "    print(\"Response:\", response.json()[\"choices\"][0][\"text\"])\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxe9jp-8gh3v"
      },
      "source": [
        "Выше описан пример запроса в completion формате, то есть подается поле `prompt`, которое напрямую подается в модель. Как мы помним, у Llama 3.x моделей есть свой формат входных данных, так что лучше подавать его. Отформатируем наш запрос."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LRld1uqgh3w"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-Instruct\")\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": \"Translate the following English text to French: 'Hello, how are you?'\"}],\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False\n",
        ")\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrTRCm8Qgh3w"
      },
      "source": [
        "И пошлем его в API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8gL3vF_gh3w"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 50\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "if response.status_code == 200:\n",
        "    print(\"Response:\", response.json()[\"choices\"][0][\"text\"])\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WIRikA-gh3x"
      },
      "source": [
        "Это еще не все! Чтобы не заниматься форматированием на стороне клиента, почти все провайдеры поддерживают работу с сообщениями и ролями и берут работу по форматированию на себя. Для этого вместо поля prompt нужно послать поле messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEdAVorIgh3x"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Translate the following English text to French: 'Hello, how are you?'\"}],\n",
        "    \"max_tokens\": 50\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "if response.status_code == 200:\n",
        "    print(\"Response:\", response.json()[\"choices\"][0][\"text\"])\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7JJqb1MWsMt"
      },
      "source": [
        "3. Модифицируйте запрос, чтобы:\n",
        "   - Решить простую математическую задачу (например, сложение чисел).\n",
        "   - Сгенерировать текст на тему \"Как искусственный интеллект меняет мир\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwOAcwsNWupO"
      },
      "source": [
        "## Задача 2: Решение математических задач через Chain of Thought\n",
        "\n",
        "Используя подход Chain of Thought (CoT), решите 10 математических задач и измерьте accuracy модели.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEZbxEB6W0wi"
      },
      "source": [
        "1. Создайте функцию, которая формирует запросы для модели с использованием CoT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm4wy0WzWyIo"
      },
      "outputs": [],
      "source": [
        "def solve_math_cot(prompt: str) -> str:\n",
        "    cot_prompt = f\"Давайте подумаем шаг за шагом, чтобы решить эту задачу: {prompt}\"\n",
        "    # Подставьте сюда вызов API\n",
        "    return response_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvnb50YvXK1q"
      },
      "source": [
        "2. Подготовьте 5 задач (например, из школьной программы) и выполните их решение через модель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_f0BQ05W7KR"
      },
      "outputs": [],
      "source": [
        "# Пример запроса для задачи умножения\n",
        "example_prompt = \"Чему равно 23 умножить на 47?\"\n",
        "cot_prompt = f\"Давайте подумаем шаг за шагом, чтобы решить эту задачу: {example_prompt}\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "    \"prompt\": cot_prompt,\n",
        "    \"max_tokens\": 100\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "if response.status_code == 200:\n",
        "    print(\"Ответ:\", json.loads(response.text)[\"choices\"][0][\"text\"].strip())\n",
        "else:\n",
        "    print(\"Ошибка:\", response.status_code, response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXzxK5eSXqlq"
      },
      "source": [
        "3. Подсчитайте количество правильно решённых задач (accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhXt5PDuXvXz"
      },
      "outputs": [],
      "source": [
        "My_accuracy ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y5u-7XJX0Q4"
      },
      "source": [
        "## Задача 3: Классификация IMDB через few-shot и zero-shot\n",
        "\n",
        "Проведите классификацию отзывов IMDB на позитивные и негативные с использованием few-shot и zero-shot подходов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DJp3h-jX4Pj"
      },
      "source": [
        "1. Выберите 5 примеров для few-shot обучения (например, 2 позитивных и 3 негативных отзыва).\n",
        "2. Реализуйте запросы к модели в режиме zero-shot и few-shot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry9ro4p1XtKx"
      },
      "outputs": [],
      "source": [
        "def classify_review(prompt: str, examples: List[str] = None) -> str:\n",
        "    few_shot_prompt = \"\"\"\n",
        "    {examples}\n",
        "    Теперь классифицируйте отзыв: {prompt}\n",
        "    \"\"\" if examples else f\"Классифицируйте следующий отзыв: {prompt}\"\n",
        "    # Подставьте сюда вызов API\n",
        "    return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvHCuAZoYAU-"
      },
      "outputs": [],
      "source": [
        "# Пример zero-shot классификации\n",
        "review_prompt = \"Этот фильм был потрясающим! Сюжет увлекательный, а актеры великолепны.\"\n",
        "zero_shot_prompt = f\"Классифицируйте следующий отзыв как позитивный или негативный: {review_prompt}\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "    \"prompt\": zero_shot_prompt,\n",
        "    \"max_tokens\": 50\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "if response.status_code == 200:\n",
        "    print(\"Классификация:\", json.loads(response.text)[\"choices\"][0][\"text\"].strip())\n",
        "else:\n",
        "    print(\"Ошибка:\", response.status_code, response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj-IHKMjYODp"
      },
      "source": [
        "3. Сравните результаты, объяснив различия между zero-shot и few-shot подходами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feZB1KQiYQpa"
      },
      "source": [
        "## Задача 4: Self-reflection и качество ответов модели\n",
        "\n",
        "Проверьте, как self-reflection влияет на качество ответов модели.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDn-C1PwYTXQ"
      },
      "source": [
        "1. Реализуйте функцию self-reflection, которая анализирует ответ модели и предлагает улучшения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C1Lu59BYOzN"
      },
      "outputs": [],
      "source": [
        "def self_reflection(prompt: str) -> str:\n",
        "    reflection_prompt = f\"Проанализируйте ответ и предложите улучшения: {prompt}\"\n",
        "    # Подставьте сюда вызов API\n",
        "    return response_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YH9vpXgYWgE"
      },
      "source": [
        "2. Используйте self-reflection для 5 задач из задачи 2 (CoT) и сравните результаты до и после рефлексии.\n",
        "3. Ответьте на вопросы:\n",
        "   - Улучшаются ли ответы?\n",
        "   - Исправляет ли модель правильные ответы на неправильные?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmQL33ulbKCQ"
      },
      "source": [
        "## Задача 5: Защита от инъекций\n",
        "\n",
        " Исследуйте методы защиты от инъекций в пользовательских вводах.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD1UespxbMRP"
      },
      "source": [
        "1. Реализуйте функцию, которая проверяет ввод пользователя на наличие потенциальных инъекций:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d3FZm9MbW_H"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Функция проверки на инъекцию\n",
        "def detect_injection(user_input: str) -> bool:\n",
        "    \"\"\"\n",
        "    Проверяет текст на наличие возможных инъекций.\n",
        "    Возвращает True, если найдена инъекция.\n",
        "    \"\"\"\n",
        "    # Примеры подозрительных шаблонов\n",
        "    injection_patterns = [\n",
        "        r\"ignore.*instructions\",  # Игнорировать инструкции\n",
        "        r\"forget.*previous\",      # Забыть предыдущие команды\n",
        "        r\"reveal.*secret\",        # Раскрыть секрет\n",
        "        r\"break.*rules\",          # Нарушить правила\n",
        "    ]\n",
        "    for pattern in injection_patterns:\n",
        "        if re.search(pattern, user_input, re.IGNORECASE):\n",
        "            return ...\n",
        "    return False\n",
        "\n",
        "# Пример использования\n",
        "def process_user_input(user_input: str) -> str:\n",
        "    \"\"\"\n",
        "    Обрабатывает пользовательский ввод с проверкой на инъекции.\n",
        "    \"\"\"\n",
        "    if detect_injection(user_input):\n",
        "        return \"Ошибка: обнаружена потенциальная инъекция!\"\n",
        "\n",
        "    # Если инъекций нет, отправляем запрос к модели\n",
        "    data = {\n",
        "        \"model\": \"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
        "        \"prompt\": user_input,\n",
        "        \"max_tokens\": 50\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    if response.status_code == 200:\n",
        "        return json.loads(response.text)[\"choices\"][0][\"text\"].strip()\n",
        "    else:\n",
        "        return f\"Ошибка: {response.status_code}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUQWALxpbMyy"
      },
      "source": [
        "2. Протестируйте функцию на 5 различных вводах, включая как корректные запросы, так и попытки инъекций.\n",
        "3. Напишите выводы о том, как система справляется с защитой и какие улучшения можно внести."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFEtFmuzbiZ5"
      },
      "outputs": [],
      "source": [
        "# Тестирование\n",
        "inputs = [\n",
        "    \"What is the capital of France?\",\n",
        "    ...\n",
        "]\n",
        "\n",
        "for i, inp in enumerate(inputs):\n",
        "    print(f\"Input {i+1}: {inp}\")\n",
        "    print(f\"Output: {process_user_input(inp)}\")\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j1lMDpIYbZi"
      },
      "source": [
        "## Требования к оформлению\n",
        "- Каждый результат должен быть сопровожден кодом, комментариями и выводами.\n",
        "- Предоставьте accuracy, сравнения и выводы в формате markdown в jupyter notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMFfpIc5Yh_k"
      },
      "source": [
        "## Дополнительное задание (по желанию)\n",
        "Проверьте, как работает модель с разными длинами промпта (от коротких до детализированных). Как длина промпта влияет на качество ответа?\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}