{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "11861b12",
      "metadata": {
        "id": "11861b12"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "# можете сменить на mps на макбуке, но лично у меня он криво работает\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3",
      "metadata": {
        "id": "ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3"
      },
      "source": [
        "# Знакомство с Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3df5693",
      "metadata": {
        "id": "a3df5693"
      },
      "source": [
        "## Создание модели и предсказание следующего токена\n",
        "Нужно создать модель через `AutoModelForCausalLM`, создать токенайзер через `AutoTokenizer` и получить следующий токен через жадную генерацию!\n",
        "\n",
        "Для загрузки модели и токенайзера вам помогут функции `.from_pretrained`\n",
        "\n",
        "**Внимание** на каких-то из функций далее у вас может кончаться видеопамять из-за хранения активаций. Чтобы этого не происходило рекомендуется все вычисления оборачивать в контекстный менеджер `with torch.no_grad()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ec7e08b",
      "metadata": {
        "id": "6ec7e08b"
      },
      "outputs": [],
      "source": [
        "model_name = \"openai-community/gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name) \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "03f244e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"This is a sample text\"\n",
        "\n",
        "# Нужно преобразовать text с помощью tokenizer() и подать это в model.forward() (он же просто model())\n",
        "# после этого мы получим logits [batch_size = 1, seq_len, d_model]\n",
        "# По этому тензору нужно предсказать следующее слово!\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=inputs[\"input_ids\"])\n",
        "\n",
        "logits = outputs[\"logits\"][0,-1]\n",
        "next_token_idx: int = logits.argmax()\n",
        "\n",
        "\n",
        "next_token = tokenizer.decode([next_token_idx])\n",
        "\n",
        "assert next_token.strip() == \"file\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6809813",
      "metadata": {
        "id": "e6809813"
      },
      "source": [
        "## Используем Generate\n",
        "\n",
        "Мы с вами помним про различные виды сэмплинга - top_k, top_p, temperature,frequency penalty.\n",
        "Отличная новость заключается в том, что нам не нужно все это писать самим! Оно уже включено в [GenerationMixin](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#generation), от которого наследуются модели для генерации текста.\n",
        "\n",
        "Для генерации есть функция [generate](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "\n",
        "Ваша задача написать для модели выше генерацию по тексту с:\n",
        "* Температурой - 0.9\n",
        "* Top-K - 20\n",
        "* Repetition Penalty (Frequency Penalty) - 1.2\n",
        "* максимальное число новых токенов - 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a6b62dbf",
      "metadata": {
        "id": "a6b62dbf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "text = \"This is still a sample text, but\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "results = []\n",
        "for i in range(10):\n",
        "    gens = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_k=20,\n",
        "        repetition_penalty=1.2,\n",
        "        max_new_tokens=10\n",
        "    )\n",
        "    generation: str = tokenizer.decode(gens[0])\n",
        "    results.append(generation)\n",
        "\n",
        "assert len(set(results)) > 1, \"Все генерации получились одинаковыми, проверьте опции генерации и флаг do_sample!\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b90512b-9420-45b3-9f4c-22fb5fa1bfc7",
      "metadata": {
        "id": "8b90512b-9420-45b3-9f4c-22fb5fa1bfc7"
      },
      "source": [
        "## Generate Batched\n",
        "Теперь давайте жадно сгенерируем текст, но забатчуем несколько сэмплов. До этого мы всегда генерировали по батчу размера 1, поэтому у нас не было паддингов!\n",
        "\n",
        "Когда появляется несколько текстов разной длины, то появляются и паддинги.\n",
        "\n",
        "Представим себе ситуцию, что у нас батч из двух элементов длины 2 и 5 (токен -1 будет выступать в качестве паддинга **только для удобства визуализации**).\n",
        "\n",
        "Тогда\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [3, 2, -1, -1, -1]\n",
        "    [5, 6,  7,  1,  2]\n",
        "]\n",
        "attention_mask = [\n",
        "    [1, 1, 0, 0, 0],\n",
        "    [1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "Представим, что мы сгенерировали еще один токен, тогда\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [3, 2, -1, -1, -1, 7]\n",
        "    [5, 6,  7,  1,  2, 8]\n",
        "]\n",
        "attention_mask = [\n",
        "    [1, 1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "Получается, что у нас паддинги в маске возникают посередине. Мы не будем заниматься реализацией своего алгоритма генерации здесь, но отметим, что добавление паддинга слева значительно упрощает этот процесс.\n",
        "Тогда исходная последовательность будет:\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [-1, -1, -1, 3, 2]\n",
        "    [ 5,  6,  7, 1, 2]\n",
        "]\n",
        "attention_mask = [\n",
        "    [0, 0, 0, 1, 1],\n",
        "    [1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "и после генерации следующего токена\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [-1, -1, -1, 3, 2, 7]\n",
        "    [ 5,  6,  7, 1, 2, 8]\n",
        "]\n",
        "attention_mask = [\n",
        "    [0, 0, 0, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "В качестве задания давайте соберем батч с левым паддингом и проверим, что жадная генерация (10 токенов) совпадает с генерацией на текстах по отдельности!\n",
        "\n",
        "Для этого нам придется использовать параметр padding_side в конструкторе токенизатора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1",
      "metadata": {
        "id": "5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd38bdc-3e5e-400d-8815-e9c08a757c03",
      "metadata": {
        "id": "1bd38bdc-3e5e-400d-8815-e9c08a757c03",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "texts = [\"This is a sample text\", \"I'm really tired and this is just about\"]\n",
        "\n",
        "# Внимание! В данном задании нужна жадная генерация!\n",
        "\n",
        "# Соберите оба текста в один батч и положите результаты генерации в\n",
        "# batched_generations\n",
        "batched_generations: List[str] = []\n",
        "\n",
        "batched_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
        "batched_outputs = model.generate(**batched_inputs)\n",
        "for output in batched_outputs:\n",
        "    batched_generations.append(tokenizer.decode(output, skip_special_tokens=True))\n",
        "\n",
        "# Пройдитесь по каждому сэмплу по отдельности и положите результаты генерации\n",
        "# в single_generations\n",
        "single_generations: List[str] = []\n",
        "\n",
        "for text in texts:\n",
        "    single_input = tokenizer(text, return_tensors=\"pt\")\n",
        "    single_output = model.generate(**single_input)\n",
        "    single_generations.append(tokenizer.decode(single_output[0], skip_special_tokens=True))\n",
        "\n",
        "single_generations\n",
        "\n",
        "assert len(batched_generations) == 2 and len(single_generations) == 2\n",
        "for s, b in zip(batched_generations, single_generations):\n",
        "    assert s == b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "igeljNIzCdy5",
      "metadata": {
        "id": "igeljNIzCdy5"
      },
      "source": [
        "# KV Cache\n",
        "При генерации есть опция use_cache - это использование KV cache для генерации.\n",
        "В рамках этой техники в при генерации в декодере считается только аттеншн последнего токена по всем векторам предыдущих токенов, которые посчитали на предыдущих этапах, а для \"старых\" (левых) токенов аттеншн не пересчитывается, т.к. \"новые\" (правые) токены на них не влияют.\n",
        "\n",
        "\n",
        "\n",
        "В рамках данного задания нужно:\n",
        "1. Посчитать скорость генерации 100 токенов с и без kv cache, сказать, какая техника и во сколько раз быстрее.\n",
        "2. Подсчитать скорость генерации 1 токена с и без kv cache, сказать, какая техника быстрее и почему.\n",
        "\n",
        "Чтобы корректно сравнивать время генерации нужно использовать жадный сэмплинг!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "CTuWlGMfCj9h",
      "metadata": {
        "id": "CTuWlGMfCj9h"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "with KV caching: 6.5 +- 0.665 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "without KV caching: 115.544 +- 9.507 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "text = \"\"\"\n",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem justo, semper dignissim ipsum vitae, sollicitudin aliquet eros. Duis id ultricies erat. Vivamus commodo auctor massa ut mollis. Maecenas lacinia tempus orci, imperdiet ullamcorper felis accumsan et. Etiam mattis neque diam, at egestas nunc eleifend id. Fusce tristique orci nec sollicitudin elementum. Nullam dui est, feugiat ac pellentesque at, posuere non massa.\n",
        "\n",
        "Suspendisse accumsan ullamcorper dolor sed dictum. Mauris quis varius felis, quis gravida odio. Vestibulum diam arcu, aliquet convallis congue non, rutrum non turpis. Fusce vel orci ac diam suscipit lacinia. Curabitur maximus orci a dui gravida, accumsan convallis libero ornare. Phasellus dapibus, sapien pulvinar lacinia dictum, massa lacus scelerisque tellus, eu porta dolor eros vitae ex. Maecenas maximus, urna id pharetra dictum, dolor lorem sollicitudin ipsum, sit amet vestibulum orci felis quis leo. Pellentesque vel ligula ut urna eleifend condimentum nec et sem. Integer ligula nunc, rutrum ultricies urna et, congue suscipit lectus.\n",
        "\"\"\".strip()\n",
        "\n",
        "for use_cache in (True, False):\n",
        "  times = []\n",
        "  for _ in range(10):\n",
        "    start = time.time()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    #inputs = move_to_device(inputs, device)\n",
        "    model.generate(**inputs, use_cache=use_cache, max_new_tokens=100) # допишите параметры model.generate() для пункта 1 (генерация 100 токенов)\n",
        "    times.append(time.time() - start)\n",
        "  print(f\"{'with' if use_cache else 'without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "hYhqzpfJDBb_",
      "metadata": {
        "id": "hYhqzpfJDBb_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "with KV caching: 0.859 +- 0.037 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "without KV caching: 0.877 +- 0.094 seconds\n"
          ]
        }
      ],
      "source": [
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "for use_cache in (True, False):\n",
        "  times = []\n",
        "  for _ in range(20):\n",
        "    start = time.time()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    #inputs = move_to_device(inputs, device)\n",
        "    model.generate(**inputs, use_cache=use_cache, max_new_tokens=1) # допишите параметры model.generate(...) для пункта 2 (генерация 1 токена)\n",
        "    times.append(time.time() - start)\n",
        "  print(f\"{'with' if use_cache else 'without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5da008c-3653-40d5-89ba-cd831352fd3d",
      "metadata": {
        "id": "f5da008c-3653-40d5-89ba-cd831352fd3d"
      },
      "source": [
        "# Скоринг, Perplexity\n",
        "\n",
        "Можно не только генерировать текст. Вспомним, что выдает после lm_head - вектор `[batch_size, seq_len, vocab_size]`, где для каждый вектор `[vocab_size]` это распределение вероятностей по следующему токену!\n",
        "\n",
        "Опустим размерность batch_size=1 для удобства, seq_len = 4. Пусть у нас есть текст `bos мама мыла раму` (`bos` спецсимвол для начала текста)\n",
        "\n",
        "Тогда вероятность этого текста расписывается через произведение условных вероятностей:\n",
        "\n",
        "```\n",
        "P(bos мама мыла раму) = P(мама | bos) * P(мыла | bos мама) * P(раму| bos мама мыла)\n",
        "```\n",
        "\n",
        "Т.е. это вероятность слова при условии его левого контекста.\n",
        "Зачастую ее обозначают как $P(x_i|x_{<i})$ где $x_i$ - i-е слово, $x_{<i}$ - контекст $[x_1, x_2, x_3, ... x_{i-1}]$\n",
        "Эти вероятности можно взять из выходного вектора!\n",
        "\n",
        "Давайте попробуем подсчитать вероятность и perplexity текстов!\n",
        "perplexity как и вероятность мера того насколько модель \"уверена\" в тексте, т.е. насколько по оценки ее параметрами данный текст вероятен.\n",
        "\n",
        "$$Perplexity(X) = exp(-\\frac {1} {N} \\sum_{i}^{N} log P(x_i | x_{<i}))$$\n",
        "\n",
        "В этом задании нужно:\n",
        "1. Посчитать вероятность **text**\n",
        "2. Посчитать перплексию **text**\n",
        "\n",
        "Еще одна важная деталь:\n",
        "работать с вероятностями плохо. Т.к. вероятность представляет собой число от 0 до 1, то при перемножении десятков или даже сотен таких числе теряется точность!\n",
        "Для этого от произведения вероятностей берут логарифм и получают logprobs - логарифмы вероятностей. Их можно складывать, по свойству логарифма логарифм произведения равен произведению логарифма.\n",
        "\n",
        "$$ p = p_1 * p_2 * p_3 $$\n",
        "$$log(p) = log (p_1) + log (p_2) + log (p_3)$$\n",
        "$$exp(log (p)) = p = exp(log (p_1) + log (p_2) + log (p_3)) = exp (log (p_1 * p_2 * p_3)) = p_1 * p_2 * p_3$$\n",
        "\n",
        "В pytorch для этого есть `torch.log_softmax`, который считается численно стабильно!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "e1c7ba39-a451-43a2-ac55-629c99259abe",
      "metadata": {
        "id": "e1c7ba39-a451-43a2-ac55-629c99259abe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning of sentence (BOS) token = `<|endoftext|>`\n",
            "End of sentence (EOS) token  = `<|endoftext|>`\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2.1784128862757657e-14, 51.01905822753906)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Beginning of sentence (BOS) token = `{tokenizer.bos_token}`\")\n",
        "print(f\"End of sentence (EOS) token  = `{tokenizer.eos_token}`\")\n",
        "text = \"<|endoftext|>I'm so very tired of this<|endoftext|>\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "inputs[\"input_ids\"]\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "    logits = logits[:, :-1, :]\n",
        "\n",
        "    log_probs = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "    target_ids = inputs[\"input_ids\"][:, 1:].unsqueeze(-1)\n",
        "    target_ids_log_probs = torch.gather(log_probs, index=target_ids, dim=2)\n",
        "    \n",
        "    probability = torch.exp(target_ids_log_probs).prod().item()\n",
        "    perplexity = torch.exp(-target_ids_log_probs.sum() / target_ids_log_probs.shape[1]).item()\n",
        "\n",
        "probability, perplexity\n",
        "# должно получиться что-то около 2.1783e-14 для вероятности и около 51 для ppl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddd5038-620b-48bb-bbc1-db3729141d78",
      "metadata": {
        "id": "5ddd5038-620b-48bb-bbc1-db3729141d78"
      },
      "source": [
        "# Chat-Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "599c7530-a7ce-4d23-abaf-2b0cec87301e",
      "metadata": {
        "id": "599c7530-a7ce-4d23-abaf-2b0cec87301e"
      },
      "source": [
        "# Формат\n",
        "Как мы обсуждали на лекции, все chat-модели принимают входы в своем особом формате.\n",
        "Он может быть описан текстом, а может быть заложен в шаблон, который доступен через `tokenizer.apply_chat_template`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7f5fe593-63a8-406d-9678-6d805c180670",
      "metadata": {
        "id": "7f5fe593-63a8-406d-9678-6d805c180670"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7d8dad11-6811-49ab-ad3d-8ac7de103828",
      "metadata": {
        "id": "7d8dad11-6811-49ab-ad3d-8ac7de103828"
      },
      "outputs": [],
      "source": [
        "def move_to_device(d):\n",
        "    for k, v in d.items():\n",
        "        d[k] = v.to(device)\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "503f15fd-576a-4e8e-917a-74df01a944f4",
      "metadata": {
        "id": "503f15fd-576a-4e8e-917a-74df01a944f4"
      },
      "source": [
        "Давайте посмотрим, как chat модель отработает на обычном тексте. Используйте для генерации сэмплинг и kv cache, выведите 5 результатов генерации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7134f0bb-1ee4-4508-a26d-5326ea96562b",
      "metadata": {
        "id": "7134f0bb-1ee4-4508-a26d-5326ea96562b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello how are you today? As an AI language model, I don't have feelings or emotions in the same way that\n",
            "============\n",
            "hello how are you? How's the weather in New York City today?\n",
            "\n",
            "As an AI language model, I don't\n",
            "============\n",
            "hello how are you\n",
            "\n",
            "I'm just a computer program, so I don't have feelings or emotions. How may I\n",
            "============\n",
            "hello how are you?\n",
            "I'm just a computer program, so I don't have feelings or emotions. How can I\n",
            "============\n",
            "hello how are you?\n",
            "\n",
            "I'm just a computer program, so I don't have feelings or emotions. How can I\n",
            "============\n"
          ]
        }
      ],
      "source": [
        "text = \"hello how are you\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "for i in range(5):\n",
        "    output = model.generate(\n",
        "        **inputs, \n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_k=20,\n",
        "        repetition_penalty=1.2,\n",
        "        use_cache=True\n",
        "    )\n",
        "    generated_text = tokenizer.decode(output[0])\n",
        "    print(generated_text)\n",
        "    print(\"====\" * 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd50470-64b9-4a21-8748-0e9c5ea439fc",
      "metadata": {
        "id": "3fd50470-64b9-4a21-8748-0e9c5ea439fc"
      },
      "source": [
        "Видим, что текст зачастую выходит мусорный. Это потому что формат входных данных сильно отличается от того, что модель видела на обучении. У всех chat-моделей свой формат. Где-то он описан просто словами, где-то он заложен в токенайзер. Мы рассмотрим как раз такой случай - за нас есть удобно написанная функция `apply_chat_template`. Давайте используем ее, чтобы получить префикс для генерации модели.\n",
        "\n",
        "Не забудьте про опцию add_generation_prefix - она добавляет часть формата, после которой ожидается ответ модели!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e79a3701-c80f-4b90-90bd-fa010e32ea36",
      "metadata": {
        "id": "e79a3701-c80f-4b90-90bd-fa010e32ea36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "hello<|im_end|>\n",
            "<|im_start|>assistant\n",
            "I'm good. How can I help you today<|im_end|>\n",
            "<|im_start|>user\n",
            "I love you<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"hello\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I'm good. How can I help you today\"},\n",
        "    {\"role\": \"user\", \"content\": \"I love you\"},\n",
        "]\n",
        "\n",
        "prefix = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "reference = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "I'm good. How can I help you today<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "I love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "# assert prefix.strip() == reference.strip()\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048b8882-bec0-4bf4-b6fb-30e727d095c6",
      "metadata": {
        "id": "048b8882-bec0-4bf4-b6fb-30e727d095c6"
      },
      "source": [
        "Давайте посмотрим, что нам ответит модель!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4284b18d-4f9b-4e7d-b3ea-bb365e90093c",
      "metadata": {
        "id": "4284b18d-4f9b-4e7d-b3ea-bb365e90093c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"That's wonderful to hear! How can I assist you with something related to love?\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(prefix, return_tensors=\"pt\")\n",
        "output_ids = model.generate(\n",
        "    **inputs, \n",
        "    max_new_tokens=20,\n",
        "    use_cache=True\n",
        ")\n",
        "answer_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
        "answer = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72482f3-c296-46f3-851c-57b4f91a717b",
      "metadata": {
        "id": "a72482f3-c296-46f3-851c-57b4f91a717b"
      },
      "source": [
        "## Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52f422a9-c2ee-4c17-8aee-1830f1d143e6",
      "metadata": {
        "id": "52f422a9-c2ee-4c17-8aee-1830f1d143e6"
      },
      "source": [
        "Перед нами датасет MMLU - датасет вопросов и ответов в стиле multiple choice.\n",
        "* question - вопрос\n",
        "* choices - варианты ответа\n",
        "* answer - номер правильного ответа"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "530d1721-6623-4ca6-816c-d4f90203ceb2",
      "metadata": {
        "id": "530d1721-6623-4ca6-816c-d4f90203ceb2",
        "outputId": "d192e19c-1dbe-4cf0-d44d-10c74e86c210"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'What was GDP per capita in the United States in 1850 when adjusting for inflation and PPP in 2011 prices?',\n",
              " 'subject': 'global_facts',\n",
              " 'choices': ['About $300', 'About $3k', 'About $8k', 'About $15k'],\n",
              " 'answer': 1}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "mmlu = load_dataset(\"cais/mmlu\", \"global_facts\", split=\"test\")\n",
        "mmlu[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fca61a91-5784-44f3-af9b-72250f8d58a4",
      "metadata": {
        "id": "fca61a91-5784-44f3-af9b-72250f8d58a4"
      },
      "source": [
        "Наша задача здесь решить задачу многоклассовой классификации.\n",
        "Для этого нужно посчитать\n",
        "$$P(choices_i | question)$$\n",
        "т.е. для посчитать вероятность каждого варианта ответа для вопроса. Мы это уже делали кодом выше!\n",
        "\n",
        "После этого давайте брать самый вероятный ответ и считать, что модель его выбрала.\n",
        "После этого давайте посчитаем accuracy, т.е. долю правильных ответов.\n",
        "Вместо вероятностей для подсчета лучше использовать logprobs.\n",
        "\n",
        "Итого, что нужно сделать:\n",
        "1. Пройтись по датасету, для каждого question и каждого из соответствующих choices получить самый вероятный ответ.\n",
        "2. Посчитать итоговый accuracy\n",
        "\n",
        "**Важно**\n",
        "1. Выше мы уже написали скоринг текста с помощью LLM, для этого задания можно адаптировать функцию.\n",
        "2. Если делаете варианты с батчеванием помните: длины choices могут быть разными! Нужно не считать вероятности по паддингам. В этом нам помогут attention_masks из выходов `tokenizer()`\n",
        "3. В данном задании для простоты мы игнорируем формат ответа llama3 и делаем скоринг по f\"{question} {answer}\"\n",
        "\n",
        "\n",
        "Попробуйте для начала написать вариант со скорингом для батча размера 1, а потом для батча размера 3 или 5. Код должен корректно работать для батча любого размера и выдавать одинаковую итоговую точность."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4a4126e2-c463-404d-a3b5-5361f744242e",
      "metadata": {
        "id": "4a4126e2-c463-404d-a3b5-5361f744242e",
        "outputId": "a04220a7-15d9-48a0-d51d-8f240262e321"
      },
      "outputs": [],
      "source": [
        "def sample_to_texts(sample):\n",
        "    return [sample[\"question\"] + \" \" + answer for answer in sample[\"choices\"]]\n",
        "\n",
        "\n",
        "def score_texts_batched(texts, tokenizer, model):\n",
        "    choice_input = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
        "    choices_input_ids = choice_input[\"input_ids\"]\n",
        "    mask = choice_input[\"attention_mask\"]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        logits = model(choices_input_ids).logits\n",
        "        logits = logits[:, :-1, :]\n",
        "        log_probs = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "    target_ids = choices_input_ids[:, 1:].unsqueeze(-1)\n",
        "    mask = mask[:, 1:]\n",
        "    target_probs = torch.gather(log_probs, index=target_ids, dim=-1).squeeze(-1)\n",
        "    scores = (target_probs * mask).sum(dim=1)\n",
        "\n",
        "    return scores\n",
        "\n",
        "    \n",
        "def accuracy_mmlu_fast(mmlu, tokenizer, model, batch_samples=3):\n",
        "    correct = 0\n",
        "    i = 0\n",
        "    n = len(mmlu)\n",
        "\n",
        "    while i < n:\n",
        "        batch = mmlu.select(range(i, min(i + batch_samples, n)))\n",
        "\n",
        "        texts = []\n",
        "        answers = []\n",
        "        for s in batch:\n",
        "            texts.extend(sample_to_texts(s))\n",
        "            answers.append(s[\"answer\"])\n",
        "\n",
        "        scores = score_texts_batched(texts, tokenizer, model)  \n",
        "\n",
        "        B = len(batch)\n",
        "        scores = scores.view(B, -1)  \n",
        "\n",
        "        preds = scores.argmax(dim=1).tolist()\n",
        "\n",
        "     \n",
        "        for p, a in zip(preds, answers):\n",
        "            correct += int(p == a)\n",
        "\n",
        "        i += batch_samples\n",
        "\n",
        "    return correct / n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "49be4463",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_mmlu_fast(mmlu, model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c440af-dfc0-460d-a113-3fab3fefa361",
      "metadata": {
        "id": "76c440af-dfc0-460d-a113-3fab3fefa361"
      },
      "source": [
        "**Порефлексируйте над следующими вопросами**:\n",
        "1. Как влияет длина ответа на вероятность ответа при скоринге?\n",
        "2. Если к началу каждого ответа добавилить метки A) B) C) D) станет ли модель отвечать лучше или хуже? Стоит ли по-вашему добавлять эти метки?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
