{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-05T21:57:27.793934Z",
          "iopub.status.busy": "2024-11-05T21:57:27.793252Z",
          "iopub.status.idle": "2024-11-05T21:57:45.041801Z",
          "shell.execute_reply": "2024-11-05T21:57:45.040664Z",
          "shell.execute_reply.started": "2024-11-05T21:57:27.793896Z"
        },
        "id": "3NjfLzK0fcwT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: peft in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.18.1)\n",
            "Requirement already satisfied: bitsandbytes in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.49.1)\n",
            "Requirement already satisfied: accelerate in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\dmitry\\appdata\\roaming\\python\\python310\\site-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\dmitry\\appdata\\roaming\\python\\python310\\site-packages (from peft) (7.2.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (4.57.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: safetensors in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dmitry\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.13.0->peft) (4.15.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (2025.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: requests in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\dmitry\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->peft) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2026.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->peft) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\dmitry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->peft) (0.22.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install peft bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-11-05T22:51:10.663094Z",
          "iopub.status.busy": "2024-11-05T22:51:10.662731Z",
          "iopub.status.idle": "2024-11-05T22:51:18.000243Z",
          "shell.execute_reply": "2024-11-05T22:51:17.999413Z",
          "shell.execute_reply.started": "2024-11-05T22:51:10.663060Z"
        },
        "id": "2PIU0pRnfcwV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig, AutoConfig, set_seed\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel, get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
        "\n",
        "set_seed(12, True)\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8xN10dnfcwV"
      },
      "source": [
        "# Gradient Accumulation\n",
        "\n",
        "Давайте реализуем собственную аккумуляцию градиентов.\n",
        "Ниже описано обучение обычного линейного слоя. Клеткой ниже этот код скопирован, там необходимо написать аккумуляцию ргадиентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-05T22:51:34.521937Z",
          "iopub.status.busy": "2024-11-05T22:51:34.520442Z",
          "iopub.status.idle": "2024-11-05T22:51:35.882571Z",
          "shell.execute_reply": "2024-11-05T22:51:35.881540Z",
          "shell.execute_reply.started": "2024-11-05T22:51:34.521879Z"
        },
        "id": "ZvuG0gQ7fcwW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.1878371238708496\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "input_size = 512\n",
        "output_size = 256\n",
        "batch_size = 64\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "\n",
        "\n",
        "model = nn.Linear(input_size, output_size).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "x = torch.randn(batch_size, input_size).to(device)\n",
        "y = torch.randn(batch_size, output_size).to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "for i in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(x)\n",
        "    loss = loss_fn(output, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiwKiKqjfcwX"
      },
      "source": [
        "Число шагов в аккумуляции определяется параметром gradient_accumulation_steps - это число шагов, которое мы хотим сделать перед оптимизацией.\n",
        "Вам нужно поправить цикл обучения следующим образом:\n",
        "1. Разбить текущий батч на gradient_accumulation_steps частей\n",
        "2. Пройтись по каждому подбатчу (микробатчу), посчитать на нем функцию потерь, посчитать градиенты. Подумайте, нужно ли на что-либо делить или умножать функцию потерь, чтобы сохранился тот же масштаб обучения?\n",
        "3. После прохождения всех микробатчей нужно сделать шаг оптимизации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-05T22:51:37.512699Z",
          "iopub.status.busy": "2024-11-05T22:51:37.511958Z",
          "iopub.status.idle": "2024-11-05T22:51:40.371231Z",
          "shell.execute_reply": "2024-11-05T22:51:40.370111Z",
          "shell.execute_reply.started": "2024-11-05T22:51:37.512653Z"
        },
        "id": "q_LKGvYQfcwX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.1768290996551514\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "input_size = 512\n",
        "output_size = 256\n",
        "batch_size = 64\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "micro_batch_size = batch_size // gradient_accumulation_steps\n",
        "\n",
        "model = nn.Linear(input_size, output_size).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "x = torch.randn(batch_size, input_size).to(device)\n",
        "y = torch.randn(batch_size, output_size).to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "for i in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    for j in range(gradient_accumulation_steps):\n",
        "        x_mb = x[j*micro_batch_size:(j+1)*micro_batch_size]\n",
        "        y_mb = y[j*micro_batch_size:(j+1)*micro_batch_size]\n",
        "        output = model(x_mb)\n",
        "        loss = loss_fn(output, y_mb) / gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item() * gradient_accumulation_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0AWLbxbfcwY"
      },
      "source": [
        "# QLORA\n",
        "Необходимо использовать аккумуляцию градиентов, чекпоинтинг активаций и обучение qlora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-05T22:51:40.372869Z",
          "iopub.status.busy": "2024-11-05T22:51:40.372586Z",
          "iopub.status.idle": "2024-11-05T22:51:40.648368Z",
          "shell.execute_reply": "2024-11-05T22:51:40.647502Z",
          "shell.execute_reply.started": "2024-11-05T22:51:40.372838Z"
        },
        "id": "LIEIne3AfcwY"
      },
      "outputs": [],
      "source": [
        "model_name = \"NousResearch/Llama-2-7b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-05T22:51:41.429253Z",
          "iopub.status.busy": "2024-11-05T22:51:41.428213Z",
          "iopub.status.idle": "2024-11-05T22:51:45.145706Z",
          "shell.execute_reply": "2024-11-05T22:51:45.144702Z",
          "shell.execute_reply.started": "2024-11-05T22:51:41.429208Z"
        },
        "id": "iB9pGFzxfcwZ"
      },
      "outputs": [],
      "source": [
        "imdb = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av0ENm-IfcwZ"
      },
      "source": [
        "Наша задача научиться генерировать класс текста posive или negative, чтобы сэкономить на fewshot промпте.\n",
        "\n",
        "Давайте напишем collate_fn, которая собирает сэмпл следующим образом:\n",
        "\n",
        "если текст имеет метку 1\n",
        "`{text} ||| posive eos`\n",
        "или\n",
        "`{text} ||| negatve eos`\n",
        "если текст имеет метку 0. (в качестве eos можно использовать tokenizer.eos_token_id)\n",
        "\n",
        "Символы ||| нужны нам, чтобы разделить входной текст и метку, иначе модель может не понять, что нужно генерировать метку и продолжит генерировать текст. Таким образом мы научим модель после ||| генерировать положительный или отрицательнй отзыв стоит до этого.\n",
        "\n",
        "\n",
        "Возвращать нужно словарь из 3х элементов:\n",
        "1. input_ids - LongTensor токенов. В качестве паддинга нужно использовать tokenizer.eos_token_id.\n",
        "2. attention_mask - LongTensor той же размерности, что и input_ids. 0 там, где стоят паддинги, 1 в остальных позициях\n",
        "3. labels - метки, которые мы предсказыаем. Должен быть равен -100 на всех позициях, кроме позиций, которые соответствуют метке и eos символу.\n",
        "Например\n",
        "```python\n",
        "tokenizer.encode(\"some text ||| positive </s>\") # [1, 777, 1426, 3830, 29989, 6374, 2]\n",
        "labels = [-100, -100, -100, -100, -100, 6374, 2]\n",
        "```\n",
        "\n",
        "Т.е. метки должны быть -100, кроме позиций, соответствующих предсказываемым токенам."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-05T22:53:40.465718Z",
          "iopub.status.busy": "2024-11-05T22:53:40.465308Z",
          "iopub.status.idle": "2024-11-05T22:53:40.488696Z",
          "shell.execute_reply": "2024-11-05T22:53:40.487733Z",
          "shell.execute_reply.started": "2024-11-05T22:53:40.465680Z"
        },
        "id": "PWleW44Ifcwa"
      },
      "outputs": [],
      "source": [
        "class_mapping = {0: \"negative\", 1: \"positive\"}\n",
        "\n",
        "def collate_fn(batch: List[Dict[str, Any]]):    \n",
        "    sep_ids = tokenizer.encode(\" ||| \", add_special_tokens=False)\n",
        "    pos_ids = tokenizer.encode(class_mapping[1], add_special_tokens=False) + [tokenizer.eos_token_id]\n",
        "    neg_ids = tokenizer.encode(class_mapping[0], add_special_tokens=False) + [tokenizer.eos_token_id]\n",
        "    target_length = max(len(pos_ids), len(neg_ids))\n",
        "    keep = 4096 - len(sep_ids) - target_length\n",
        "\n",
        "    raw_texts = [sample[\"text\"] for sample in batch]\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        raw_texts,\n",
        "        add_special_tokens=False,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=keep\n",
        "    )\n",
        "\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for i, text_ids in enumerate(tokenized[\"input_ids\"]):\n",
        "        label_ids = pos_ids if batch[i][\"label\"] == 1 else neg_ids\n",
        "        input_ids = torch.tensor(text_ids + sep_ids + label_ids, dtype=torch.long)\n",
        "        \n",
        "        prefix_len = len(text_ids) + len(sep_ids)\n",
        "        label = torch.tensor([-100] * prefix_len + label_ids, dtype=torch.long)\n",
        "\n",
        "        inputs.append(input_ids)\n",
        "        labels.append(label)\n",
        "        lengths.append(input_ids.size(0))\n",
        "    \n",
        "    inputs = pad_sequence(inputs, padding_value=tokenizer.eos_token_id, batch_first=True)\n",
        "    labels = pad_sequence(labels, padding_value=-100, batch_first=True)\n",
        "\n",
        "    attention_mask = torch.zeros_like(inputs, dtype=torch.long)\n",
        "    for i, L in enumerate(lengths):\n",
        "        attention_mask[i, :L] = 1\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": inputs,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "res = collate_fn([imdb[\"train\"][0], imdb[\"train\"][12505], imdb[\"train\"][2]])\n",
        "\n",
        "assert tokenizer.decode(res[\"input_ids\"][res[\"labels\"] != -100]) == \"negative</s> positive</s> negative</s>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQtnGnKLfcwa"
      },
      "source": [
        "Далее нам нужно создать модель в nf4, т.е. 4-битной квантизации. Конфиг уже написан, нужно лишь подать его в модель. После этого нужно:\n",
        "1. Создать конфиг адаптера LoraConfig (используйте r=8 или r=4, если будет OOM) и создать модель\n",
        "2. Создать модель с адаптером с помощью PeftModel и LoraConfig\n",
        "3. Чтобы обучение шло только по lora частям, нужно пройтись по всем параметрам модели с помощью model.named_parameters() и проставить у параметров, соответствующих lora атрибут requires_grad = True, а у всех остальных False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-05T22:52:14.146891Z",
          "iopub.status.busy": "2024-11-05T22:52:14.146554Z",
          "iopub.status.idle": "2024-11-05T22:52:45.531779Z",
          "shell.execute_reply": "2024-11-05T22:52:45.530854Z",
          "shell.execute_reply.started": "2024-11-05T22:52:14.146854Z"
        },
        "id": "K9JcLa1Mfcwa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.55s/it]\n"
          ]
        }
      ],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # храним веса в 4-битном виде (коды 0..15)\n",
        "    bnb_4bit_quant_type=\"nf4\", # определяет схему квантизации, по которой выбирается один из 16 уровней\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # тип, в котором выполняются вычисления. bfloat - больше диапазон, меньше точность\n",
        "    bnb_4bit_use_double_quant=True, # двойная квантизация - квантуются не только веса, но и статистики для декодирования\n",
        "#    bnb_4bit_quant_storage=torch.bfloat16, тип хранения кодов, тут странное значение\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False # не используем KV-cache во время обучения\n",
        "\n",
        "model = prepare_model_for_kbit_training(model) # стабилизирует LayerNorm, активирует нужные флаги, сочетается с gradient checkpointing\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"v_proj\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "for name, p in model.named_parameters():\n",
        "    if \"lora_\" in name:\n",
        "        p.requires_grad = True\n",
        "    else:\n",
        "        p.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DKo1phvfcwb"
      },
      "source": [
        "Осталось самое важное, аргументы обучения. Обязательно заполните следующие параметры:\n",
        "\n",
        "1. Батч сайз и число шагов аккумуляции выставьте так, чтобы эффективный батч сайз был 16\n",
        "2. Включите чекпоинтинг активаций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-05T22:52:45.533792Z",
          "iopub.status.busy": "2024-11-05T22:52:45.533468Z",
          "iopub.status.idle": "2024-11-05T22:53:19.086601Z",
          "shell.execute_reply": "2024-11-05T22:53:19.083502Z",
          "shell.execute_reply.started": "2024-11-05T22:52:45.533757Z"
        },
        "id": "dqnBqeOifcwb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "C:\\Users\\Dmitry\\AppData\\Local\\Temp\\ipykernel_4836\\4011455066.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 58:56, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.053800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.117500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.075800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.114200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.073500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.065900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.067100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.060800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.066500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.052500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=0.5747581481933594, metrics={'train_runtime': 3566.7589, 'train_samples_per_second': 0.449, 'train_steps_per_second': 0.028, 'total_flos': 3.08067253997568e+16, 'train_loss': 0.5747581481933594, 'epoch': 0.064})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "    bf16=True,\n",
        "    max_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    report_to=None,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=imdb[\"train\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collate_fn,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTGEBmr6fcwc"
      },
      "source": [
        "Давайте протестируем, что модель что-то выучила"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-05T22:53:19.088081Z",
          "iopub.status.idle": "2024-11-05T22:53:19.088619Z",
          "shell.execute_reply": "2024-11-05T22:53:19.088379Z",
          "shell.execute_reply.started": "2024-11-05T22:53:19.088353Z"
        },
        "id": "SllwF0a1fcwc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "negative\n",
            "<s> STAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Stephen Lord, otherwise known as Jase from East Enders) who is using a special method he learned in Afghanistan to fight off his opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />After Until Death, Van Damme appeared to be on a high, showing he could make the best straight to video films in the action market. While that was a far more drama oriented film, with The Shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since Derailed. It's nowhere near as bad as that film, but what I said still stands.<br /><br />A dull, predictable film, with very little in the way of any exciting action. What little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. Being a Mexican set film, director Isaac Florentine has tried to give the film a Robert Rodriguez/Desperado sort of feel, but this only adds to the desperation.<br /><br />VD gives a particularly uninspired performance and given he's never been a Robert De Niro sort of actor, that can't be good. As the villain, Lord shouldn't expect to leave the beeb anytime soon. He gets little dialogue at the beginning as he struggles to muster an American accent but gets mysteriously better towards the end. All the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />This is one shepherd that's strayed right from the flock. * |||  negative\n"
          ]
        }
      ],
      "source": [
        "input_text = imdb[\"test\"][3][\"text\"] + \" ||| \"\n",
        "label = imdb[\"test\"][3][\"label\"]\n",
        "x = tokenizer(input_text, return_tensors=\"pt\")\n",
        "for k, v in x.items():\n",
        "    x[k] = v.cuda()\n",
        "\n",
        "print(class_mapping[label])\n",
        "g = model.generate(**x, max_new_tokens=1, do_sample=False)\n",
        "print(tokenizer.decode(g[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
