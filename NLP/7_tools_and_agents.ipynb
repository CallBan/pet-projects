{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33c44059",
   "metadata": {
    "id": "33c44059"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "from together import Together\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f4847",
   "metadata": {
    "id": "782f4847"
   },
   "source": [
    "# Предобработка входных данных\n",
    "\n",
    "В данном задании мы будем ходить в онлайн модель. Для получения моего API-ключа пишем мне в лс! Посмотрим, как походы в API соотносятся с тем, что мы делали в практике в шаге \"LLM в индустрии\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3ca6131",
   "metadata": {
    "id": "c3ca6131"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3.1-70B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c702b9d",
   "metadata": {
    "id": "6c702b9d"
   },
   "source": [
    "## Ручное форматирование промпта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad05b7",
   "metadata": {
    "id": "f0ad05b7"
   },
   "source": [
    "Давайте попробуем собрать вход для llama3.1 руками, для этого допишем функцию `format_messages_to_prompt`.\n",
    "Она принимает messages - массив словарей, где указаны роли и текст сообщений, а возвращает она текст в формате, который нужно подать модели.\n",
    "\n",
    "Например для истории сообщений\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Some system message\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n",
    "]\n",
    "```\n",
    "\n",
    "должен выдаваться итоговый промпт\n",
    "\n",
    "```text\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Some system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "This is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "this is a mesage from the assistant<|eot_id|>\n",
    "```\n",
    "\n",
    "Что важно:\n",
    "1. Текст начинается со спецтокена bos\n",
    "2. Дальше идет заголовок start_header_id + end_header_id, которые содержат роль\n",
    "3. Дальше после \\n\\n идет текст, заканчивающийся на eot_id\n",
    "4. Дальше следующий заголовок с новой ролью и т.д.\n",
    "\n",
    "**Важно** - в данной функции нельзя использовать `tokenizer.apply_chat_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0fb161e",
   "metadata": {
    "id": "f0fb161e"
   },
   "outputs": [],
   "source": [
    "def format_messages_to_prompt(messages: List[Dict[str, str]]) -> str:\n",
    "    result = \"<|begin_of_text|>\"\n",
    "\n",
    "    for msg in messages:\n",
    "        format_msg = f\"<|start_header_id|>{msg['role']}<|end_header_id|>\\n\\n{msg['content']}<|eot_id|>\"\n",
    "        result += format_msg\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Some system message\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "reference_text = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Some system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "This is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "this is a mesage from the assistant<|eot_id|>\"\"\"\n",
    "\n",
    "\n",
    "assert format_messages_to_prompt(messages) == reference_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a536107",
   "metadata": {
    "id": "3a536107"
   },
   "source": [
    "Мы также помним, что раньше у нас была `tokenizer.apply_chat_template`. Т.к. у нас неофициальный форк llama3.1, то chat_template в токенайзер нам не завезли, поэтому придется добавить его руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e5db895",
   "metadata": {
    "id": "3e5db895"
   },
   "outputs": [],
   "source": [
    "chat_template = \"\"\"\n",
    "{{- bos_token }}\n",
    "{%- if custom_tools is defined %}\n",
    "    {%- set tools = custom_tools %}\n",
    "{%- endif %}\n",
    "{%- if not tools_in_user_message is defined %}\n",
    "    {%- set tools_in_user_message = true %}\n",
    "{%- endif %}\n",
    "{%- if not date_string is defined %}\n",
    "    {%- set date_string = \"26 Jul 2024\" %}\n",
    "{%- endif %}\n",
    "{%- if not tools is defined %}\n",
    "    {%- set tools = none %}\n",
    "{%- endif %}\n",
    "\n",
    "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
    "{%- if messages[0]['role'] == 'system' %}\n",
    "    {%- set system_message = messages[0]['content']|trim %}\n",
    "    {%- set messages = messages[1:] %}\n",
    "{%- else %}\n",
    "    {%- set system_message = \"\" %}\n",
    "{%- endif %}\n",
    "\n",
    "{#- System message + builtin tools #}\n",
    "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
    "{%- if builtin_tools is defined or tools is not none %}\n",
    "    {{- \"Environment: ipython\\n\" }}\n",
    "{%- endif %}\n",
    "{%- if builtin_tools is defined %}\n",
    "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
    "{%- endif %}\n",
    "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
    "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
    "{%- if tools is not none and not tools_in_user_message %}\n",
    "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
    "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
    "    {{- \"Do not use variables.\\n\\n\" }}\n",
    "    {%- for t in tools %}\n",
    "        {{- t | tojson(indent=4) }}\n",
    "        {{- \"\\n\\n\" }}\n",
    "    {%- endfor %}\n",
    "{%- endif %}\n",
    "{{- system_message }}\n",
    "{{- \"<|eot_id|>\" }}\n",
    "\n",
    "{#- Custom tools are passed in a user message with some extra guidance #}\n",
    "{%- if tools_in_user_message and not tools is none %}\n",
    "    {#- Extract the first user message so we can plug it in here #}\n",
    "    {%- if messages | length != 0 %}\n",
    "        {%- set first_user_message = messages[0]['content']|trim %}\n",
    "        {%- set messages = messages[1:] %}\n",
    "    {%- else %}\n",
    "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
    "{%- endif %}\n",
    "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
    "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
    "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
    "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
    "    {{- \"Do not use variables.\\n\\n\" }}\n",
    "    {%- for t in tools %}\n",
    "        {{- t | tojson(indent=4) }}\n",
    "        {{- \"\\n\\n\" }}\n",
    "    {%- endfor %}\n",
    "    {{- first_user_message + \"<|eot_id|>\"}}\n",
    "{%- endif %}\n",
    "\n",
    "{%- for message in messages %}\n",
    "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
    "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
    "    {%- elif 'tool_calls' in message %}\n",
    "        {%- if not message.tool_calls|length == 1 %}\n",
    "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
    "        {%- endif %}\n",
    "        {%- set tool_call = message.tool_calls[0].function %}\n",
    "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
    "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
    "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
    "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
    "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
    "                {%- if not loop.last %}\n",
    "                    {{- \", \" }}\n",
    "                {%- endif %}\n",
    "                {%- endfor %}\n",
    "            {{- \")\" }}\n",
    "        {%- else  %}\n",
    "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
    "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
    "            {{- '\"parameters\": ' }}\n",
    "            {{- tool_call.arguments | tojson }}\n",
    "            {{- \"}\" }}\n",
    "        {%- endif %}\n",
    "        {%- if builtin_tools is defined %}\n",
    "            {#- This means we're in ipython mode #}\n",
    "            {{- \"<|eom_id|>\" }}\n",
    "        {%- else %}\n",
    "            {{- \"<|eot_id|>\" }}\n",
    "        {%- endif %}\n",
    "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
    "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
    "        {%- if message.content is mapping or message.content is iterable %}\n",
    "            {{- message.content | tojson }}\n",
    "        {%- else %}\n",
    "            {{- message.content }}\n",
    "        {%- endif %}\n",
    "        {{- \"<|eot_id|>\" }}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
    "{%- endif %}\n",
    "\"\"\".strip()\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea72a3",
   "metadata": {
    "id": "18ea72a3"
   },
   "source": [
    "## Автоматическая сборка промпта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe9b9f",
   "metadata": {
    "id": "32fe9b9f"
   },
   "source": [
    "Давайте вспомним теперь на деле, как используется chat_template! Попробуем использовать функцию `tokenizer.apply_chat_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e493aae",
   "metadata": {
    "id": "7e493aae"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Some system message\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fee33f0b",
   "metadata": {
    "id": "fee33f0b"
   },
   "outputs": [],
   "source": [
    "reference_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 26 Jul 2024\n",
    "\n",
    "Some system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "This is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "this is a mesage from the assistant<|eot_id|>\"\"\"\n",
    "\n",
    "assert prompt == reference_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed104b62",
   "metadata": {
    "id": "ed104b62"
   },
   "source": [
    "Обратите внимание, что в заданном chat_template указаны Cutting Knowledge Date, т.е. до данные до какого периода видела модели, и Today Date - захардкоженная дата текущего диалога.\n",
    "\n",
    "Подумайте, на что влияет аргумент `add_generation_prompt` в функции `tokenizer.apply_chat_template`? Зачем его использовать?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab24edd",
   "metadata": {
    "id": "8ab24edd"
   },
   "source": [
    "## Походы в API\n",
    "\n",
    "Теперь давайте посмотрим, как можно ходить в API. Вообще говоря различных провайдеров много, API у них у всех очень похожий, т.к. все мимикрируют под OpenAI. Собственно, за апишкой – ко мне!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f3931a",
   "metadata": {
    "id": "a3f3931a"
   },
   "outputs": [],
   "source": [
    "# Вставьте свой ключ из https://api.together.ai/\n",
    "API_KEY = \"tgp_v1_gw1i6zpF-X3Ptq5aSuzMhlu0ATGk0lhuIhABsa46Obo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df9d33",
   "metadata": {
    "id": "d5df9d33"
   },
   "source": [
    "Есть несколько способов сходить в API. Можно ходить напрямую через библиотеку **requests**. Допишите post запрос в `url` с данными `data` и заголовками `headers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e9f97ed",
   "metadata": {
    "id": "9e9f97ed"
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Authorization': 'Bearer ' + API_KEY,\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "url = \"https://api.together.xyz/v1/chat/completions\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n",
    "]\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"messages\": messages\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "model_answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "assert \"london\" in model_answer.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8efb521",
   "metadata": {
    "id": "a8efb521"
   },
   "source": [
    "Мы подали messages, дальше они каким-то образом собрались в promt и подались модели. Мы не знаем, какой промпт используется на стороне провайдера. Вспомним про Today Date из предыдущего пункта задания - использует ли его together? Обновляют ли они его сегодняшним днем или оставляют Today Date? Если обновляют, то по какому часовому поясу?\n",
    "\n",
    "Чтобы ответы на эти и многие другие вопросы не мучали нас по ночам, можно использовать prompt формат, а именно подать модели текст напрямую на генерацию. Давайте для этого используем `tokenizer.apply_chat_template`. Модель будет принимать текст ровно так, как вы его подадите, без каких-либо предобработок. Подумайте, нужно ли вам использовать аргумент `add_generation_prompt`?\n",
    "\n",
    "Чтобы послать запрос напрямую, нужно в предыдущем запросе убрать messages, который представляет из себя список словарей, и послать поле prompt - строку с промптом для модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "772bd506",
   "metadata": {
    "id": "772bd506"
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Authorization': 'Bearer ' + API_KEY,\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "url = \"https://api.together.xyz/v1/chat/completions\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"prompt\": prompt\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "model_answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "assert \"london\" in model_answer.lower() and \"assistant\" not in model_answer.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9789bff9",
   "metadata": {
    "id": "9789bff9"
   },
   "source": [
    "## Клиент\n",
    "\n",
    "Теперь мы понимаем общую схему взаимодействия с провайдером - они предоставляют апи, куда можно посылать или промтп или историю диалога. При посылке промпта вся ответственность за формат ложится на нас, при посылке messages форматтинг происходит на стороне провайдера, но мы не всегда представляем, как он работает. Выбор в пользу того или иного варианта всегда остается на вас.\n",
    "\n",
    "Мы использовали выше библиотеку requests, чтобы послать HTTP-запрос на сервера together, однако есть способ и проще - python client. Давайте познакомимся с ним поближе. Для этого давайте используем функцию `client.chat.completions.create`. Также давайте добавим опции сэмплинга, которые в этой функции поддержаны. Их можно посылать и в запросах через requests, но мы здесь и далее будем пользоваться клиентом.\n",
    "* top_k = 100\n",
    "* temperature = 0.5\n",
    "* top_p = 0.9\n",
    "* repetition_penalty = 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88dc9341",
   "metadata": {
    "id": "88dc9341"
   },
   "outputs": [],
   "source": [
    "client = Together(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b88e3ef8",
   "metadata": {
    "id": "b88e3ef8"
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    top_k=100,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.05,\n",
    "    max_tokens=64\n",
    ")\n",
    "\n",
    "response_text = response.choices[0].message.content\n",
    "assert \"london\" in response_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c4a4c",
   "metadata": {
    "id": "1c0c4a4c"
   },
   "source": [
    "Аналогично посылать просто prompt можно через `client.completions.create`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5758f",
   "metadata": {
    "id": "08d5758f"
   },
   "source": [
    "## Tools\n",
    "\n",
    "Давайте теперь посмотрим, как можно использовать tools в связке с моделями. У нас есть функция, которая входит в базу данных и получает информацию о юзере. Базы данных, конечно же, у нас никакой нет, но у нас есть некоторая функция, которая эмулирует это поведение, так что давайте попробуем ее описать.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dd5090a",
   "metadata": {
    "id": "2dd5090a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job': 'DeepSchool Founder', 'city': 'Novosibirsk'}\n"
     ]
    }
   ],
   "source": [
    "def get_user_info_from_db(person_name: str) -> Dict[str, str]:\n",
    "    database = {\n",
    "        \"ilya\": {\n",
    "            \"job\": \"Software Developer\",\n",
    "            \"pets\": \"dog\",\n",
    "        },\n",
    "        \"farruh\": {\n",
    "            \"job\": \"Senior Data & Solution Architect\",\n",
    "            \"hobby\": \"travelling, hiking\",\n",
    "        },\n",
    "        \"timur\": {\n",
    "            \"job\": \"DeepSchool Founder\",\n",
    "            \"city\": \"Novosibirsk\",\n",
    "        }\n",
    "    }\n",
    "    no_info = {\"err\": f\"No info about {person_name}\"}\n",
    "    return database.get(person_name.lower(), no_info)\n",
    "\n",
    "print(get_user_info_from_db(\"Timur\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e8052",
   "metadata": {
    "id": "bf7e8052"
   },
   "source": [
    "Давайте попробуем описать эту функцию в формате json, чтобы модель могла ее увидеть!\n",
    "Заполните поля в определении дальше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbb3a3cf",
   "metadata": {
    "id": "dbb3a3cf"
   },
   "outputs": [],
   "source": [
    "get_user_info_from_db_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_user_info_from_db\",\n",
    "        \"description\": \"Fetches information about a person from a simulated database and returns a dictionary of known attributes (e.g., job, pets, hobby, city). If the person is not found, returns an error dict.\", \n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"person_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\":\"Name of the person to look up in the database (case-insensitive).\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"person_name\"] \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f5bd6",
   "metadata": {
    "id": "5e9f5bd6"
   },
   "source": [
    "Теперь давайте подадим это описание в `tokenizer.apply_chat_template`. Обратите внимание на его аргумент `tools`! Не забудьте `add_generation_prompt`, если он нужен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ad9aaf2",
   "metadata": {
    "id": "0ad9aaf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
      "\n",
      "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.Do not use variables.\n",
      "\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"get_user_info_from_db\",\n",
      "        \"description\": \"Fetches information about a person from a simulated database and returns a dictionary of known attributes (e.g., job, pets, hobby, city). If the person is not found, returns an error dict.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"person_name\": {\n",
      "                    \"type\": \"string\",\n",
      "                    \"description\": \"Name of the person to look up in the database (case-insensitive).\"\n",
      "                }\n",
      "            },\n",
      "            \"required\": [\n",
      "                \"person_name\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "What do you know about Ilya?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools=[get_user_info_from_db_tool],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e980a9be",
   "metadata": {
    "id": "e980a9be"
   },
   "source": [
    "Давайте пошлем наш запрос в модель. На выбор 2 модели, если не будет работать с 8b, то предлагается посылать в 70b.\n",
    "Для данного запроса для 8b был подобран работающий `seed=9706540181089681000`, который можно подать в функцию.\n",
    "\n",
    "Давайте воспользуемся `client.completions.create` для генерации ответа от модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e080b5",
   "metadata": {
    "id": "25e080b5"
   },
   "outputs": [],
   "source": [
    "model_8b = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "model_70b = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2cdaebf",
   "metadata": {
    "id": "b2cdaebf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}\n",
      "{\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}\n"
     ]
    }
   ],
   "source": [
    "response_8b = client.completions.create(\n",
    "    model=model_8b,\n",
    "    prompt=prompt,\n",
    "    seed=9706540181089681000,\n",
    "    max_tokens=64\n",
    ")\n",
    "response_70b = client.completions.create(\n",
    "    model=model_70b,\n",
    "    prompt=prompt,\n",
    "    max_tokens=64\n",
    ")\n",
    "\n",
    "print(response_8b.choices[0].text)\n",
    "print(response_70b.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b870a67",
   "metadata": {
    "id": "3b870a67"
   },
   "source": [
    "Если все хорошо, то мы получили ответ от модели, который выглядит как некоторый структурированный вывод, который можно использовать для вызова модели. Давайте попробуем написать функцию, которая принимает ответ модели в \"сыром виде\", выбирает, какую функцию с какими аргументами вызвать.\n",
    "\n",
    "Здесь нам поможет FUNCTION_REGISTRY и то, что параметры в функцию можно передавать как словарь, например так\n",
    "```python\n",
    "def foo(a, b, c):\n",
    "    print(a, b, c)\n",
    "\n",
    "obj = {'b':10, 'c':'lee'}\n",
    "\n",
    "foo(100, **obj)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92feaf07",
   "metadata": {
    "id": "92feaf07"
   },
   "outputs": [],
   "source": [
    "FUNCTION_REGISTRY = {\"get_user_info_from_db\": get_user_info_from_db}\n",
    "# На случай, если модель не генерит function call\n",
    "reference_answer = \"\"\"{\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}\"\"\"\n",
    "\n",
    "\n",
    "def parse_function_call(model_answer):\n",
    "    s = model_answer.strip()\n",
    "\n",
    "    try:\n",
    "        payload = json.loads(s)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "    \n",
    "    name = payload.get(\"name\")\n",
    "    params = payload.get(\"parameters\")\n",
    "\n",
    "    fn = FUNCTION_REGISTRY.get(name)\n",
    "\n",
    "    return fn(**params)\n",
    "\n",
    "\n",
    "assert parse_function_call(reference_answer) == get_user_info_from_db(\"Ilya\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed947f2",
   "metadata": {
    "id": "eed947f2"
   },
   "source": [
    "Теперь давайте попробуем объединить все это в историю диалога и сгенерировать моделью финальный ответ.\n",
    "Для этого в messages, где хранится наша история диалога нужно добавить\n",
    "1. Вызов function call моделью с ролью ХХХ (это часть задания, напишите сами)\n",
    "2. Ответ function call с ролью tool\n",
    "\n",
    "После этого данный промпт нужно послать модели снова, чтобы получить финальный ответ.\n",
    "Для этого опять используем `tokenizer.apply_chat_template` и `client.completions.create`.\n",
    "\n",
    "В зависимости от модели может понадобиться убрать tools (на 8b, 70b должна справиться). Для 8b опять же подобран seed=2017684582943914000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e7e6d47",
   "metadata": {
    "id": "5e7e6d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What do you know about Ilya?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}<|eot_id|><|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "{\"job\": \"Software Developer\", \"pets\": \"dog\"}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Ilya is a software developer and has a dog.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
    "]\n",
    "\n",
    "prompt1 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools=[get_user_info_from_db_tool],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "resp1 = response_8b = client.completions.create(\n",
    "    model=model_8b,\n",
    "    prompt=prompt,\n",
    "    seed=9706540181089681000,\n",
    "    max_tokens=64\n",
    ")\n",
    "ans1 = resp1.choices[0].text\n",
    "messages.append({\"role\": \"assistant\", \"content\": ans1})\n",
    "\n",
    "tool_output = parse_function_call(ans1)\n",
    "messages.append({\"role\": \"tool\", \"content\": tool_output})\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "response_8b = client.completions.create(model=model_70b, prompt=prompt, seed=2017684582943914000)\n",
    "print(response_8b.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce7746",
   "metadata": {
    "id": "93ce7746"
   },
   "source": [
    "Теперь давайте посмотрим на chat-API, как обрабатываются function calls там?\n",
    "Используем для этого уже знакомый `client.chat.completions.create`, обратим внимание на аргумент tools внутри него. Здесь рекомендуется использовать 70b модель. На всякий случай работающий seed=14157400267283583000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bae78906",
   "metadata": {
    "id": "bae78906"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=model_70b,\n",
    "    messages=messages,\n",
    "    tools=[get_user_info_from_db_tool],\n",
    "    seed=14157400267283583000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2dd36",
   "metadata": {
    "id": "f9e2dd36"
   },
   "source": [
    "Мы можем видеть, что у нас не работает предыдущий подход с полем `content`, однако должно было появиться поле `tool_calls`, которое содержит в себе информацию о вызове инструмента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13c84279",
   "metadata": {
    "id": "13c84279",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolCalls(id='call_p1fouj4ylwkg12frb0qpt8he', type='function', function=FunctionCall(name='get_user_info_from_db', arguments='{\"person_name\":\"Ilya\"}'), index=0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907c917",
   "metadata": {
    "id": "e907c917"
   },
   "source": [
    "# Использование библиотек\n",
    "\n",
    "Теперь, когда мы руками прошли весь пути обработки function call можно посмотреть уже на готовые инструменты.\n",
    "Мы много чего сделали руками:\n",
    "1. Писали описание функции\n",
    "2. Обрабатывали ответ\n",
    "3. Вызывали функцию\n",
    "4. Возвращали все это в модель\n",
    "\n",
    "Давайте теперь посмотрим, как оно работает в библиотеках!\n",
    "\n",
    "**NB** - библиотеки развиваются и вполне, возможно, что к концу роадмапа те интерфейсы, которые мы используем в этом домашнем задании будут уже неактуальны, но я уверен, что знаний и принципов, полученных из этих заданий хватит, чтобы адаптироваться к будущим вызовам!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89c627",
   "metadata": {
    "id": "6d89c627",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain==0.2.16 llama-index-core==v0.11.16 langchain-together==0.2.0 llama-index-llms-together==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b964f9",
   "metadata": {
    "id": "51b964f9"
   },
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef83e32",
   "metadata": {
    "id": "5ef83e32"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_together import ChatTogether\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b08ae",
   "metadata": {
    "id": "0a7b08ae"
   },
   "source": [
    "Давайте ознакомимся с langchain-интеграцией together.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f75658",
   "metadata": {
    "id": "f7f75658"
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOGETHER_API_KEY\"] = API_KEY\n",
    "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "\n",
    "llm = ChatTogether(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d548c4e",
   "metadata": {
    "id": "9d548c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several notable individuals and entities named Ilya. Here are a few:\n",
      "\n",
      "1. Ilya Muromets: Ilya Muromets is a legendary hero in Russian folklore. He was a medieval warrior and a prince from the city of Murom. According to legend, he was a skilled fighter and a wise leader who fought against the Mongols and other invaders.\n",
      "\n",
      "2. Ilya Kovalchuk: Ilya Kovalchuk is a Russian professional ice hockey player who played in the National Hockey League (NHL) for the Atlanta Thrashers, New Jersey Devils, and Los Angeles Kings. He is a highly skilled forward and one of the top scorers in NHL history.\n",
      "\n",
      "3. Ilya Prigogine: Ilya Prigogine was a Russian-born Belgian chemist and Nobel laureate who made significant contributions to the field of thermodynamics and the study of complex systems. He was awarded the Nobel Prize in Chemistry in 1977 for his work on the theory of dissipative structures.\n",
      "\n",
      "4. Ilya Repin: Ilya Repin was a Russian painter who is considered one of the most important figures in Russian art. He was a master of portraiture and historical painting, and his works often depicted scenes from Russian history and everyday life.\n",
      "\n",
      "5. Ilya Salkind: Ilya Salkind is a Russian-born American film producer who is best known for producing the Superman film series, including the 1978 film \"Superman\" starring Christopher Reeve.\n",
      "\n",
      "These are just a few examples of notable individuals and entities named Ilya. If you have any specific information or context about the Ilya you are interested in, I may be able to provide more information.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d698905",
   "metadata": {
    "id": "1d698905"
   },
   "source": [
    "Теперь, когда мы разобрались, как базово работать с langchain, давайте попробуем добавить инструментов. Чтобы нам было не так скучно, давайте напишем новую функцию, которая считает \"волшебную операцию\".\n",
    "\n",
    "Эта функция принимает 2 строки, возвращает строку строку b в обратном порядке, сконкатенированную со строкой a. Допишите эту функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb6976bf",
   "metadata": {
    "id": "bb6976bf"
   },
   "outputs": [],
   "source": [
    "def magic_operation(a, b):\n",
    "    return b[::-1] + a\n",
    "\n",
    "assert magic_operation(\"456\", \"321\") == \"123456\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89726ec1",
   "metadata": {
    "id": "89726ec1"
   },
   "source": [
    "Теперь давайте обернем эту функцию в декоратор tool из langchain, аннотируем типы и допишем docstring. После этого можно будет автоматически сгенерировать описани функции в function call формате!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "482c8b38",
   "metadata": {
    "id": "482c8b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Concatenate the reverse of `b` with `a`.\\n\\nArgs:\\n    a: First string (will be appended to the end).\\n    b: Second string (will be reversed and placed at the beginning).\\n\\nReturns:\\n    A new string equal to reverse(b) + a.', 'properties': {'a': {'title': 'A', 'type': 'string'}, 'b': {'title': 'B', 'type': 'string'}}, 'required': ['a', 'b'], 'title': 'magic_operation_tool', 'type': 'object'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitry\\AppData\\Local\\Temp\\ipykernel_22660\\1743887400.py:15: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  print(magic_operation_tool.args_schema.schema())\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def magic_operation_tool(a: str, b: str) -> str:\n",
    "    \"\"\"\n",
    "    Concatenate the reverse of `b` with `a`.\n",
    "\n",
    "    Args:\n",
    "        a: First string (will be appended to the end).\n",
    "        b: Second string (will be reversed and placed at the beginning).\n",
    "\n",
    "    Returns:\n",
    "        A new string equal to reverse(b) + a.\n",
    "    \"\"\"\n",
    "    return magic_operation(a, b)\n",
    "\n",
    "print(magic_operation_tool.args_schema.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a60cf8b",
   "metadata": {
    "id": "5a60cf8b"
   },
   "source": [
    "Теперь давайте попробуем подать запрос в нашу LLM и обогатить ее нашим function_call. Для этого нужна функция `llm.bind_tools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6c465f4",
   "metadata": {
    "id": "f6c465f4"
   },
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([magic_operation_tool])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db362d",
   "metadata": {
    "id": "d0db362d"
   },
   "source": [
    "Теперь давайте как и раньше:\n",
    "1. Сгенерируем ответ на messages\n",
    "2. Проверим в ответе resp.tool_calls, вызовем нужный инструмент\n",
    "3. Расширим messages ответом модели и ответом инструмента, сгенерируем финальный ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a3c3f",
   "metadata": {
    "id": "b15a3c3f"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`\"}\n",
    "]\n",
    "resp = llm_with_tools.invoke(messages)\n",
    "\n",
    "if resp.tool_calls:\n",
    "    messages.append(resp)\n",
    "\n",
    "    tool_out = magic_operation_tool.invoke(resp.tool_calls[0])\n",
    "    messages.append(tool_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9a20e11",
   "metadata": {
    "id": "f9a20e11"
   },
   "outputs": [],
   "source": [
    "assert len(messages) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d462fa5",
   "metadata": {
    "id": "0d462fa5"
   },
   "outputs": [],
   "source": [
    "res = llm.invoke(messages).content\n",
    "assert \"123456\" in res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb1a5b",
   "metadata": {
    "id": "3feb1a5b"
   },
   "source": [
    "# LlamaIndex\n",
    "\n",
    "Аналогичный инструмент LlamaIndex. В ней не так хороша поддержка function calls не для OpenAI, поэтому придется забежать вперед и использовать ReActAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6485db",
   "metadata": {
    "id": "9e6485db"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.together import TogetherLLM\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import ReActAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfef1b75",
   "metadata": {
    "id": "cfef1b75"
   },
   "outputs": [],
   "source": [
    "llm = TogetherLLM(model=model_70b, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea9b3e",
   "metadata": {
    "id": "b6ea9b3e"
   },
   "source": [
    "Скопируйте magic_operation_tool из части с langchain сюда,  но без декоратора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6a47a7",
   "metadata": {
    "id": "4b6a47a7"
   },
   "outputs": [],
   "source": [
    "def magic_operation_tool(a: str, b: str) -> str:\n",
    "    \"\"\"\n",
    "    Concatenate the reverse of `b` with `a`.\n",
    "\n",
    "    Args:\n",
    "        a: First string (will be appended to the end).\n",
    "        b: Second string (will be reversed and placed at the beginning).\n",
    "\n",
    "    Returns:\n",
    "        A new string equal to reverse(b) + a.\n",
    "    \"\"\"\n",
    "    print(\"INSIDE FUNCTION CALL\")\n",
    "    return magic_operation(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfc72e3",
   "metadata": {
    "id": "3bfc72e3"
   },
   "source": [
    "Мы можем аналогично создать инструмент с помощью `FunctionTool.from_defaults`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50ff121",
   "metadata": {
    "id": "b50ff121"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolMetadata(description='magic_operation_tool(a: str, b: str) -> str\\n\\n    Concatenate the reverse of `b` with `a`.\\n\\n    Args:\\n        a: First string (will be appended to the end).\\n        b: Second string (will be reversed and placed at the beginning).\\n\\n    Returns:\\n        A new string equal to reverse(b) + a.', name='magic_operation_tool', fn_schema=<class 'llama_index.core.tools.utils.magic_operation_tool'>, return_direct=False)\n"
     ]
    }
   ],
   "source": [
    "magic_operation_tool_llamaindex = FunctionTool.from_defaults(fn=magic_operation_tool)\n",
    "print(magic_operation_tool_llamaindex.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73306d2",
   "metadata": {
    "id": "a73306d2"
   },
   "source": [
    "Давайте создадим ReActAgent: ему нужно передать tools, llm, memory=None и verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84236969",
   "metadata": {
    "id": "84236969"
   },
   "outputs": [],
   "source": [
    "agent = ReActAgent(\n",
    "    tools=[magic_operation_tool_llamaindex],\n",
    "    llm=llm,\n",
    "    memory=None,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12a204d8",
   "metadata": {
    "id": "12a204d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSIDE FUNCTION CALL\n"
     ]
    }
   ],
   "source": [
    "text = \"Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`\"\n",
    "handler = agent.run(text)\n",
    "response = await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa4ec5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The result of the magic operation for strings '456' and '321' is 123456.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725419ad",
   "metadata": {
    "id": "725419ad"
   },
   "source": [
    "# Agents\n",
    "\n",
    "Настала пора сделать своего агента!\n",
    "Попробуем сделать финансового аналитика. Требования следующие:\n",
    "бот должен по запросу данных о какой-либо компании смотреть самые большие изменения цены ее акций за последний месяц, после чего бот должен объяснить, с какой новостью это связано.\n",
    "\n",
    "Предлагается не строить сложную систему с классификаторами, а отдать всю сложную работу агенту. Давайте посмотрим, какие API нам доступны.\n",
    "\n",
    "Первым делом получение котировок - для этого нам поможет библиотека yfinance. По названию компании и периоду отчетности можно посмотреть открывающие цены на момент открытия и закрытия биржи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a00d2f0",
   "metadata": {
    "id": "5a00d2f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-12-29 00:00:00-05:00</th>\n",
       "      <td>272.690002</td>\n",
       "      <td>273.760010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-30 00:00:00-05:00</th>\n",
       "      <td>272.809998</td>\n",
       "      <td>273.079987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-31 00:00:00-05:00</th>\n",
       "      <td>273.059998</td>\n",
       "      <td>271.859985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-02 00:00:00-05:00</th>\n",
       "      <td>272.260010</td>\n",
       "      <td>271.010010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-05 00:00:00-05:00</th>\n",
       "      <td>270.640015</td>\n",
       "      <td>267.260010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-06 00:00:00-05:00</th>\n",
       "      <td>267.000000</td>\n",
       "      <td>262.359985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-07 00:00:00-05:00</th>\n",
       "      <td>263.200012</td>\n",
       "      <td>260.329987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-08 00:00:00-05:00</th>\n",
       "      <td>257.019989</td>\n",
       "      <td>259.040009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-09 00:00:00-05:00</th>\n",
       "      <td>259.079987</td>\n",
       "      <td>259.369995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-12 00:00:00-05:00</th>\n",
       "      <td>259.160004</td>\n",
       "      <td>260.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-13 00:00:00-05:00</th>\n",
       "      <td>258.720001</td>\n",
       "      <td>261.049988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-14 00:00:00-05:00</th>\n",
       "      <td>259.489990</td>\n",
       "      <td>259.959991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-15 00:00:00-05:00</th>\n",
       "      <td>260.649994</td>\n",
       "      <td>258.209991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-16 00:00:00-05:00</th>\n",
       "      <td>257.899994</td>\n",
       "      <td>255.529999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-20 00:00:00-05:00</th>\n",
       "      <td>252.729996</td>\n",
       "      <td>246.699997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-21 00:00:00-05:00</th>\n",
       "      <td>248.699997</td>\n",
       "      <td>247.649994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-22 00:00:00-05:00</th>\n",
       "      <td>249.199997</td>\n",
       "      <td>248.350006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-23 00:00:00-05:00</th>\n",
       "      <td>247.320007</td>\n",
       "      <td>248.039993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-26 00:00:00-05:00</th>\n",
       "      <td>251.479996</td>\n",
       "      <td>255.410004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-27 00:00:00-05:00</th>\n",
       "      <td>259.170013</td>\n",
       "      <td>258.269989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-28 00:00:00-05:00</th>\n",
       "      <td>257.649994</td>\n",
       "      <td>256.440002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Open       Close\n",
       "Date                                             \n",
       "2025-12-29 00:00:00-05:00  272.690002  273.760010\n",
       "2025-12-30 00:00:00-05:00  272.809998  273.079987\n",
       "2025-12-31 00:00:00-05:00  273.059998  271.859985\n",
       "2026-01-02 00:00:00-05:00  272.260010  271.010010\n",
       "2026-01-05 00:00:00-05:00  270.640015  267.260010\n",
       "2026-01-06 00:00:00-05:00  267.000000  262.359985\n",
       "2026-01-07 00:00:00-05:00  263.200012  260.329987\n",
       "2026-01-08 00:00:00-05:00  257.019989  259.040009\n",
       "2026-01-09 00:00:00-05:00  259.079987  259.369995\n",
       "2026-01-12 00:00:00-05:00  259.160004  260.250000\n",
       "2026-01-13 00:00:00-05:00  258.720001  261.049988\n",
       "2026-01-14 00:00:00-05:00  259.489990  259.959991\n",
       "2026-01-15 00:00:00-05:00  260.649994  258.209991\n",
       "2026-01-16 00:00:00-05:00  257.899994  255.529999\n",
       "2026-01-20 00:00:00-05:00  252.729996  246.699997\n",
       "2026-01-21 00:00:00-05:00  248.699997  247.649994\n",
       "2026-01-22 00:00:00-05:00  249.199997  248.350006\n",
       "2026-01-23 00:00:00-05:00  247.320007  248.039993\n",
       "2026-01-26 00:00:00-05:00  251.479996  255.410004\n",
       "2026-01-27 00:00:00-05:00  259.170013  258.269989\n",
       "2026-01-28 00:00:00-05:00  257.649994  256.440002"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "stock = yf.Ticker(\"AAPL\") # посмотрим котировки APPLE\n",
    "df = stock.history(period=\"1mo\")\n",
    "df[[\"Open\", \"Close\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d2b6b",
   "metadata": {
    "id": "026d2b6b"
   },
   "source": [
    "Для поиска новостей нам поможет https://newsapi.org/\n",
    "Можно легко получить свой ключ за короткую регистрацию, дается 1000 запросов в день, каждый запрос может включать в себя ключевое слово и промежуток дат. По бесплатному апи ключу дается ровно 1 месяц, что нам подходит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639e4310",
   "metadata": {
    "id": "639e4310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What Apple and Google’s Gemini deal means for both companies\n",
      "For years, Apple and Google have had a will-they-won't-they type of relationship, as far as which AI company Apple would pick to underpin its Siri virtual assistant and give it new AI-fueled personalization and agentic capabilities. Apple has spent the last y…\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "api_key = \"90b6dcd60c4049ac8cc5293002e95020\" \n",
    "api_template = \"https://newsapi.org/v2/everything?q={keyword}&apiKey={api_key}&from={date_from}\"\n",
    "\n",
    "articles = requests.get(api_template.format(keyword=\"Apple\", api_key=api_key, date_from=\"2026-01-01\")).json()\n",
    "\n",
    "for article in articles[\"articles\"]:\n",
    "    if article[\"title\"] != \"[Removed]\":\n",
    "        print(article[\"title\"])\n",
    "        print(article[\"description\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f75126",
   "metadata": {
    "id": "32f75126"
   },
   "source": [
    "Очень много статей заблокированы и имеют название `[Removed]`, нужно их отфильтровать. В оставшихся статьях будем брать только title (заголовок) и description (описание или краткий пересказ)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9378f6",
   "metadata": {
    "id": "cc9378f6"
   },
   "source": [
    "Вам необходимо реализовать [ReAct Agent](https://react-lm.github.io/). Особенность этого агента заключается в том, что он вначале формирует мысль, а потом вызывает действие (function call) для достижения какой-либо цели.\n",
    "\n",
    "Что нужно сделать:\n",
    "1. Описать и реализовать function call для определения, в какой день была самая большая разница в цене акций в момент открытия и закрытия биржи. Функция получает один аргумент - название акций компании (например AAPL для Apple), а выдает словарь с 2мя полями: с датой максимальной разницы в ценах и самой разницей в ценах.\n",
    "2. Описать и реализовать function call для получения 5 релевантных новостей о компании. В качестве аргумента принимаются название компании и дата. Ваша задача - сходить в newsapi, получить новости и вернуть 5 случайных новостей, которые произошли не позже чем день торгов. Если новостей меньше 5, то верните столько, сколько получится.\n",
    "3. После этого агент должен вернуть ответ, в котором постарается аргументировать изменения в цене.\n",
    "\n",
    "\n",
    "Реализовывать агента можно любым удобным способом, в том числе взять готовые имплементации.\n",
    "1. [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/agent/react_agent/) - вдобавок можно посмотреть предыдущее задание, где он уже используется.\n",
    "2. [Langchain/Langgraph](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/#code)\n",
    "3. Написать полностью свою реализацию\n",
    "\n",
    "\n",
    "Не забудьте, что очень важно описать задачу в промпте: нужно сказать, какие цели у агента и что он должен сделать. У функций должны быть говорящие описания, чтобы LLM без лишних проблем поняла, какие есть функции и когда их использовать. По всем вопросам можно обращаться в наш телеграм-чат в канал \"Tools & Agents\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b0dac5",
   "metadata": {
    "id": "a2b0dac5"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "\n",
    "def get_max_open_close_diff(ticker: str) -> dict:\n",
    "    \"\"\"\n",
    "    Находит день за последний месяц, когда абсолютная разница |Open - Close| была максимальной.\n",
    "    \n",
    "    Args:\n",
    "        ticker: тикер акции (например \"AAPL\")\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"date\": \"YYYY-MM-DD\", \"diff\": float}\n",
    "    \"\"\"\n",
    "    stock = yf.Ticker(ticker)\n",
    "    df = stock.history(period=\"1mo\")\n",
    "\n",
    "    diff_series = (df[\"Open\"] - df[\"Close\"]).abs()\n",
    "\n",
    "    max_ts = diff_series.idxmax()\n",
    "    max_diff = float(diff_series.loc[max_ts])\n",
    "\n",
    "    max_date = max_ts.date().isoformat()\n",
    "\n",
    "    return {\"date\": max_date, \"diff\": max_diff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import requests\n",
    "\n",
    "os.environ[\"NEWSAPI_KEY\"] = \"90b6dcd60c4049ac8cc5293002e95020\" \n",
    "\n",
    "def get_company_news(company: str, trade_day: str, k: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Получить до k случайных релевантных новостей по компании, опубликованных не позже trade_day.\n",
    "\n",
    "    Args:\n",
    "        company: название компании (например \"Apple\")\n",
    "        trade_day: дата торгов в формате \"YYYY-MM-DD\"\n",
    "        k: сколько новостей вернуть (по умолчанию 5)\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: список новостей (title, description)\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"NEWSAPI_KEY\")\n",
    "    api_template = \"https://newsapi.org/v2/everything?q={keyword}&apiKey={api_key}&from={date_from}\"\n",
    "\n",
    "    articles = requests.get(api_template.format(keyword=company, api_key=api_key, date_from=trade_day)).json()\n",
    "\n",
    "    filtered = []\n",
    "    for article in articles[\"articles\"]:\n",
    "        if article[\"title\"] != \"[Removed]\":\n",
    "            filtered.append({\"title\": article[\"title\"], \"description\": article[\"description\"]})\n",
    "\n",
    "    random.shuffle(filtered)\n",
    "    return filtered[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621b82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def tool_get_max_open_close_diff(ticker: str) -> dict:\n",
    "    \"\"\"Find date in last 1mo where abs(Open-Close) is maximal. Input: ticker like AAPL. Output: {date, diff}.\"\"\"\n",
    "    return get_max_open_close_diff(ticker)\n",
    "\n",
    "@tool\n",
    "def tool_get_company_news(company: str, trade_day: str, k: int = 5) -> list[dict]:\n",
    "    \"\"\"Get up to 5 random relevant news about company published not later than trade_day (YYYY-MM-DD).\"\"\"\n",
    "    return get_company_news(company, trade_day, k)\n",
    "\n",
    "\n",
    "TOOLS = [tool_get_max_open_close_diff, tool_get_company_news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c1c1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_together import ChatTogether\n",
    "\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"tgp_v1_gw1i6zpF-X3Ptq5aSuzMhlu0ATGk0lhuIhABsa46Obo\"\n",
    "\n",
    "llm = ChatTogether(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766d04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Ты финансовый аналитик.\n",
    "Твоя задача: по запросу пользователя про компанию определить, в какой день за последний месяц была максимальная разница между ценой открытия и закрытия акции, а затем объяснить, с какой новостью это связано.\n",
    "\n",
    "Правила:\n",
    "1) Сначала определи тикер акции (например, AAPL) и название компании (например, Apple) из запроса пользователя.\n",
    "   Если компания не указана явно, используй тикер как keyword для новостей.\n",
    "2) Вызови tool_get_max_open_close_diff(ticker) и получи {date, diff}.\n",
    "3) Затем вызови tool_get_company_news(company, trade_day) с датой из шага 2.\n",
    "4) Верни ответ по-русски:\n",
    "   - дата и величина движения (Open vs Close),\n",
    "   - 1–3 самых релевантных заголовка,\n",
    "   - объяснение вероятной связи новости и движения цены.\n",
    "Если новостей нет — честно скажи это и предложи возможные причины (рынок/сектор/отчетность).\n",
    "Не показывай внутренние размышления (Thought/Chain-of-thought), только итоговый ответ.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e71999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=TOOLS,\n",
    "    state_modifier=SYSTEM_PROMPT,\n",
    "    checkpointer=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d37b477e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Проанализируй AAPL (Apple)', additional_kwargs={}, response_metadata={}, id='2a19f565-0955-4bcc-b8cf-25f145cdb4ec'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_v8yj739zp5qn4mdtohwq0wg6', 'function': {'arguments': '{\"ticker\":\"AAPL\"}', 'name': 'tool_get_max_open_close_diff'}, 'type': 'function', 'index': 0}, {'id': 'call_cktwa1tdleuhulwnofwdwou0', 'function': {'arguments': '{\"company\":\"Apple\",\"trade_day\":\"2024-01-28\",\"k\":3}', 'name': 'tool_get_company_news'}, 'type': 'function', 'index': 1}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 626, 'total_tokens': 686, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'cached_tokens': 512}, 'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', 'system_fingerprint': None, 'id': 'oVDboGF-2j9zxn-9c57d852ca6fec20', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--019c092c-6cfa-7113-91c4-a9ab8e7a62ff-0', tool_calls=[{'name': 'tool_get_max_open_close_diff', 'args': {'ticker': 'AAPL'}, 'id': 'call_v8yj739zp5qn4mdtohwq0wg6', 'type': 'tool_call'}, {'name': 'tool_get_company_news', 'args': {'company': 'Apple', 'trade_day': '2024-01-28', 'k': 3}, 'id': 'call_cktwa1tdleuhulwnofwdwou0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 626, 'output_tokens': 60, 'total_tokens': 686, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  ToolMessage(content='{\"date\": \"2026-01-20\", \"diff\": 6.029998779296875}', name='tool_get_max_open_close_diff', id='4a55a3ae-c3be-4d5d-8c28-2238bcff7be5', tool_call_id='call_v8yj739zp5qn4mdtohwq0wg6'),\n",
       "  ToolMessage(content=\"Error: KeyError('articles')\\n Please fix your mistakes.\", name='tool_get_company_news', id='221a487c-888d-46f9-bd4b-f75c7708e9a0', tool_call_id='call_cktwa1tdleuhulwnofwdwou0', status='error'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_umb3rlon66tb2mbp53qu2dtz', 'function': {'arguments': '{\"company\":\"Apple\",\"trade_day\":\"2026-01-20\"}', 'name': 'tool_get_company_news'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 397, 'total_tokens': 428, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'cached_tokens': 0}, 'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', 'system_fingerprint': None, 'id': 'oVDbosp-4YNCb4-9c57d85f8f8dec20', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--019c092c-76bc-7053-ab0b-eaebb7400bde-0', tool_calls=[{'name': 'tool_get_company_news', 'args': {'company': 'Apple', 'trade_day': '2026-01-20'}, 'id': 'call_umb3rlon66tb2mbp53qu2dtz', 'type': 'tool_call'}], usage_metadata={'input_tokens': 397, 'output_tokens': 31, 'total_tokens': 428, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  ToolMessage(content='[{\"title\": \"Apple va a romper todos los récords con su nuevo MacBook Pro: su nuevo chip M5 Max va a ser una locura\", \"description\": \"2026 podría ser el año en el que Apple por fin presente su ordenador más económico, pero también será el año en el que la compañía lanzará nuevos MacBook Pro de 14 y 16 pulgadas con chips M5 Pro y M5 Max. Estos modelos, de hecho, podrían anunciarse durante es…\"}, {\"title\": \"Why is the Apple Weather App on the iPhone Predicting So Much Snow?\", \"description\": \"A major storm system is expected to deliver significant snowfall and freezing rain across more than half of the United States this weekend, with winter weather alerts in effect in cities like Atlanta, Baltimore, Boston, Charlotte, Cleveland, Dallas, Indianapo…\"}, {\"title\": \"Apple Music is sneakily becoming the best music streamer for Android\", \"description\": \"With Spotify continually raising prices and YouTube Music failing to meet the moment as a music-first device, Apple Music is emerging as the top option.\"}, {\"title\": \"WatchOS 26.2.1 Brings AirTag 2nd Gen Precision Finding to Apple Watch\", \"description\": \"I\\'m not a bat. I want my Apple Watch to show me where my AirTag second-generation trackers are, not make me listen for chirps throughout the house.\"}, {\"title\": \"iOS 26.2.1 Addresses Emergency Call Problems on Older iPhones in Australia\", \"description\": \"The iOS 26.2.1 update that Apple released today further addresses an issue preventing some older mobile phones from being able to make emergency calls.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nIn an updated support document, Apple says that iPhone users with an \\u200ciPhone\\u200c 12 or earlier should ins…\"}]', name='tool_get_company_news', id='72b387d8-2499-4a94-8e8b-8de96573cc20', tool_call_id='call_umb3rlon66tb2mbp53qu2dtz'),\n",
       "  AIMessage(content='Дата и величина движения: 20 января 2026 года, Open vs Close: 6,029998779296875.\\n\\n1. Apple va a romper todos los récords con su nuevo MacBook Pro: su nuevo chip M5 Max va a ser una locura.\\n2. Why is the Apple Weather App on the iPhone Predicting So Much Snow?\\n3. Apple Music is sneakily becoming the best music streamer for Android.\\n\\nВероятная связь новости и движения цены: Движение цены Apple на 20 января 2026 года может быть связано с новостями о новом MacBook Pro с чипом M5 Max, который может быть анонсирован в этом году. Это может привести к повышению цены акций Apple из-за ожиданий отрасли по поводу нового продукта.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 180, 'prompt_tokens': 837, 'total_tokens': 1017, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'cached_tokens': 255}, 'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', 'system_fingerprint': None, 'id': 'oVDbpE8-4YNCb4-9c57d86b5ff8ec20', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--019c092c-7e1d-7d03-b813-fb85b6bfb4dc-0', usage_metadata={'input_tokens': 837, 'output_tokens': 180, 'total_tokens': 1017, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    input={\"messages\": [(\"user\", \"Проанализируй AAPL (Apple)\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"demo-1\"}}\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f4f6fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дата и величина движения: 20 января 2026 года, Open vs Close: 6,029998779296875.\n",
      "\n",
      "1. Apple va a romper todos los récords con su nuevo MacBook Pro: su nuevo chip M5 Max va a ser una locura.\n",
      "2. Why is the Apple Weather App on the iPhone Predicting So Much Snow?\n",
      "3. Apple Music is sneakily becoming the best music streamer for Android.\n",
      "\n",
      "Вероятная связь новости и движения цены: Движение цены Apple на 20 января 2026 года может быть связано с новостями о новом MacBook Pro с чипом M5 Max, который может быть анонсирован в этом году. Это может привести к повышению цены акций Apple из-за ожиданий отрасли по поводу нового продукта.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
